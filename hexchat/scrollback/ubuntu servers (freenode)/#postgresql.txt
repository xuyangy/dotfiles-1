T 1567243356 19*	Now talking on 22#postgresql
T 1567243356 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567243356 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567243356 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567243410 18<dob118>	hi, what is wrong about this query https://pastebin.com/xTxPhkZZ   it complains about s
T 1567243514 18<xocolatl18>	dob1: you cannot use the alias in an expression
T 1567243533 18<xocolatl18>	dob1: so you need  WHERE ricerca ->> 'nome' = 'test'
T 1567243601 18<dob118>	xocolatl, ok thanks
T 1567244931 18<BigFoot45518>	How can you explain an increase of buffer hits for an index-scan after some columns are removed? I expected that after removing two columns (one var and one timestamp) the whole table will consist of less database pages and therefore lead to less buffer hits in the index scan.
T 1567244966 18<BigFoot45518>	The query I'm executing is executed completely in memory.
T 1567245002 18<BigFoot45518>	And after this "optimization" the query is minimal slower than before.
T 1567245172 18<BigFoot45518>	The query itself didn't change. The columns were just useless for this query.
T 1567245207 18<incognito18>	BigFoot455: what did you change in the index ?
T 1567245257 18<xocolatl18>	BigFoot455: removing columns doesn't actually remove the columns from existing data
T 1567245273 18<BigFoot45518>	Nothing on the index. I just dropped two columns from the table, that were not used in the query or any indices.
T 1567245316 18<incognito18>	the index scan buffer hits is only for the index_tuple
T 1567245323 18<xocolatl18>	cool, so that's kind of important information that you should provide
T 1567245329 18<xocolatl18>	did you do a normal vacuum after that?
T 1567245351 18<BigFoot45518>	Normal vacuum after the full vacuum?
T 1567245359 18<incognito18>	if it increases after a query, maybe more pages of your index are cached in the shared buffer
T 1567245359 18<xocolatl18>	yes
T 1567245363 18<BigFoot45518>	No. Is this necessary?
T 1567245376 18<xocolatl18>	it is necessary, yes
T 1567245382 18<BigFoot45518>	Why?
T 1567245390 18<xocolatl18>	several reasons
T 1567245429 18<xocolatl18>	the main ones are to recreate the visibility map and free space map
T 1567245504 18<BigFoot45518>	I thought a full vacuum, does everything a "normal" vacuum does, but first rewrites the whole table.
T 1567245520 18<xocolatl18>	nope
T 1567245522 18<incognito18>	BigFoot455: wanna show the explain and the query ?
T 1567245610 18<BigFoot45518>	Alright let me try with an additional vacuum then.
T 1567245648 18<BigFoot45518>	I can uploaded, but just the obfuscated version. One second.
T 1567245670 18<xocolatl18>	BigFoot455: don't bother with an obfuscated version
T 1567245701 18<xocolatl18>	if your table names are that important, you should hire a consultant agency and sign an nda with them
T 1567245857 18<BigFoot45518>	Thing is that I didn't talk to my supervisor, whether company is okay with sharing the queries on the internet. So that's what I can do for now.
T 1567245894 18<BigFoot45518>	https://explain.depesz.com/s/IXVb
T 1567245954 18<RhodiumToad18>	and what was the plan before?
T 1567245985 18<BigFoot45518>	As you can see the "optimized" query has about 20k more buffer hits than before.
T 1567246023 18<BigFoot45518>	This is the before plan. There should be a link to the "optimized" query. At least for me there is. I used the "Add optimization" feature.
T 1567246099 18<BigFoot45518>	One question in between: I see you guys somehow referencing my nickname all the time. How can I do this?
T 1567246119 18<xocolatl18>	just type it
T 1567246172 18<xocolatl18>	you didn't vacuum
T 1567246283 18<RhodiumToad18>	BigFoot455: so your two queries are not exactly the same?
T 1567246311 18<Rembane18>	BigFoot455: If you have a reasonable client you can type the two first letter of the nick you want to hilight and press TAB and then it will autocomplete.
T 1567246320 18<BigFoot45518>	xocolatl: I didn't vacuum. I need to setup my database again, because I tried some other stuff meanwhile.
T 1567246352 18<BigFoot45518>	Rembane: Yes it works. thanks ;).
T 1567246382 18<RhodiumToad18>	this clause: ((quebec_seven five NOT NULL) OR (quebec_seven five NULL))  appears only in one of the queries
T 1567246408 18<Rembane18>	BigFoot455: No worries, welcome to IRC. :)
T 1567246430 18<RhodiumToad18>	(I grant you, that should not affect the result)
T 1567246439 18<BigFoot45518>	RhodiumToad: I did some minor adjustments. Like removing this tautology.
T 1567246513 18<RhodiumToad18>	what else did you change?
T 1567246684 18<BigFoot45518>	RhodiumToad: In the query? Nothing.
T 1567246709 18<RhodiumToad18>	see, I get suspicious when people say that
T 1567246722 18<RhodiumToad18>	especially when they said it before and have just been proved wrong
T 1567246786 18<RhodiumToad18>	is the order of items the same in that ANY clause?
T 1567247032 18<BigFoot45518>	Except for some whitespace changes and the tautology, I can confirm that nothing changed. I just double checked.
T 1567247040 18<BigFoot45518>	Same. Yes.
T 1567247303 18<BigFoot45518>	Okay. So after removing the columns I'll run VACUUM FULL, then VACUUM and then ANALYZE.
T 1567248753 18<m1chael18>	hello. I am trying to use pg_dump... when i'm logged in to the postgres user, doing pg_dump works, however pg_dump -h localhost or pg_dump -h 127.0.0.1 produces an error: FATAL:  Ident authentication failed for user "postgres"  # i know the short answer would be to remove the -h host switch, but can anyone explain why this doesn't work and how i can fix it so that it works with -h localhost ?
T 1567248927 18<dennis_18>	m1chael: your pg_hba.conf file decide how auth is handled, in your case when you don't use -h it uses a "local" line in the pg_hba.conf and with -h it uses some other line
T 1567248948 18<m1chael18>	ah
T 1567249008 18<m1chael18>	i just got something unrelated working after 1 year of troubleshooting, so this one sounds easy
T 1567249443 18<dennisb18>	You probably have a line stating that when you come from 127.0.0.1 it should use the "ident" method. If you change it to "md5" or "scram-sha-256" it will ask you for a password. If you set it to "trust" it will let you in, no questions asked, but that's probably not the security you want.
T 1567249539 18<m1chael18>	i'm going to modify the script to leave out -h since i'll be running this from the pg user on a crontab... after i get this working i'd like to look at Barman
T 1567249630 18<RhodiumToad18>	"ident" should generally not be used on "host" lines since it uses an external identd server, which (a) you probably don't have running, (b) you probably should not run, and (c) isn't all that secure anyway
T 1567249655 18<DrAgOn_18>	CiauuuZ!!!!!!!
T 1567249674 18<RhodiumToad18>	whut
T 1567253878 18<BigFoot45518>	So I run the query now again, after executing VACUUM (after VACUUM FULL). The result is the same. With the smaller table about 20k more buffer hits are counted.
T 1567254004 18<RhodiumToad18>	which table exactly did you vacuum?
T 1567254035 18<BigFoot45518>	The table that I columns I dropped.
T 1567254098 18<RhodiumToad18>	yes but which one is it in the query
T 1567254280 18<BigFoot45518>	Is it possible, that due to the dropped columns, the rows are organized less efficient in the database pages for this query? Another query shows a performance boost (a query that has about 600k less buffer reads after dropping columns)
T 1567254343 18<RhodiumToad18>	dropping the columns doesn't move any data. the vacuum full afterwards did, but any effect on the efficiency would be pure chance
T 1567254376 18<BigFoot45518>	RhodiumToad: the "juliet yankee_golf" table.
T 1567255489 18<RhodiumToad18>	BigFoot455: the number of buffer hits for an index scan may vary slightly depending on the order in which rows are visited. not sure if this would explain what you see
T 1567255524 18<RhodiumToad18>	for example, if it visits a row on block 1, then one on block 2, then one on block 1, that would be three hits, but 1,1,2 would be two hits
T 1567255614 18<RhodiumToad18>	("hit" is incremented by taking a pin on a buffer that didn't need to be read in, but there's an optimization whereby if the scan's last tuple was on the same block as the current one, it reuses the old pin rather than taking a new one)
T 1567255846 18<unclechu18>	hello. what actually `select into` does? does it really creates a new table?
T 1567255876 18<unclechu18>	can i somehow mark such a shortcut to be automatically removed what transaction is commited?
T 1567255992 18<RhodiumToad18>	select into  as a plain sql statement, or in plpgsql? they are different
T 1567256015 18<RhodiumToad18>	select into  in plain sql is just another way to write  create table ... as select ...
T 1567256028 18<RhodiumToad18>	if that's what you want, you can use  create temp table ... as select ...
T 1567256109 18<BigFoot45518>	RhodiumToad: could be an explanation. Even though on average I'd expect the order to become more efficient. Maybe for this query it's just unfortunate.
T 1567256136 18<unclechu18>	RhodiumToad: in `psql` repl
T 1567256183 18<RhodiumToad18>	unclechu: then use  CREATE TEMP TABLE x ON COMMIT DROP AS SELECT ...;   and "x" will be dropped on commit
T 1567256185 18<unclechu18>	RhodiumToad: is there any difference between `create temp table ... as select ...` and `with` statement?
T 1567256210 18<RhodiumToad18>	unclechu: yes, they're completely different things
T 1567256226 18<unclechu18>	RhodiumToad: what would be more efficient in context of memory and performance?
T 1567256228 18<RhodiumToad18>	create temp table as select  actually creates a new temp table, that you can't access in the same query
T 1567256244 18<RhodiumToad18>	but which you can access in following queries, create indexes on, etc.
T 1567256256 18<unclechu18>	i assumet `with` would work better? since it probably doesn't create such a big structure but doing it on the fly?
T 1567256299 18<RhodiumToad18>	until pg12 (not out yet), WITH actually materializes a copy of the query result, in memory up to work_mem and then spilling to disk
T 1567256330 18<RhodiumToad18>	it's more efficient in some ways, but it has the problem that you can't create indexes on the WITH result, which can be an issue
T 1567256354 18<unclechu18>	RhodiumToad: thanks a lot, now it's a lot more clear for me
T 1567256522 18<BigFoot45518>	RhodiumToad: Also thanks from me :)
T 1567258277 18<dennisb18>	So, shouldn't --data-checksums be the default for initdb? Do you people use checksums in production?
T 1567258811 18<Zr4018>	dennisb: I'd want to, but since this is production this will have to wait until we decide to upgrade from 9.4
T 1567258856 18<Zr4018>	at previous $work I did enable it as part of upgrading to 10
T 1567260554 18<Foxfir318>	troule connection with php. installed the php postgres module. guessing that takes care of the postgres side, but php has no idea. build in php server says: Uncaught Error: Call to undefined function pg_connect()
T 1567260598 18<Foxfir318>	assuming its a very common beginners mistake
T 1567260974 18<wkalt18>	anyone know why this declarative partitioning fails? https://gist.github.com/wkalt/27eddddd6a69cfb2b8f0f38cc43d2a13
T 1567261007 18<wkalt18>	those tables don't seem like they should overlap to me
T 1567263181 18<xocolatl18>	wkalt: the overlap condition is the high end of the first partition compared with the low end of the second partition.  the condition expands like this:  1 > 1 OR (1 = 1 AND '2019-01-01' > '2019-01-02') OR (1 = 1 AND '2019-01-01' = '2019-01-02' AND 2 > 1)
T 1567263187 18<xocolatl18>	which is false, so they overlap
T 1567263208 18<RhodiumToad18>	why is it false?
T 1567263214 18<RhodiumToad18>	actually the problem is more subtle
T 1567263222 18<xocolatl18>	which one of those conditions is true?
T 1567263255 18<RhodiumToad18>	oh yeah
T 1567263308 18<RhodiumToad18>	anyway, if retention_policy is to be used to decide which partition it goes in, that has to come before the date
T 1567263366 18<wkalt18>	yeah, that's what I'm seeing experimentally. Thanks for confirming.
T 1567263401 18<RhodiumToad18>	remember that (1,1,2) comes between (1,1,1) and (1,2,1)
T 1567263460 18<wkalt18>	if that's the way to think about it, makes plenty sense
T 1567263506 18<xocolatl18>	switching upload_time and retention_policy give: (1 > 1) OR (1 = 1 AND 2 > 1) OR (1 = 1 AND 2 = 1 AND '2019-01-01' > '2019-01-02')
T 1567263512 18<xocolatl18>	and that one has a true condition
T 1567263554 18<xocolatl18>	in other news, it is very likely that upload_time should actually be of type timestamptz
T 1567263592 18<pgwhatever18>	Im trying to register my blog to postgresql.  Not really sure about all this RSS feed stuff.  Do i just provide my blog's URL for the Feedurl?
T 1567263602 18<RhodiumToad18>	no
T 1567263623 18<wkalt18>	yeah, the issue is I really want to be able to get appropriate pruning behavior for queries on upload+dataset alone
T 1567263640 18<RhodiumToad18>	you need to provide an RSS url, ideally one that includes only the postgresq-related content
T 1567263645 18<xocolatl18>	pgwhatever: what is your blog's url?
T 1567263655 18<pgwhatever18>	blog.sqlexec.com
T 1567263663 18<wkalt18>	seems like there could be a trick to get the execution time pruning to do it
T 1567263690 18<xocolatl18>	pgwhatever: right at the bottom of your page you have http://blog.sqlexec.com/index.php?feed/atom
T 1567263699 18<xocolatl18>	pgwhatever: that's what you give to planet
T 1567263742 18<RhodiumToad18>	except that if you have non-postgresql content on the blog too, you want to give it a feed of just the relevant posts
T 1567263755 18<pgwhatever18>	hmmm I don't see that link at the bottom of the page
T 1567263771 18<xocolatl18>	it's called "Entries feed"
T 1567263798 18<pgwhatever18>	ahhh
T 1567263832 18<xocolatl18>	but as RhodiumToad says, if your feed publishes something that is not postgres related, you will be immediately de-listed
T 1567263846 18<pgwhatever18>	gotcha, the name of my blog is PG Stuff, hehehe, guess its ONLY PG
T 1567263856 18<pgwhatever18>	thanks big time!
T 1567263877 18<pgwhatever18>	hot doggy, pending approval
T 1567263890 18<xocolatl18>	moderation is manual and may take some time
T 1567263899 18<pgwhatever18>	no problemo, im on the right path FINALLY!
T 1567263903 18<xocolatl18>	(I moderate a lot of postgres stuff, but not planet)
T 1567263924 18<sobriquet18>	RhodiumToad: copy and paste please.  I would like to see the answer.
T 1567263943 18<RhodiumToad18>	what answer?
T 1567263949 18<xocolatl18>	42
T 1567263978 18<dob118>	can I rename the key of a json object stored in jsonb column?
T 1567263999 18<RhodiumToad18>	dob1: you can set a new key to the old value and remove the old key
T 1567264053 18<dob118>	RhodiumToad, I am looking at json functions
T 1567264085 18<RhodiumToad18>	the - operator can delete a key, jsonb_set or || can be used to set a new key
T 1567264121 18<RhodiumToad18>	sobriquet: what answer?
T 1567264162 18<dob118>	RhodiumToad, can I do this in an update statement?
T 1567264166 18<wkalt18>	ok xocolatl RhodiumToad, I'm trying to get my pruning working and I found this: https://gist.github.com/wkalt/079356ac924cc8e41b7a57e641c46d25
T 1567264178 18<wkalt18>	that last result seems incorrect to me. Thoughts?
T 1567264214 18<RhodiumToad18>	dob1: yes of course
T 1567264216 18<dob118>	RhodiumToad, or I need to get the value in the old key, delete key, and then store a new key ?
T 1567264240 18<RhodiumToad18>	dob1: update ... set col = (expression to calculate new value from col) where ...
T 1567264266 18<RhodiumToad18>	if we're talking about top-level keys, the expression might look something like,
T 1567264271 18<dob118>	RhodiumToad, but then I need to delete the key in another statement
T 1567264279 18<dob118>	(old key)
T 1567264353 18<RhodiumToad18>	why would you need another statement?
T 1567264398 18<RhodiumToad18>	jsonb_set(col - text 'b', array['c'], col->'b', true)  removes the key "b" from "col" and stores its old value in key "c"
T 1567264399 18<dob118>	RhodiumToad, the update create the new key copying the vcalue from the old key, but I don't see how it delete the old key too
T 1567264409 18<xocolatl18>	wkalt: I get a result when I try that here
T 1567264409 18<dob118>	ah
T 1567264424 18<wkalt18>	xocolatl: I'm on 11.3. Downloading latest now.
T 1567264450 18<xocolatl18>	I doubt anything has changed here between 11.3 and 11.5
T 1567264479 18<dob118>	RhodiumToad, I don't know this functions/operators, I have to read a bit :)
T 1567264527 18<wkalt18>	xocolatl: curious to see. That script returns no result for me at the end.
T 1567264560 18<RhodiumToad18>	wkalt: \d retention_policies
T 1567264585 18<RhodiumToad18>	oh never mind
T 1567264600 18<wkalt18>	https://gist.github.com/wkalt/245c616cb17ade8acbbf235095f81f49 fwiw
T 1567264610 18<xocolatl18>	not sure what there is to "see", but https://dpaste.de/mjHG
T 1567264646 18<RhodiumToad18>	wkalt: explain analyze that last query
T 1567264674 18<xocolatl18>	missed a bit, https://dpaste.de/H1x9
T 1567264721 18<wkalt18>	https://gist.github.com/wkalt/ebee4a1aafb897ed909b77a6a6d4cbb1 RhodiumToad
T 1567264724 18<wkalt18>	it never hits the files table
T 1567264757 18<RhodiumToad18>	One-Time Filter: false
T 1567264785 18<xocolatl18>	I don't see where that filter is coming from
T 1567264849 18<RhodiumToad18>	try it with 11.5
T 1567264854 18<wkalt18>	alright give me a min
T 1567264867 18<xocolatl18>	my reproduction doesn't have that.  https://dpaste.de/OgL6
T 1567264875 18<RhodiumToad18>	what version are you on?
T 1567264893 18<xocolatl18>	I guess the filter is coming from partition pruning
T 1567264917 18<RhodiumToad18>	"Fix assorted errors in run-time partition pruning logic" -- 11.4 relnotes
T 1567264928 18<xocolatl18>	ouch
T 1567264942 18<RhodiumToad18>	no, that's possibly not it since this is plan-time pruning... still worth testing on 11.5
T 1567264951 18<xocolatl18>	mine is 11.5
T 1567265004 18<wkalt18>	works on 11.5
T 1567265023 18<RhodiumToad18>	so it may have been that bug, or one of the others fixed between 11.3 and 11.5
T 1567265034 18<xocolatl18>	that's a nasty bug
T 1567265089 18<RhodiumToad18>	such false one-time filters are added in planning any time the planner thinks it has proved a contradiction, of which excluding all the partitions of a partitioned table is one example
T 1567265143 18<RhodiumToad18>	if you have constraint exclusion enabled, you can get them from just normal query conditions like   WHERE x > 2 AND x < 1
T 1567265174 18<RhodiumToad18>	but that level of checking isn't done by default, only partition and inheritance constraints are checked
T 1567265192 18<wkalt18>	is what I really need subpartitioning here? that sounds a little scary
T 1567265205 18<incognito18>	RhodiumToad: 12beta3 no pruning
T 1567265206 18<wkalt18>	but I could partition on (dataset, upload_time) and subpartition on retention_policy
T 1567265216 18<wkalt18>	that actually feels somewhat sensible
T 1567265345 18<wkalt18>	RhodiumToad: xocolatl thanks for the help btw
T 1567265456 18<incognito18>	xocolatl:  did you notice it works w/o the subquery ?
T 1567265478 18<xocolatl18>	did you notice it's been fixed since 11.3?
T 1567265491 18<xocolatl18>	I almost (but not quite) want to bisect it
T 1567265531 18<wkalt18>	xocolatl: if you're talking to me, yeah it works fine in 11.5
T 1567265611 18<xocolatl18>	wkalt: no, I was talking to incognito
T 1567267818 18<Alexthek1d18>	hello
T 1567267828 18<Alexthek1d18>	does postgresql come with a gui on board?
T 1567267837 18<pmjdebruijn18>	not that I recall
T 1567267842 18<Alexthek1d18>	or are GUIs just 3rd party?
T 1567267846 18<Alexthek1d18>	okay
T 1567267847 18<pmjdebruijn18>	afaik
T 1567267861 18<pmjdebruijn18>	not sure what you want/need to gui for though
T 1567267994 18<Alexthek1d18>	I don't know. i just want a database system that i can edit without queries.. Already looked up mongodb but it doesn't has this feature
T 1567268015 18<Alexthek1d18>	in the end maybe what i'm looking for is just a excel table xD
T 1567268216 18<pmjdebruijn18>	Alexthek1d: if you just need an excel table, why would even consider a real database?
T 1567268250 18<Alexthek1d18>	pmjdebruijn, ye it just came to my mind
T 1567268281 18<Alexthek1d18>	i make a stats-website and just have to save football players and their stats
T 1567268290 18<Alexthek1d18>	editing would be easier with a gui then
T 1567268310 18<Alexthek1d18>	so in the end i think that using a normal table is better
T 1567268319 18<pmjdebruijn18>	Alexthek1d: wouldn't the admin backend of your website be the "gui" then
T 1567268350 18<pmjdebruijn18>	anyhow, for a web application it will likely be a decent idea to have a db backend, unless it's only lookups
T 1567268373 18<pmjdebruijn18>	if it's readonly, you might as well just place a static json file on your website, that gets read in my some javascript
T 1567268414 18<Alexthek1d18>	pmjdebruijn, in the ends it would be read only and editing with an editor
T 1567268419 18<Alexthek1d18>	pmjdebruijn, yes good idea
T 1567275239 18<daemon18>	hey all I installed 9.4.24 and even though contrib appers to be installed ... hstore is still not installable
T 1567275247 18<daemon18>	I 'make world' when I compiled it
T 1567275254 18<daemon18>	is there some other requirements to include that extension?
T 1567275266 18<xocolatl18>	don't install 9.4
T 1567275269 18<xocolatl18>	install 11
T 1567275285 18<daemon18>	xocolatl, I do not have a choice its an existing dataset and the requirements clearly state 9.4
T 1567275290 18<daemon18>	something to do with something called postgis
T 1567275309 18<xocolatl18>	then complain loudly.  9.4 is on the verge of being obsolete
T 1567275319 18<xocolatl18>	postgis doesn't need such an old version
T 1567275340 18<daemon18>	so any knowledge about hstore?
T 1567275350 18<xocolatl18>	plenty
T 1567275413 18<daemon18>	and how to make it available?
T 1567275495 18<xocolatl18>	you're installing from self-compiled source?
T 1567275499 18<daemon18>	yes
T 1567275504 18<RhodiumToad18>	you said you did make world, did you also do make install-world when installing?
T 1567275511 18<xocolatl18>	make install-world
T 1567275521 18<daemon18>	RhodiumToad, I did not, just a plain install; that would be what I missed thank you :)
T 1567275757 18<xocolatl18>	then once you get it running, upgrade to 11 and whatever the newest postgis is
T 1567275943 18<JamesB18>	Hello. I am finding after pg_dumpall, that the restore misses whole tables, because I have triggers on some tables, and also it misses some functions, because they depend on other functions, etc.
T 1567275947 18<JamesB18>	What can I do? This is a disaster..
T 1567275999 18<xocolatl18>	JamesB: okay, this isn't consistent with what usually happens.  how are you restoring it?
T 1567276010 18<JamesB18>	psql
T 1567276015 18<JamesB18>	Is there any better option?
T 1567276022 18<JamesB18>	after pg_dumpall
T 1567276036 18<xocolatl18>	I'm asking for exact command lines
T 1567276053 18<JamesB18>	psql -f backup.sql --username ... -d ...
T 1567276059 18<xocolatl18>	psql just puts you in interactive mode, so that would certainly be a disaster if you're expecting a cluster to be restored
T 1567276077 18<JamesB18>	It's just a single server. What's a better way then? All the documentation shows pg_dumpall followed by psql.
T 1567276105 18<xocolatl18>	I don't understand what you're trying to do
T 1567276112 18<JamesB18>	I'm trying to dump and restore to a new server.
T 1567276127 18<JamesB18>	Upgrading from Postgres 9 to 11.
T 1567276130 18<xocolatl18>	what are the complete commands you are running?
T 1567276148 18<JamesB18>	pg_dumpall -f backup.sql -h (old server) -U (username)
T 1567276211 18<JamesB18>	and then, psql -U (username) -d postgres -f backup.sql
T 1567276521 18<JamesB18>	Would --disable-triggers help? It says "during data-only restore"
T 1567276557 18<xocolatl18>	help what?
T 1567276564 18<JamesB18>	It to actually restore properly?
T 1567276574 18<JamesB18>	instead of copying half the tables and leaving the other half empty?
T 1567276596 18<xocolatl18>	it restores everything properly
T 1567276617 18<RhodiumToad18>	JamesB: what is the actual error output from psql
T 1567276618 18<JamesB18>	No, it doesn't.
T 1567276629 18<xocolatl18>	yes it does
T 1567276638 18<RhodiumToad18>	also why -U username? dumpalls should be restored as the postgres user only
T 1567276642 18<JamesB18>	Funny, I'm watching it not do it.
T 1567276649 18<JamesB18>	Because I had renamed the postgres user on this server.
T 1567276668 18<RhodiumToad18>	by "postgres user" I mean the db superuser, not necessarily the user named "postgres"
T 1567276680 18<JamesB18>	Yes, I know.
T 1567276700 18<RhodiumToad18>	so what is the error output?
T 1567276733 18<JamesB18>	Thousands of them. I'm redumping with --disable-triggers to see if that helps, and then I'm going to initdb again, and then maybe I can tell you.
T 1567276759 18<xocolatl18>	you don't want to restore your triggers?
T 1567276766 18<JamesB18>	I do. I want to disable them while I am restoring.
T 1567276770 18<JamesB18>	That's going to screw everything up.
T 1567276773 18<RhodiumToad18>	JamesB: do not use --disable-triggers, it will do no good
T 1567276779 18<JamesB18>	Great
T 1567276781 18<RhodiumToad18>	just tell us what the errors are
T 1567276797 18<RhodiumToad18>	if it helps, psql -v ON_ERROR_STOP=1  will stop at the first error
T 1567276805 18<JamesB18>	Well that'd be a start
T 1567276808 18<JamesB18>	But I need to redo the initdb first
T 1567276809 18<JamesB18>	Just a sec
T 1567276814 18<JamesB18>	because there's tons of crap there now.
T 1567276853 18<RhodiumToad18>	the problem with text-mode dumps is that any error invariably causes a ton of fallout from following commands, usually whining about \N
T 1567276861 18<JamesB18>	Yup
T 1567276866 18<RhodiumToad18>	which is why you look at the first error and work forwards
T 1567276885 18<JamesB18>	Is there any way to make the database not screwed up when it does it?
T 1567276892 18<JamesB18>	I'd really rather it did the whole thing in a transaction if it can.
T 1567276895 18<RhodiumToad18>	no
T 1567276899 18<JamesB18>	Well that's terrible
T 1567276900 18<JamesB18>	ok
T 1567276903 18<JamesB18>	Going to initdb yet again
T 1567276912 18<RhodiumToad18>	can't really do that because dumpall uses create database which isn't allowed in transactions
T 1567276934 18<RhodiumToad18>	this is why people usually prefer pg_dumpall -r and individual pg_dump commands on each db
T 1567276948 18<xocolatl18>	or pg_upgrade
T 1567276963 18<JamesB18>	It's a separate server. I can't do pg_upgrade.
T 1567277177 18<JamesB18>	Well, the first error is
T 1567277180 18<JamesB18>	that the role already exists
T 1567277186 18<JamesB18>	for the admin user
T 1567277198 18<JamesB18>	ANy way to go to the second error?
T 1567277210 18<JamesB18>	That's just a blatantly obvious error
T 1567277218 18<RhodiumToad18>	unfortunately not. pipe the error output into a file or pager
T 1567277230 18<RhodiumToad18>	or edit the dump to make the first error go away
T 1567277236 18<JamesB18>	That'd be 2>test.txt or somesuch?
T 1567277241 18<RhodiumToad18>	yes
T 1567277248 18<RhodiumToad18>	or psql ... 2>&1 | more
T 1567277274 18*	RhodiumToad makes a bet with himself
T 1567277296 18<RhodiumToad18>	my bet is fe60dbac172e35be65ac84eb0be424d1cabf32e7c8e3ad133595ac8cb303420c
T 1567277329 18<JamesB18>	Functio does not exist
T 1567277331 18<JamesB18>	is the first one
T 1567277335 18<RhodiumToad18>	what function?
T 1567277353 18<JamesB18>	psql:/root/backup2.sql:3256617: ERROR:  function check_order_code(text) does not exist
T 1567277353 18<JamesB18>	LINE 1: SELECT check_order_code(x) or check_age_code_long(x)
T 1567277353 18<JamesB18>	               ^
T 1567277353 18<JamesB18>	HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
T 1567277353 18<JamesB18>	QUERY:  SELECT check_order_code(x) or check_age_code_long(x)
T 1567277354 18<JamesB18>	CONTEXT:  PL/pgSQL function public.check_age_code(text) line 1 at RETURN
T 1567277361 18<JamesB18>	It's one of mine. It should have dumped it, I'd think...
T 1567277381 18<JamesB18>	After that, psql:/root/backup2.sql:3561347: ERROR:  function base32_char_regex() does not exist
T 1567277381 18<JamesB18>	LINE 2:   select exists(select regexp_matches($1, '^' || base32_char...
T 1567277381 18<JamesB18>	                                                         ^
T 1567277381 18<JamesB18>	HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
T 1567277381 18<JamesB18>	QUERY:
T 1567277382 18<JamesB18>	  select exists(select regexp_matches($1, '^' || base32_char_regex() || '{10}$'));
T 1567277387 18<RhodiumToad18>	use a paste site!
T 1567277417 18<JamesB18>	I would, but I am going to be back in a few hours. I thought this would be a quick affair, but it's my son's birthday today, so all of our servers are just going to have to be down. I dunno, very frustrating
T 1567277425 18<RhodiumToad18>	this is probably the search_path issue.
T 1567277433 18<RhodiumToad18>	are these functions used in CHECK constraints?
T 1567277434 18<JamesB18>	Oh?
T 1567277436 18<JamesB18>	Yes probably so.
T 1567277454 18<JamesB18>	The server I am connecting to to pg_dumpall is running 9.4 if it helps
T 1567277458 18<JamesB18>	Is this fixable by altering the old tables?
T 1567277466 18<RhodiumToad18>	what pg_dumpall version did you use?
T 1567277487 18<RhodiumToad18>	oh wait that wouldn't help
T 1567277497 18<JamesB18>	Yes, check_age_code is called in a check constraint.
T 1567277503 18<JamesB18>	I will be back later, have to go.
T 1567277518 18<RhodiumToad18>	the problem is that your functions call other functions assuming that "public" is on the search path, and for dubious security reasons that's no longer the case during restore
T 1567277527 18<JamesB18>	....um
T 1567277539 18<JamesB18>	So, all functions have to be reworked to call public.function_this_that?
T 1567277545 18<RhodiumToad18>	so in the old db, you could use ALTER FUNCTION ... SET search_path =   to fix it
T 1567277578 18<JamesB18>	search_path to blank? I don't understand, why would that fix it?
T 1567277580 18<JamesB18>	brb pizza to oven
T 1567277592 18<RhodiumToad18>	if you use public for everything, then  alter function ... set search_path = public;   should do it if you do it on all plpgsql funcs that call other funcs
T 1567278680 18<syf_z_otchlani18>	hello
T 1567278696 18<syf_z_otchlani18>	i have a problem connecting from docker to postgres
T 1567278705 18<JamesB18>	So is there no explicit search_path,
T 1567278711 18<JamesB18>	it no longer includes public, is the basic issue?
T 1567278736 18<syf_z_otchlani18>	No pg_hba.conf entry for host "192.168.1.59", user "jakubsadowski", database "adventureworks", ssl off.
T 1567278764 18<syf_z_otchlani18>	i set host all all 0.0.0.0/0 trust
T 1567278773 18<syf_z_otchlani18>	in ph_hba.conf
T 1567278777 18<RhodiumToad18>	JamesB: while executing the restore, public is not in the search path
T 1567278782 18<JamesB18>	All of the functions that call other f unctions, or all of the functions that call other functions, and are in CHECK statements?
T 1567278784 18<syf_z_otchlani18>	so what is blocking me?
T 1567278796 18<syf_z_otchlani18>	TIA for help..
T 1567278796 18<JamesB18>	Is there any way I could put public in the search path during restore?
T 1567278917 18<RhodiumToad18>	JamesB: not conveniently
T 1567278942 18<RhodiumToad18>	syf_z_otchlani: did you reload after changing the file?
T 1567278959 18<JamesB18>	Aha. I could change backup.sql
T 1567278962 18<JamesB18>	It has a set_config search path line..
T 1567278971 18<RhodiumToad18>	syf_z_otchlani: but seriously don't use that line, you don't want to open your db up to the whole world, people do scan for and exploit open dbs
T 1567278984 18<RhodiumToad18>	JamesB: probably more than one?
T 1567279019 18<RhodiumToad18>	JamesB: I guess you could edit that (do check whether there's just one or many of them)
T 1567279109 18<syf_z_otchlani18>	RhodiumToad: yes, restarted the server
T 1567279135 18<JamesB18>	So is this literally all of my functions that call any other functions basically?
T 1567279136 18<syf_z_otchlani18>	this is a home computer behind NAT
T 1567279159 18<RhodiumToad18>	syf_z_otchlani: are you editing the correct file? you can check with  show hba_file;  and  select * from pg_hba_file_rules;
T 1567279347 18<RhodiumToad18>	JamesB: editing all set_config('search_path'...) instances in the dump should be enough.
T 1567279350 18<syf_z_otchlani18>	          82 | host  | {all}         | {all}     | 127.0.0.1 | 255.255.255.255                         | md5         |         |
T 1567279365 18<RhodiumToad18>	syf_z_otchlani: don't paste in channel, use a paste site
T 1567279387 18<syf_z_otchlani18>	but interesting fact is that I just have 4 lines in my config file..
T 1567279404 18<syf_z_otchlani18>	and this one shows this config comes from 82th line..??
T 1567279423 18<syf_z_otchlani18>	anyway it should accept ANY user connecting ANY database
T 1567279423 18<RhodiumToad18>	you're looking at the wrong file then, check the  show hba_file;  output
T 1567279428 18<myrkraverk18>	syf_z_otchlani: this line has nothing to do with 192.168.1.59
T 1567279431 18<syf_z_otchlani18>	shouldn't it?
T 1567279445 18<RhodiumToad18>	that line you quoted only matches connections from 127.0.0.1
T 1567279456 18<RhodiumToad18>	not from 192.168.1.59
T 1567279471 18<syf_z_otchlani18>	uh, right that's 32 bit mask
T 1567279516 18<JamesB18>	RhodiumToad: Is there any larger consequence of that though? Aren't those lines to improve the database security?
T 1567279525 18<JamesB18>	I'm guessing it changes the default security, yes?
T 1567279529 18<JamesB18>	default search path,r ather
T 1567279596 18<JamesB18>	By the way, is there any reason to have pg_temp in the search path at all? I notice I did that in a few. Is it still pg_temp at the start by default if it is not listed?
T 1567279624 18<RhodiumToad18>	you should not put it in the path explicitly.
T 1567279636 18<RhodiumToad18>	it is added at the front if it exists
T 1567279653 18<RhodiumToad18>	should not usually put it in the path, that is
T 1567279704 18<JamesB18>	Hmm. I thought with earlier Postgres, if it is not listed (say, at the end), it went at the front.
T 1567279920 18<JamesB18>	It would be nice if ALTER FUNCTION didn't require parameters if the function weren't overloaded
T 1567279958 18<myrkraverk18>	JamesB: I guess that's something you get to add yourself to alter function.
T 1567280880 18<JamesB18>	All right. I hope that is all of the functions.
T 1567280891 18<JamesB18>	I guess it's good to know this before I try upgrading my really big database
T 1567280898 18<JamesB18>	This one at least is only a few gigs
T 1567281173 18<JamesB18>	So far no other errors.
T 1567407102 20*	Disconnected (20)
T 1567407128 19*	Now talking on 22#postgresql
T 1567407128 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567407128 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567407129 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567407949 20*	Disconnected (20)
T 1567407972 19*	Now talking on 22#postgresql
T 1567407972 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567407972 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567407972 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567408165 18<alireza18>	How can I do a three way join update faster?
T 1567408167 18<alireza18>	http://sqlfiddle.com/#!17/28d65
T 1567408382 18<alireza18>	it's about 400k of records
T 1567408399 18<alireza18>	i'm just curios if there's a faster way to update recorsd
T 1567408414 18<alireza18>	err, records that need other tables informatino (join)
T 1567408756 18<Widdershins18>	joins for updates aren't enormously different than joins for read as i understand it
T 1567408921 18<incognito18>	alireza: try create unique index, you can also defer the constraint check
T 1567409128 18<photonios18>	Hi everyone! I have a patch that adds a hook to copydir().. I looked a bit at how other hooks are implemented and I can't find any tests for other hooks.. Is that correct or I am I just blind?
T 1567409163 18<incognito18>	alireza : if you make your column like that, it should do what i said : id serial primary key deferrable initially deferred, ....
T 1567409874 20*	Disconnected (20)
T 1567409897 19*	Now talking on 22#postgresql
T 1567409897 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567409897 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:06 2019)
T 1567409897 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567415487 18<ysch18>	??dont
T 1567415487 18<pg_docbot18>	https://wiki.postgresql.org/wiki/Don%27t_Do_This
T 1567415521 18<Aram18>	Hi, so I have this query: http://ix.io/1U2J which uses GROUP BY to count things.
T 1567415524 18<Aram18>	analyze: http://ix.io/1U1F
T 1567415526 18<Aram18>	as you can see, it's quite slow.
T 1567415529 18<Aram18>	since this has to iterate through every row in that range, I assume there's not much I can do to make it faster?
T 1567415539 18<Aram18>	sorry, analyze link: https://explain.depesz.com/s/ExRB
T 1567415789 18<Aram18>	I also have an index on author, though I don't think that would help, as iterating by author would mean random i/o instead of linear i/o (and there are many authors)
T 1567415939 18<Myon18>	Aram: you need an index on messages(date)
T 1567415954 18<Aram18>	I have that
T 1567415996 18<Myon18>	1.6M messages from that timestamp, wtf
T 1567416005 18<Aram18>	yeah
T 1567416036 18<Myon18>	well, you'll have to scan everything with that date, so there's little room for optimization
T 1567416044 18<Aram18>	so I thought
T 1567416206 18<Aram18>	I could get the TOTAL number of messages per author from the index, but not restricted to a particular date range, no?
T 1567416243 18<Aram18>	also, I have both author, and author_id, which is a bigint instead of a string. I expected that to be faster, but no, it's eaxactly the same.
T 1567416314 18<Aram18>	it's about ~5% faster
T 1567416314 18<incognito18>	Aram can you show the index on messages(date) ? \d+ messages
T 1567416343 18<Aram18>	yes: http://ix.io/1U2L
T 1567416360 18<Aram18>	oops, let me run \d+
T 1567416373 18<Aram18>	http://ix.io/1U2M
T 1567416535 18<incognito18>	Aram how much rows do you have in this table (approximation) ?
T 1567416584 18<Aram18>	7.80928e+06
T 1567416701 18<nickb18>	I'd argue that if you need the number of messages per author you should keep track of it instead of running a query like that
T 1567416731 18<Aram18>	I suppose so, yes.
T 1567416756 18<nickb18>	also partitioning in on date in that kind of setup will make life a lot easier
T 1567416761 18<nickb18>	s/in//
T 1567416812 18<vedu_18>	Hello. Probably a silly question: I get `syntax error at or near "ORDER"` for `UPDATE core_job SET is_part_time=true ORDER BY random() LIMIT 1000;`
T 1567416820 18<nickb18>	Aram: is that _the_ Discord, or do you just have some sort of discord bot that also has that data around? (you don't have to answer)
T 1567416834 18<nickb18>	vedu_: you can't `order` in update
T 1567416845 18<Aram18>	the latter. we have a discord bot on some large servers
T 1567416845 18<incognito18>	can you try : WHERE  date BETWEEN '2018-09-01T23:52:55.355' AND '2019-09-01T23:52:55.325' ?
T 1567416863 18<incognito18>	without the Z
T 1567416871 18<Aram18>	let me see
T 1567416876 18<vedu_18>	nickb: Ohh. Subquery then :)
T 1567416877 18<incognito18>	explain only
T 1567416897 18<nickb18>	vedu_: what are you trying to achieve?
T 1567416913 18<nickb18>	vedu_: in that context your order by doesn't make any sense
T 1567416957 18<vedu_18>	nickb: update 1000 rows but not first 100
T 1567416961 18<Aram18>	incognito: without the Z: https://explain.depesz.com/s/6NPy
T 1567416965 18<Aram18>	not much difference
T 1567416968 18<vedu_18>	s/100/1000
T 1567417021 18<[patrik]18>	why, oh why?
T 1567417027 18<nickb18>	vedu_: ah.. Yeah you want `UPDATE core_job c SET is_part_time=true FROM (SELECT id FROM core_job ORDER BY random() LIMIT 1000) AS s WHERE c.id=s.id`
T 1567417036 18<nickb18>	vedu_: I hope I didn't mess up the syntax
T 1567417077 18<vedu_18>	UPDATE core_job SET is_part_time=true WHERE ID IN (SELECT id FROM core_job ORDER BY random() LIMIT 1000);
T 1567417081 18<vedu_18>	nickb: I did this ^
T 1567417100 18<nickb18>	compare the plans with EXPLAIN UPDATE ...
T 1567417113 18<nickb18>	I'm pretty sure your version is going to be bad
T 1567417131 18<vedu_18>	nickb: :')
T 1567417132 18<incognito18>	Aram: when you : explain select author_id from messages where date BETWEEN  '2018-09-01 23:52:55.325' AND  '2019-09-01 23:52:55.325' ?
T 1567417204 18<nickb18>	??loose indexscan
T 1567417204 18<pg_docbot18>	http://wiki.postgresql.org/wiki/Loose_indexscan
T 1567417251 18<incognito18>	Aram: does it seqscan or indexscan ?
T 1567417267 18<Aram18>	"Seq Scan on messages  "
T 1567417341 18<nickb18>	one thing I'd check is if its (a) even possible to use the index (which it should be) and (b) if index would make it faster; So I'd try `set enable_seqscan = off` and run the explain again to see what happens
T 1567417374 18<incognito18>	you said 7M rows, but according to your 1st explain: the filter only removes 913k, and retrieve 1.6M
T 1567417396 18<Aram18>	because of the date range, I think
T 1567417399 18<nickb18>	Aram: also, do you run regular VACUUMs on the table?
T 1567417470 18<Aram18>	I don't think vacuum ever ran, let me check.
T 1567417493 18<Aram18>	but this db is written in date order and nothing is ever deleted or updated.
T 1567417503 18<Aram18>	yeah, no vacuum
T 1567417510 18<xocolatl18>	then you should certainly cron regular vacuums
T 1567417523 18<nickb18>	ok so if you run VACUUM ANALYZE on the table, there is a good chance you can get an index-only scan
T 1567417538 18<nickb18>	you might need to rewrite the query as `count(*)` instead of `count(id)`
T 1567417543 18<incognito18>	+1 only to gather good stats
T 1567417562 18<nickb18>	(not sure if postgres checks the column for NOT NULL and rewrites count(id) as count(*))
T 1567417564 18<Aram18>	tried count(*) instead of count(*), but it's very slightly slower.
T 1567417571 18<xocolatl18>	incognito: no, ANALYZE will still run.  VACUUM is needed for the VM
T 1567417572 18<nickb18>	after VACUUM?
T 1567417591 18<nickb18>	Aram: ^
T 1567417594 18<xocolatl18>	what's the link to the query we're optimizing?
T 1567417601 18<Aram18>	no. I just ran vacuum
T 1567417606 18<Aram18>	let me run the query again
T 1567417608 18<incognito18>	xocolatl: https://explain.depesz.com/s/6NPy
T 1567417615 18<nickb18>	xocolatl: http://ix.io/1U2J http://ix.io/1U2M https://explain.depesz.com/s/ExRB
T 1567417676 18<Aram18>	ok, so after vacuum it's doing "Index Only Scan "
T 1567417677 18<nickb18>	Aram: one more thing I wanted to point out is you don't really need all those indexes. Postgres can use an index on (a,b) to filter on (a), so you can drop a few of those
T 1567417681 18<Aram18>	it's not faster though
T 1567417690 18<Aram18>	nickb: ah yeah, good point
T 1567417692 18<nickb18>	Aram: its not faster because it has to scan the entire table
T 1567417692 18<incognito18>	explain SELECT
T 1567417692 18<incognito18>	  author,
T 1567417692 18<incognito18>	  count(id)
T 1567417692 18<incognito18>	FROM (select * from messages WHERE date BETWEEN '2018-09-01T23:52:55.325Z' AND '2019-09-01T23:52:55.325Z') m
T 1567417692 18<incognito18>	GROUP BY author
T 1567417693 18<incognito18>	ORDER BY count(id) DESC
T 1567417693 18<incognito18>	LIMIT 5;
T 1567417695 18<incognito18>	oups
T 1567417699 18<Aram18>	yeah
T 1567417714 18<nickb18>	incognito: please don't paste into the channel
T 1567417741 18<incognito18>	nickb: sorry, i typed without checking in which window i am
T 1567417749 18<photonios18>	Re-posting: Hi everyone! I have a patch that adds a hook to copydir().. I looked a bit at how other hooks are implemented and I can't find any tests for other hooks.. Is that correct or I am I just blind?
T 1567417751 18<xocolatl18>	this table is... not normalized
T 1567417753 18<nickb18>	Aram: but its as good as it gets. You won't get a faster plan for this query
T 1567417759 18<Aram18>	yeah, I imagined
T 1567417760 18<julius18>	ive tried pgmodeler as recommended here, if the database does exist and you choose "drop db" from the connection dialog it says: cannot drop the currently open db.   -  disconnection all users with; https://stackoverflow.com/questions/36502401/postgres-drop-database-error-pq-cannot-drop-the-currently-open-database    does nothing for pgmodeler.  if you delete the database and export again: new_database does not exist. please remove this tool from your
T 1567417760 18<julius18>	recommendations!
T 1567417785 18<Myon18>	photonios: C-level things are hard to test
T 1567417806 18<Aram18>	nickb: after sorting, the hashing can be done in parallel, no? will more cores help?
T 1567417815 18<xocolatl18>	Aram: what's the new plan?
T 1567417830 18<photonios18>	Myon: Ye they are. I read the guide on submitting patches (its my first) and not adding test is a quick way to get your patch rejected. Figured I should ask first before submitting
T 1567417833 18<Myon18>	photonios: there's a regress.so that contains some things, but probably not very extensive
T 1567417844 18<nickb18>	Aram: sorting is done _after_ the heavy part. Yeah post explain analyze of current plan
T 1567417857 18<Aram18>	current plan: https://explain.depesz.com/s/ef0w
T 1567417884 18<nickb18>	Aram: huh? you said it does Index-only scan?
T 1567417892 18<Myon18>	photonios: src/test/regress/regress.c
T 1567417943 18<photonios18>	Myon: Thanks! I'll try to whip up a simple test there
T 1567417968 18<Aram18>	nickb: sorry, I confused myself, I does index only scan for another query without the WHERE (whereas before it did seq scan for that). this query doesn't seem to have changed before and after.
T 1567417999 18<nickb18>	yeah, that kinda makes sense
T 1567418000 18<Aram18>	this is the important query, so we can forget about the other one
T 1567418053 18<nickb18>	ok so my point stands, if this is an important query that you want to run regularly, you're better off maintaining a per-user count as part of inserting a new message into `messages`
T 1567418067 18<incognito18>	Aram: can you also try : select author_id, count(1) filter(where date between ... and ...) from messages group by author_id order by 2 desc?
T 1567418077 18<Myon18>	if you want a per-user count, why are you restricting by date?
T 1567418089 18<nickb18>	Aram: also I agree with what xocolatl said, having `author` and `author_id` seems dubious
T 1567418143 18<Aram18>	well author can change, although for unrelated reasons it doesn't change for our bot.
T 1567418160 18<xocolatl18>	but author_id doesn't change with it?
T 1567418165 18<Aram18>	no, that should stay fixed
T 1567418170 18<xocolatl18>	eww
T 1567418187 18<Aram18>	author is just a name that the user can change.
T 1567418189 18<nickb18>	ah, discord allows that yeah
T 1567418200 18<xocolatl18>	so you have messages from different authors with the same id
T 1567418206 18<nickb18>	anyway, what resolution do you need on message counts?
T 1567418233 18<Aram18>	xocolatl: no, that should never happen.
T 1567418236 18<nickb18>	xocolatl: author is actually "name" which isn't unique. This is denormalized still, but not too bad
T 1567418257 18<xocolatl18>	so when the name changes you update all the rows?
T 1567418293 18<Aram18>	nickb: well for recent queries, like last hour or day, we need exact resolution. for larger time span queries, like last year, resolution is not so important. probably 3 digit of resolution is enough.
T 1567418354 18<nickb18>	Aram: I'd propose you add daily totals per author and then do `SELECT author, sum(cnt) FROM author_digest WHERE date ... ORDER BY 2 DESC LIMIT 5`
T 1567418410 18<nickb18>	or partition `messages` and run summarize on finalized partitions so that you don't have to count them and count only _live_ partitions
T 1567418638 20*	Disconnected (20)
T 1567418664 19*	Now talking on 22#postgresql
T 1567418664 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567418664 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567418665 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567418686 18<incognito18>	this one, a little
T 1567418704 18<incognito18>	Aram: do you have more threads available for this server ?
T 1567418723 18<Aram18>	if that helps, I can probably make another VM
T 1567418732 18<Aram18>	at the moment no
T 1567418742 18<xocolatl18>	a brin index on date *might* help
T 1567418811 18<nickb18>	keep in mind that BRIN index requires manual maintenance in a heavily append-dominated _database_
T 1567418873 18<xocolatl18>	I think that was fixed recently
T 1567418933 18<ccoffey18>	@nickb oh, maybe I mean sequences. i.e.   ```pg_restore: [archiver (db)] Error from TOC entry 4904; 0 0 SEQUENCE OWNED BY <name>_seq some_user    pg_restore: [archiver (db)] could not execute query: ERROR:  must be owner of relation <name>_seq ```
T 1567418985 18<nickb18>	xocolatl: last I checked `summarize` was run by autovacuum worker connected to the $database
T 1567419018 18<xocolatl18>	nickb: correct, but I seem to remember something being done about that after I pointed it out
T 1567419067 18<xocolatl18>	nickb: 7526e10224f0792201e99631567bbe44492bbde4
T 1567419268 18<nickb18>	xocolatl: yeah thats cool, but I also read it as: you still need autovacuum worker to start somehow for the summarization to happen.
T 1567419293 18<xocolatl18>	maybe
T 1567419296 18<xocolatl18>	I'd have to study it closer
T 1567419301 18<nickb18>	its just an update of the algorithm of listing the pages that need to be summarized. I'll check the code later, thanks for the pointer
T 1567419340 18<nickb18>	coffee: can you show your restore command?
T 1567419369 18<nickb18>	ooops
T 1567419372 18<nickb18>	ccoffey: ^
T 1567419807 18<ccoffey18>	@nickb I'll get a better error, I should have been more prepared before I came in here. I was trying to use the AWS DMS system to load the data, but that's a different error. Before starting I create a DB like so: ```pg_restore -U postgres -h myrds.rds.amazonaws.com --dbname=web_db_synced --schema-only -C ../synced/web_db_synced.schema.sql --verbose ``` I'm running the command as an rds_superuser but I'll get errors
T 1567419807 18<ccoffey18>	like similar to ```pg_restore: [archiver (db)] could not execute query: ERROR:  must be owner of relation clientemailtemplates_id_seq    Command was: ALTER SEQUENCE clientemailtemplates_id_seq OWNED BY cientemailtemplates.id;```
T 1567420428 18<ccoffey18>	Here's an easier example. I re-ran it DMS with drop tables and re-create, as an rds_superuser, but it can't drop the tables it doesn't own. I created these tables as the same user I am using now. I clearly don't understand how permissions work in RDS `Failed to drop table public.datajob_upload`  Do I need to run the RDS database with a different parameter group?  I'm am using the "default.postgres9.6" group
T 1567422137 18<Moonsilence18>	Hi! Is it possible to dump and restore data within a single transaction? I want to dump data, drop and recreate some tables, then restore the data. I have an idea that within a psql session, I could use \copy..to to put the data into files, do my ddl stuff, then generate the corresponding \copy..from commands. This seems a bit finicky, hence my question if there is a simpler way?
T 1567422216 18<nickb18>	Moonsilence: `pg_dump --clean .... | psql -1` should do the trick?
T 1567425360 20*	Disconnected (20)
T 1567425386 19*	Now talking on 22#postgresql
T 1567425386 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567425386 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567425386 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567426416 18<ufk18>	hi! :) i loaded auto_explain, set log_nested_statements to on, log_analyze to true and log_min_duration to zero and when I explain a select from a function that executes a dynamic query, i don't see any deep info in the postgresql logs. any ideas ?
T 1567426551 18<RhodiumToad18>	loaded it how?
T 1567426572 18<RhodiumToad18>	and ran the query how?
T 1567426705 18<ufk18>	ok.... first.. the logs are not empty, i do see function scan in the logs, but it doesn't drill down to the actual query. I connect with psql and run: LOAD 'auto_explain'; SET auto_explain.log_nested_statements = ON;SET auto_explain.log_analyze = true;set auto_explain.log_min_duration=0; explain select * from my_function(my_params)"
T 1567426783 18<RhodiumToad18>	you have to actually run the function not explain it
T 1567426793 18<RhodiumToad18>	i.e.  select * from my_function(my_params);
T 1567426832 18<RhodiumToad18>	btw, you can do  set client_min_messages='log';  and you'll get the log output in psql as well
T 1567427119 18<ufk18>	oh nice
T 1567427126 18<ufk18>	thanks
T 1567427166 18<iamyojimbo18>	Hi all, question about how I could/should be using postgres. Say for example I have a stream of RMQ messages of JSON format coming in (around 20/second). These messages describe create and update events on certain entities from a data provider. Would it be insane to just dump all those messages as JSONB messages to a single table and then use only
T 1567427166 18<iamyojimbo18>	views and JSON indexing to look at the data.
T 1567427231 18<iamyojimbo18>	or should I be doing some ETL before on my data and storing it in a relational way. I do need see a live view of the latest data as up to date as possible.
T 1567427356 18<fabian__18>	Hello, if I try to select data from a view, I get  "permission denied for relation" error. But if I run the query that was used to define the view, it works. What could be the reason for that?
T 1567427496 18<xocolatl18>	fabian__: you don't have select privileges on the view
T 1567427510 18<xocolatl18>	grant select on the_view to the_user;
T 1567427613 18<RhodiumToad18>	either that or the view owner does not have select privileges on the table
T 1567427743 18<fabian__18>	RhodiumToad: THX!!! the user had select privileges on the table, but not the view owner
T 1567428143 18*	xocolatl wonders why relation_needs_vacanalyze() doesn't check for pending work requests
T 1567428802 18<ufk18>	can I somehow dump only index  creation for a specific table ?
T 1567428848 18<dim18>	use pg_dump -Fc format, and then use pg_restore -l and -L to list the objects from the dump and filter out non indexes from the list, and then use the new list with pg_restore
T 1567428880 18<dim18>	note that you can pg_dump -Fc --schema-only, so that you don't have to dump the whole data set in your case
T 1567428883 18<ufk18>	sounds simple enough :)
T 1567428964 18<xocolatl18>	or select pg_get_indexdef('indexname');
T 1567429035 18<ufk18>	ohh that's nicer
T 1567431469 18<dim18>	ah yeah good point xocolatl ; then you can to a catalog query and use \gexec IIRC
T 1567431534 18<dim18>	(well I guess you're recreating the index on another system so maybe \gexec is not useful here)
T 1567432668 18<Moonsilence18>	Hi! How do I obtain a single value as a block of text from a query yielding one column of text with multiple rows? I want to store that "block of text" into a psql variable.
T 1567432836 18<ilmari18>	Moonsilence: you mean you want to combine a single column from multiple rows into a single string?
T 1567432848 18<azeem18>	if my username has a @ in it, can I use a 'psql postgres://[...]' type connection string and how to escape the @? It seems the part afterwards always gets interpreted as the hostname
T 1567432865 18<ilmari18>	Moonsilence: string_agg()
T 1567432867 18<Moonsilence18>	ilmari, yes exactly. ah, think I got it with string_agg(mytextcol, E'\n')
T 1567432870 18<Moonsilence18>	:)
T 1567432873 18<ilmari18>	azeem: %40
T 1567432882 18<ilmari18>	(standard URI escaping)
T 1567433456 18<azeem18>	ilmari: thx!
T 1567498452 20*	Disconnected (20)
T 1567498477 19*	Now talking on 22#postgresql
T 1567498477 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567498477 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567498478 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567510873 18<pstef_18>	"pg_dump -S x --disable-triggers --data-only" will generate SET SESSION AUTHORIZATION 'x' -- but I'm missing the point of it. The assumption is that x is a superuser, but in order to SET SESSION AUTHORIZATION you already have to be a superuser role
T 1567510915 18<pstef_18>	with my limited understanding, it would have been much more useful to me if the line generated was SET ROLE ...
T 1567510931 18<RhodiumToad18>	none of those options are really any use
T 1567510963 18<RhodiumToad18>	SET SESSION AUTHORIZATION is historical, pg_dump used to use that for everything before it was changed to use ALTER ... OWNER TO ...
T 1567510967 18<pstef_18>	a non-superuser would be able to SET ROLE into a superuser, so that would be a bit of help
T 1567510975 18<RhodiumToad18>	uh, no?
T 1567511025 18<pstef_18>	why not? I use a role like that all the time. It's not a superuser until I do SET ROLE superuser; (which has been granted to my personal database role)
T 1567511293 18<RhodiumToad18>	that's not any different from a security perspective than making your personal role a superuser
T 1567511475 18<adsf18>	RhodiumToad ilmari found the issue with copy i was having yesterday. I ran csvclean from csvkit and it found null byte chars. Replaced them all with tr and copy worked :)
T 1567511510 18<RhodiumToad18>	hm
T 1567511519 18<RhodiumToad18>	I'd have expected a different error for that
T 1567511566 18<adsf18>	command with tr was tr < file -d '\000' > outfile
T 1567511621 18<adsf18>	this morning i exported the single row and re-saved it in us-ascii, which also seemed to work for some reason
T 1567513274 18<pstef_18>	RhodiumToad: sorry, I was distracted at work. So I think it is a bit different as being non-superuser by default at least prevents me from making mistakes. besides, this is an application role, which it doesn't need to be a superuser most of the time - it only needs the ability to disable system triggers (for foreign keys)
T 1567515966 18<Zaab1t18>	hello hackers. What's the difference between having all privileges and being the owner of a database?
T 1567515985 18<tangara18>	i need help to know why psql can't find my table
T 1567515997 18<tangara18>	i am using postgres as user
T 1567516021 18<tangara18>	postgres=# \copy oldmembers from 'd:\memberparticulars.csv' csv header;ERROR:  relation "oldmembers" does not exist
T 1567516075 18<mobidrop18>	is the table there?
T 1567516146 18<hruske18>	tangara: you seem to be connected to postgres database, I am guessing your data is not in that database
T 1567516178 18<hruske18>	tangara: \l to see all databases and \c databasename to switch psql client to use that
T 1567516180 18<tangara18>	i follow this tutorial to create a table first before I do the copying
T 1567516189 18<tangara18>	https://popsql.com/learn-sql/postgresql/how-to-import-a-csv-in-postgresql/
T 1567516219 18<tangara18>	i am using psql in windows run batch
T 1567516222 18<Myon18>	Zaab1t: only the owner can drop the database
T 1567516222 18<tangara18>	not linux
T 1567516261 18<Myon18>	Zaab1t: and only the owner can grant privileges (see also: WITH GRANT OPTION)
T 1567516270 18<hruske18>	tangara: sure, all this should still work
T 1567516320 18<tangara18>	hey I used \l it shows me all the database
T 1567516332 18<tangara18>	it shows me the database schema name without the table names
T 1567516357 18<Zaab1t18>	Myon: I tried `alter database mydb set enable_sort to on;` and got "ERROR: must be owner of database mydb"
T 1567516364 18<tangara18>	postgres=# \copy membership.oldmembers from 'd:\memberparticulars.csv' csv header;ERROR:  schema "membership" does not exist
T 1567516378 18<tangara18>	it shows me membership so why it is giving me error still?
T 1567516396 18<hruske18>	tangara: \l shows databases
T 1567516403 18<hruske18>	you need to do \c membership
T 1567516413 18<hruske18>	to connect to specified database
T 1567516416 18<tangara18>	ok. let me try
T 1567516436 18<tangara18>	postgres=# \copy c:\membership.oldmembers from 'd:\memberparticulars.csv' csv header;
T 1567516437 18<Myon18>	Zaab1t: what's your actual question?
T 1567516438 18<tangara18>	like this ?
T 1567516468 18<hruske18>	tangara: no
T 1567516485 18<hruske18>	tangara: in psql prompt, you write "\connect membership"
T 1567516490 18<hruske18>	without the quotes
T 1567516493 18<tangara18>	oh ok
T 1567516539 18<tangara18>	ERROR:  extra data after last expected column
T 1567516546 18<tangara18>	it gives me error :(
T 1567516557 18<hruske18>	tangara: success, looks like it finds the table!
T 1567516566 18<Zaab1t18>	Myon: What can the owner do, that a user with all privileges cannot?
T 1567516574 18<hruske18>	now you only need to have a valid CSV for the given table
T 1567516585 18<Myon18>	I told you some things, you found some more
T 1567516591 18<tangara18>	what do you mean by valid CSV ?
T 1567516598 18<Zaab1t18>	Or really, is there any security implications of upgrading a user with all privileges on a database to the owner of said database?\
T 1567516611 18<tangara18>	It is a database that  I have downloaded from my host
T 1567516616 18<tangara18>	i mean webhost
T 1567516626 18<tangara18>	cos I need to migrate it to Postgresql
T 1567516628 18<hruske18>	tangara: "ERROR:  extra data after last expected column" is a common error when you have for example a table with 5 fields and the CSV has 7 fields
T 1567516630 18<Myon18>	they can drop all schemas
T 1567516656 18<Myon18>	Zaab1t: no implications outside that database
T 1567516657 18<tangara18>	@hruske how do i overcome the problem
T 1567516665 18<hruske18>	tangara: or if the separator is not what postgreqsl expects
T 1567516687 18<Myon18>	Zaab1t: except maybe if there's objects in there with bad permissions (like user-executable admin functions)
T 1567516689 18<tangara18>	ok. how do I make it work ?
T 1567516715 18<tangara18>	it's been 2 months I have tried to import and only now able to use the psql command line...
T 1567516718 18<Myon18>	tangara: make sure each row the the correct number of fields
T 1567516739 18<tangara18>	so, you mean I have to check row by row ?
T 1567516751 18<tangara18>	it is supposed to have all fields
T 1567516778 18<Myon18>	no context for that ERROR?
T 1567516786 18<Myon18>	there's probably a line number
T 1567516798 18<Myon18>	but yes, you have to check each row
T 1567516806 18<Myon18>	but there's thing called computer that can help you
T 1567516808 18<tangara18>	yes. it gives me line 2
T 1567516818 18<tangara18>	so, i just go check that line ?
T 1567516846 18<tangara18>	last time when i tried the other way to import, it gives me same error and it never stops
T 1567516869 18<tangara18>	is there an easier way to tackle this problem?
T 1567516877 18<Myon18>	checking line 2 doesn't seem too hard
T 1567516980 18<tangara18>	i checked that line. There isn't any missing field
T 1567517041 18<Myon18>	pastebin the table definition and the first few lines from that file
T 1567517072 18<tangara18>	table definition?
T 1567517074 18<Myon18>	plus the full error message
T 1567517080 18<tangara18>	i just merely create a table that's all
T 1567517086 18<Myon18>	\d oldmembers
T 1567517156 18<tangara18>	how do i go back to postgres# ? cos it is now at membership#
T 1567517167 18<Myon18>	why?
T 1567517174 18<Myon18>	that would be \c postgres
T 1567517182 18<Myon18>	but that's hardly useful now?
T 1567517195 18<hruske18>	tangara: membership database is where your table apparently is
T 1567517216 18<tangara18>	membership=# \d oldmmembersDid not find any relation named "oldmmembers".
T 1567517234 18<tangara18>	yes because i can't find it so i thought i have to go back to postgres
T 1567517249 18<hruske18>	what is your table named again?
T 1567517286 18<Myon18>	no wonder it took you two months to get there
T 1567517289 18<tangara18>	membership-# \d oldmembers           Table "public.oldmembers" Column | Type | Collation | Nullable | Default
T 1567517312 18<Myon18>	tangara: put the full output on http://paste.debian.net/
T 1567517321 18<tangara18>	oh..so i need to set the collation to utf-8 right?
T 1567517408 18<Myon18>	wat
T 1567517420 18<Myon18>	you need to stop doing 3 things at once
T 1567517452 18<Myon18>	if you know where the problem is, fix it
T 1567517456 18<Myon18>	if not, answer questions
T 1567517486 18<RhodiumToad18>	note that  membership-#  as the prompt means that you're part-way through entering a command
T 1567517500 18<RhodiumToad18>	the prompt is =# or => if it's waiting for the start of a command
T 1567517512 18<tangara18>	hi RhodiumToad finally see you here
T 1567517544 18*	RhodiumToad may not have time to help
T 1567517582 18<tangara18>	i need your help in a problem - I got this error from my java code :  ResultSet not positioned properly.
T 1567517609 18<dognosewhiskers18>	I'm grabbing statistics from pg_catalog.pg_database and information_schema to display how big my databases and tables are. I ran it once before and once after I had DELETEd hundreds of thousands of records. Both outputs are identical. Either all those rows took literally 0 bytes of storage (seems logically impossible to me) OR they have not updated. How do I force them to update?
T 1567517621 18<tangara18>	i searched all over the net but there is only one which suggested putting a else to see if there is any rows after going thru the resultset.next() loop
T 1567517629 18<RhodiumToad18>	tangara: that means you did something wrong in your java code. I don't do java.
T 1567517650 18<tangara18>	i doubt so ...
T 1567517659 18<RhodiumToad18>	dognosewhiskers: deleted rows are just marked dead.
T 1567517664 18<tangara18>	cos it is saying a posgresql error
T 1567517695 18<RhodiumToad18>	dognosewhiskers: vacuum will then convert them back to free space, but unless that free space happens to fall at the end of a data file, the file doesn't shrink and so no space is freed back to the OS
T 1567517710 18<RhodiumToad18>	dognosewhiskers: (but the space will be reused by future inserts)
T 1567517727 18<RhodiumToad18>	dognosewhiskers: if you absolutely need to reduce the on-disk size, there is vacuum full.
T 1567517734 18<dognosewhiskers18>	:/
T 1567517760 18<dognosewhiskers18>	?? VACUUM FULL
T 1567517760 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/sql-vacuum.html#AEN88966 :: http://rhaas.blogspot.com/2014/03/vacuum-full-doesnt-mean-vacuum-but.html
T 1567517794 18<tangara18>	no. i can't define the utf encoding...
T 1567517873 18<Myon18>	tangara: if you don't answer any questions, this will take more months to resolve
T 1567517892 18<tangara18>	@Myon what did you ask ?
T 1567517946 18<Myon18>	15:24 <Myon> pastebin the table definition and the first few lines from that file
T 1567517955 18<tangara18>	ERROR:  extra data after last expected column
T 1567517992 18<tangara18>	sorry Myon.. can I come back again cos it's been 3 hours I have been working on this...
T 1567518015 18<tangara18>	i seem to be working on this kind of things non-stop....
T 1567518021 18<Myon18>	?
T 1567518030 18<Myon18>	just show us the details of your problem
T 1567518034 18<tangara18>	and can't find a way to resolve all the coding problem...hours after hours
T 1567518039 18<tangara18>	days after days
T 1567518044 18<tangara18>	months after months
T 1567518047 18<Myon18>	15:39 <Myon> 15:24 <Myon> pastebin the table definition and the first few lines from that file
T 1567518050 18<tangara18>	may be this is not suitable for me
T 1567518077 18<tangara18>	i already showed the table definition just now
T 1567518084 18<Myon18>	no you didn't
T 1567518096 18<Myon18>	you showed the header of the definition without any content
T 1567518169 18<RhodiumToad18>	remember not to paste in channel, instead paste to a paste site (e.g. dpaste.de) and give us the url
T 1567518268 18<dognosewhiskers18>	Ah. That's better. 90 megs freed.
T 1567518300 18<tangara18>	https://pastebin.com/tE1Zi7PA
T 1567518314 18<dognosewhiskers18>	I have a feeling that there's probably a good reason this isn't done automatically, but I can't really think of why.
T 1567518315 18<tangara18>	there is no content
T 1567518325 18<tangara18>	that's all the things that was printed out
T 1567518404 18<hruske18>	tangara: did you by any chance create a table with no columns?
T 1567518408 18<tangara18>	https://pastebin.com/azUZjWjd
T 1567518435 18<tangara18>	yes. according to that tutorial columns need not be created to do the csv import
T 1567518450 18<hruske18>	that is not correct.
T 1567518480 18<tangara18>	so, you are saying i should create the columns as well before anything can be imported?
T 1567518487 18<hruske18>	correct
T 1567518505 18<tangara18>	ok. let me do it and shall i come back later or ?
T 1567518812 18<hruske18>	sure
T 1567524259 18<karlpinc18>	I'm having a brain freeze.  Is there a way to take an integer number of seconds and convert to an interval without going through a string?  The undo of: extract(epoch from someinterval)::integer
T 1567524461 18<karlpinc18>	I guess I can do: someseconds * '1 second'::interval
T 1567524479 18<karlpinc18>	Which way is "better"?
T 1567524515 18<Zr4018>	that form is correct
T 1567524592 18<karlpinc18>	Well, "(someseconds || ' seconds')::interval" works too.
T 1567524632 18<Zr4018>	it works, but it's terrible (-:
T 1567524650 18<karlpinc18>	Feels terrible.
T 1567524684 18<ilmari18>	yes, creating and re-parsing a string at runtime is just silly
T 1567524727 18<ilmari18>	'1 second'::interval (or interval '1 second') gets turned into an interval at parse time
T 1567524740 18<karlpinc18>	Ok.  *duh*
T 1567531242 20*	Disconnected (20)
T 1567531266 19*	Now talking on 22#postgresql
T 1567531266 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567531266 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:06 2019)
T 1567531266 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567531655 18<StuckMojo18>	yeah i'm kind of wondering if the numbers dpm
T 1567531661 18*	StuckMojo sign
T 1567531681 18<StuckMojo18>	don't match because they're two different queries run slightly out of sync
T 1567531734 18<davidfetter_work18>	that's a possibility. SHOW doesn't appear to have any kind of transactionality
T 1567531784 18<davidfetter_work18>	you're at least not worse off than you would be through the standard console, though
T 1567531990 18*	davidfetter_work wonders whether it'd even be possible to implement snapshots or other concurrency control in `pgbouncer`
T 1567532326 18<xocolatl18>	you mean like a transaction or something?
T 1567532439 18<davidfetter_work18>	yes
T 1567532594 18<davidfetter_work18>	It's not clear even whether the data structures SHOW reads are self-consistent, let alone whether multiple such structures could be said to have a cohesive state, as you'd be doing for a JOIN.
T 1567535144 20*	Disconnected (20)
T 1567535172 19*	Now talking on 22#postgresql
T 1567535172 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567535172 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567535172 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567535237 18<davidfetter_work18>	what are all those bouncers for?
T 1567535259 18<xocolatl18>	funneling, most likely
T 1567535274 18<lordcirth_18>	What's funneling in this context?
T 1567535303 18<xocolatl18>	reducing the number of connections that postgres actually sees
T 1567535328 18*	xocolatl has seen bouncers pointing at other bouncers, four levels deep
T 1567535356 18<lordcirth_18>	Ah, so merging requests from many clients into a few continuous connections?
T 1567535365 18<xocolatl18>	yeah
T 1567535377 18<lordcirth_18>	Is starting a new connection really expensive?
T 1567535421 18<xocolatl18>	http://ecx.images-amazon.com/images/I/61+ss5iSfaL._SL1000_.jpg
T 1567535427 18<xocolatl18>	a funnel
T 1567535434 18<Zr4018>	having max_connections be so high to support thousands of idle connections is what's expensive
T 1567535482 18<Zr4018>	xocolatl: what would be the use of having more than two levels of bouncers?
T 1567535522 18<xocolatl18>	it may have been overkill, I don't know.  the apps had millions of connections
T 1567535645 18<xocolatl18>	(popular website)
T 1567535685 18<Zr4018>	sounds like either 'website' is understating the app, or 'millions of connections' is a design mistake
T 1567535777 18<xocolatl18>	the website is kind of like youtube
T 1567535795 18<lordcirth_18>	Ah, idle connections, I see.
T 1567535812 18<energizer18>	is there a difference in what people mean by "one to many" vs "many to one"?
T 1567535821 18<xocolatl18>	energizer: perspective
T 1567535864 18<xocolatl18>	one client can have many orders / many orders can be for one client
T 1567535891 18<energizer18>	the ddl for those would be the same, right?
T 1567535902 18<xocolatl18>	yes
T 1567535906 18<energizer18>	thanks
T 1567535923 18<Zr4018>	lordcirth_: the connection being idle isn't really a problem, it's more like max_connections to 15000 instead of 150 because you have so many idle connections. iirc there's still operations that scale with max_connections.
T 1567535957 18<lordcirth_18>	Zr40, only if the connections exist, or just by setting it?
T 1567535957 18<xocolatl18>	and memory
T 1567535961 18<Zr4018>	just by setting it
T 1567535990 18<Zr4018>	if it didn't cost anything, it wouldn't need to be a setting
T 1567536030 18<xocolatl18>	this particular one would, until we get dynamic shared memory
T 1567536172 18<peerce18>	what you don't want is a 1000 of those 15000 connections making queries at the same time, that will hammer the CPU hard, and cause slower throughput then keeping the number of concurrent queries/transactions to something under 2-4X your cpu core count
T 1567536229 18<xocolatl18>	lordcirth_: what peerce just said is why I said earlier that StuckMojo's max_connections should be about 200 for his 80 cores
T 1567536251 18<lordcirth_18>	Good to know, thanks!
T 1567536283 18<xocolatl18>	it's better to have some transactions wait in line than for everyone to storm the db at once
T 1567536316 18<lordcirth_18>	If I don't have a bouncer, and I hit max_connections, clients will just wait and retry?
T 1567536342 18<xocolatl18>	no, they will fail
T 1567536351 18<peerce18>	we had a system with 1000s of terminal nodes generating short OLTP transactions.  the terminals didn't talk sql, they sent messages over a MQ style system to the app servers, the app servers would run a tunable number of worker threads that would fetch a message, process it, reply over the MQ with the result.
T 1567536391 18<peerce18>	we get the best TPS (trnasaction per second) throughput at about 2X the database server core/thread count
T 1567536471 18<xocolatl18>	my base formula is 2cpu + a handful more
T 1567538422 18<DuckyDev18>	Hi guys. do you know if there exists an offline toole like dbdiagram ( https://dbdiagram.io/d ) which draws a diagram with the same style and simplicity for linux?
T 1567538865 18<snatcher18>	DuckyDev: pgmodeler?
T 1567538909 18<jarlopez_18>	Looking at a TCP dump of a logical replication connection, it appears that RDS issues a RST,ACK packet after a few minutes of activity, whcih forces the logical replication consumer to restart its replication connection and resume replicating from the same LSN
T 1567538948 18<DuckyDev18>	I've looked at pgModeller and DBeaver which can autogenerate ER/UML Diagrams, howver they don't look very "pretty" and with the same degree of simplicity as dbdiagram.io
T 1567538978 18<jarlopez_18>	What conditions might trigger this scenario?
T 1567539036 18<xocolatl18>	??erd
T 1567539037 18<pg_docbot18>	https://wiki.postgresql.org/wiki/Design_Tools :: https://wiki.postgresql.org/wiki/Documentation_Tools
T 1567539457 18<Myon18>	jarlopez_: stupid firewalls forgetting connections after some time
T 1567539540 18<nbjoerg18>	TCP idle timer?
T 1567539550 18<nbjoerg18>	aka keep alive
T 1567539644 18<jarlopez_18>	nbjoerg: Perhaps. From the consumer, I set keep-alive to 5min. Not sure what RDS is configured to by default
T 1567539668 18<jarlopez_18>	Myon: That's a possibility. Do you have suggestions for confirming that suspicion?
T 1567539683 18<Myon18>	I don't know your or Amazon's infrastructure
T 1567539703 18<Myon18>	is the connection idle for a while when that happens?
T 1567539714 18<jarlopez_18>	This is being run across a VPC peering connection on AWS between an EC2 instance and RDS. I'll try to do some digging.
T 1567539742 18<jarlopez_18>	Myon: No, the TCP stream is active all the way to the RST,ACK packet
T 1567539753 18<Myon18>	that's weird
T 1567539817 18<RhodiumToad18>	you can get RST if one end closes the connection in the middle of receiving data
T 1567539840 18<RhodiumToad18>	check the server logs on both ends?
T 1567539905 18<Primer18>	Wait, isn't RDS one of those unknown AWS things?
T 1567539911 18<jarlopez_18>	The consumer reports a TCP error: connection reset by peer. The PG logs from RDS tell me "LOG: invalid message length" followed by "unexpected EOF on standby connection". I'm not yet sure which happens first
T 1567539940 18<jarlopez_18>	Primer: How do you mean?
T 1567539967 18<Primer18>	When I brought up a planner issue with Aurora, people were quite adamant that Aurora != postgres
T 1567539979 18<RhodiumToad18>	RDS is also not stock postgres
T 1567539990 18<RhodiumToad18>	but closer to it than aurora I believe
T 1567540011 18<RhodiumToad18>	jarlopez_: the consumer is stock postgres?
T 1567540017 18<Primer18>	yes, this was my understanding as well. So we're OK with RDS, just not Aurora? Trying to determine where the boudnaries are
T 1567540040 18<JamesHarrison18>	RDS is pretty close to stock postgres
T 1567540043 18<jarlopez_18>	RhodiumToad: The consumer is a standalone Go application using the pgx library, which speaks the replication protocol
T 1567540049 18<RhodiumToad18>	we're ok with either of them up to the point at which they diverge from community pg, at which point the answer becomes "take it up with amazon"
T 1567540080 18<RhodiumToad18>	jarlopez_: ah. then I'd take a careful look at the requests it's sending, since it could be a client-side bug
T 1567540109 18<jarlopez_18>	It's very possible that this is a networking issue. The closest to the symptoms I'm seeing is the converstation at https://www.postgresql.org/message-id/20180627131652.4zibah4oqscpkijh%40vault.lan
T 1567540114 18<Primer18>	I simply didn't think there'd be any room for gray area. Good to know there is some. I'm experimenting a lot with Aurora postgres, and have refrained from bringing things up here, given my experience last time
T 1567540138 18<jarlopez_18>	But in that email thread they seem to be getting multiple RST packets with identical sequence numbers
T 1567540153 18<schemanic[m]18>	Hey all
T 1567540160 18<schemanic[m]18>	Is it possible to write a query that takes a specific amount of time to complete?
T 1567540172 18<jarlopez_18>	Primer: That's fair. I'll continue digging and try to reach out to AWS as well
T 1567540189 18<xocolatl18>	schemanic[m]: you can make it quit after a certain time, but it will quit in error
T 1567540196 18<Primer18>	jarlopez_: What are you using for logical replication?
T 1567540197 18<RhodiumToad18>	multiple RST packets on a connection break are normal if there is data in flight at the time, the (closed) receiving end will respond to each of the in-flight packets with RST
T 1567540222 18<schemanic[m]18>	My ops team is trying to stress test a postgres server and we need each connection to be running a single query for it's test duration
T 1567540227 18<RhodiumToad18>	schemanic[m]: if you just want to delay and do nothing else, there's pg_sleep()
T 1567540256 18<jarlopez_18>	RhodiumToad: Ah, that makes sense. In this case, the consumer application restarts and re-establishes its replication connection without problem
T 1567540269 18<xocolatl18>	schemanic[m]: the pg_sleep() or pg_sleep_for() should be good enough for you
T 1567540272 18<xocolatl18>	*then
T 1567540299 18<schemanic[m]18>	That is perfect. Thank you
T 1567540320 18<jarlopez_18>	Primer: On the RDS side, I'm using the wal2json logical decoding plugin. On the consumer side, the application establishes a replication connection and starts replication on it, sending back heartbeats at regular intervals
T 1567540341 18<rosterok18>	I've got a table with about 20 million rows.  I run select * from giant_table where exists (select 1 from medium_table ...) or exists (select 1 from large_table ...).  It takes about 15 seconds when I constrain giant_table to about 1 month of data.  Are there any optimizations I can do?
T 1567540345 18<rosterok18>	I could probably add two boolean columns to giant_table, but this would be slow to add and probably not the best solution.
T 1567540492 18<RhodiumToad18>	rosterok: paste the explain analyze on explain.depesz.com
T 1567540687 18<rosterok18>	probably a silly question, but can a run that on my small test database or does it need to be the larger database
T 1567540748 18<rosterok18>	probably doesn't make much sense to do it on my small instance
T 1567540764 18<xocolatl18>	which one do you want us to optimize for?
T 1567540781 18<rosterok18>	exactly.  thanks :)
T 1567541343 18<rosterok18>	thanks a lot for the offer, RhodiumToad.  i unfortunately have to wait for this information
T 1567543243 18<depesz18>	anyone of you has ready query that will contain trigger information in "explain analyze"? if yes, could you please share yaml version of it?
T 1567543426 18<xocolatl18>	depesz: https://dpaste.de/0hqR
T 1567543439 18<depesz18>	thanks.
T 1567576502 20*	Disconnected (20)
T 1567576526 19*	Now talking on 22#postgresql
T 1567576526 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567576526 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567576527 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567576632 18<xocolatl18>	is the temp table declared as ON COMMIT DROP or something?
T 1567576649 18<andehhh18>	no
T 1567576758 18<incognito18>	andehhh: and the change are already commited from the DML side when you are executing your proc ?
T 1567576978 18<andehhh18>	incognito: yes
T 1567577010 18<incognito18>	andehhh: can you pastebin the code ?
T 1567577010 18<andehhh18>	there is a function changing the temp table and after that is a commit
T 1567577070 18<incognito18>	andehhh: is there also a commit after the insertion into the out_table ?
T 1567577160 18<andehhh18>	incognito: yes
T 1567577246 18<incognito18>	andehhh: maybe, if we see the stored procedure code ...
T 1567577258 18<andehhh18>	ahh wait
T 1567577333 18<andehhh18>	its not the insert
T 1567577354 18<andehhh18>	it's exactly at the second loop at FOR ... IN
T 1567577442 18<xocolatl18>	I have to go now.  hopefully I'll read about this on pgsql-bugs tonight
T 1567577479 18<incognito18>	my thought : if you are in read_committed transaction isolation, maybe postGIS geometries changes are not properly handled when the GIS object is toasted
T 1567577498 18<incognito18>	that's why i asked about COMMITed data
T 1567577681 18<incognito18>	as a workaround, i would try to put "SET TRANSACTION ISOLATION LEVEL SERIALIZABLE" at the beginning of the stored procedure commands
T 1567577696 18<andehhh18>	http://dpaste.com/3PD4W2B
T 1567577718 18<andehhh18>	incognito: ok will try that now
T 1567577861 18<andehhh18>	 ERROR: SET TRANSACTION ISOLATION LEVEL must be called before any query
T 1567577878 18<andehhh18>	it's right after the BEGIN now
T 1567577891 18<andehhh18>	what would be the correct position?
T 1567584933 20*	Disconnected (20)
T 1567584956 19*	Now talking on 22#postgresql
T 1567584956 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567584956 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:06 2019)
T 1567584956 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567584979 18<Zr4018>	if the constraint is declared deferrable, then validation is at least deferred to end-of-statement. If it's additionally set as deferred, then it is deferred until end-of-transaction
T 1567585127 18<petercpw18>	do constraints have be be declared as deferrable
T 1567585169 18<petercpw18>	if it's just a one-time transaction, can we still the check be deferred for non-deferrable constraints?
T 1567585555 18<petercpw18>	is it possible to modify an existing constraint
T 1567585559 18<petercpw18>	can't seem to find it in the docs
T 1567586096 18<adsf18>	when creating a view, is it possible for a column to be another query? or would this be inefficient?
T 1567586202 18<Myon18>	you can use any query in a view
T 1567586214 18<Myon18>	if the query is slow, so is the view
T 1567586247 18<adsf18>	that makes sense :) Thank you
T 1567586269 18<[patrik]18>	adsf it would probably be inefficient, depending on the query.
T 1567586294 18<[patrik]18>	adsf: what do you need to accomplish?
T 1567586321 18<adsf18>	i need a count of rows matching a condition
T 1567586339 18<adsf18>	(as well as a bunch of other things obviously)
T 1567586368 18<adsf18>	i could have a trigger write that count out to one of the tables.
T 1567586474 18<[patrik]18>	that would probably be inefficient. Like so i understand your model...you have a table A and then a view ontop of table A, and you want to include a count from table B of rows that fullfills some criteria in A?
T 1567586531 18<adsf18>	so the view currently already does a join between table a and table b (which has a fkey for a)
T 1567586540 18<[patrik]18>	ok.
T 1567586547 18<adsf18>	it selects a single row via the pkey on a
T 1567586556 18<adsf18>	b only returns aggregate information
T 1567586615 18<adsf18>	so like, avg of b.column1, sum b.column2, (then count of matching rows on b)
T 1567586619 18<[patrik]18>	ok. so you could make a bm materialized view to materialize the stuff in b. or does b change very often?
T 1567586656 18<adsf18>	so i am running materialized views in other areas and this was one of the things i was considering expanding it to
T 1567586667 18<[patrik]18>	if b is pretty static you could use a materialized view ontop of b, and refresh it however often is needed.
T 1567586674 18<adsf18>	our mat views only refresh every 15min
T 1567586693 18<adsf18>	thank you for the insights :)
T 1567586702 18<[patrik]18>	well, every 15mins might be good enough, i dont know about your requirement.
T 1567586728 18<adsf18>	fortunately its fine for us :) No crazy rush. Mat views have been quite performant for us so far!
T 1567586782 18<[patrik]18>	yeah they usually are quite handy. heh. so just slap the counter you need in b, and use it in the view you have ontop of A already.
T 1567586833 18<adsf18>	winner! Will give that a whirl.
T 1567586855 18<[patrik]18>	woohoo.
T 1567586884 18<adsf18>	usually i just do a bit of small scale testing with a regular view, then make a mat view out of it once all my code works :)
T 1567586890 18<adsf18>	makes my life a bit easier!
T 1567587638 18<raddy18>	Hello
T 1567587682 18<raddy18>	I have tried importing the csv file with \copy transaction2019_sep_1_15 from /tmp/sep1.csv, but I received invalid input file
T 1567587716 18<raddy18>	Even though the data has been taken from a different server with exact table setup
T 1567587964 18<[patrik]18>	copy table from '/tmp/blah.csv' csv
T 1567587979 18<Myon18>	what's the actual error message?
T 1567587992 18<[patrik]18>	you might need to specify the delimiter too, and if your csv file contains a header row.
T 1567588280 18<Moonsilence18>	Hi! How can I get notified about new Postgres minor releases by email?
T 1567588537 18<Berge18>	Moonsilence: https://www.postgresql.org/list/pgsql-announce/ would be a good list
T 1567588573 18<Berge18>	Or https://www.postgresql.org/news/pgsql.rss if RSS is your thing (or you setup a RSS-to-email-thing)
T 1567588614 18<macdice18>	you could use a cron job that emails you when a new version is available to upgrade via your package manager
T 1567588635 18<Moonsilence18>	Thanks!
T 1567588644 18<andehhh18>	??bug
T 1567588644 18<pg_docbot18>	https://www.postgresql.org/account/submitbug/
T 1567588678 18<Moonsilence18>	I just signed up for this one here which also contains various pg news: https://postgresweekly.com/
T 1567588693 18<Berge18>	Moonsilence: It's also posted to pgsql-announce
T 1567588702 18<Berge18>	You didn't ask about general postgres news, though (-:
T 1567588711 18<Moonsilence18>	true
T 1567589325 18<peerce18>	if you really wanna get on top of postgres, subscribe and read postgresql-general and -announce ....
T 1567589345 18<peerce18>	-general wil be full of stupid threads from idiots that need help, but that might be you some day....
T 1567590112 18<xocolatl18>	I'm trying to find the cause of an incident that happened last month where there was a big spike in buffer_content locks, but I don't know the locking at that level well enough to know what to look for.  log_lock_waits is on but there is nothing in the logs about that, and I don't see anything about relation extension either.  any ideas for what to search for?  I saw an insert take 24 seconds around that time, too
T 1567590255 18<peerce18>	wild guess from a half drunk birthday boy?   sounds like too many concurrent queries per CPU and/or storage
T 1567590269 18<xocolatl18>	happy birthday :)
T 1567591624 18<andehhh18>	xocolatl: Your bug report has been received, and given id #15990
T 1567591649 18<andehhh18>	here it is as requested ;)
T 1567591905 18<xocolatl18>	andehhh: excellent.  can't look at it until tonight, though
T 1567592162 18<lifeboy18>	??paste
T 1567592162 18<pg_docbot18>	https://explain.depesz.com/ :: https://pasteboard.co/
T 1567592163 18<pg_docbot18>	https://www.db-fiddle.com/ :: https://paste.depesz.com/
T 1567592163 18<pg_docbot18>	https://dpaste.de
T 1567592536 18<lifeboy18>	I have 5 plpgsql functions that each returns a single value.  Because the are called with a select statement, the the returned value (a counter), is a single column single row.https://dpaste.de/gk4D
T 1567592597 18<lifeboy18>	I want to call all five these in sequence in another function, but can't figure out how to do that, since I'm in effect getting 5 result sets returned.
T 1567592601 18<lifeboy18>	postgres 10
T 1567593812 18<lifeboy18>	I have created separate functions for each query so that I can return the column count for each.  I could not figure out how to run multiple queries in one function and return the result of each in the same function...
T 1567593922 18<Myon18>	using OUT parameters
T 1567593929 18<Myon18>	??out
T 1567593929 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/plpgsql-declarations.html :: https://www.postgresql.org/docs/current/static/xfunc-sql.html#XFUNC-OUTPUT-PARAMETERS
T 1567594009 18<Myon18>	or simply select imprt_imp(), flag(), import_abs(), ...;
T 1567596020 18<[Terra]18>	Hi all, I'd like your opinion. Am I right in that I try to minimize the number of idle connections to my postgres servers?
T 1567596169 18<Myon18>	these don't hurt that much
T 1567596181 18<Myon18>	you really want to minimize idle *in transaction*
T 1567596194 18<Myon18>	really idle connections just consume some RAM
T 1567596199 18<[Terra]18>	Well, developers here think it's good for their performance to write a daemon that opens 5 (or more) connections to postgres, issue several 'prepare statements' in it, and then they sit and wait for connection from their webfrontend.
T 1567596211 18<Myon18>	yes, that's standard practise
T 1567596213 18<[Terra]18>	For every app they write.
T 1567596227 18<Myon18>	and 5 is usually fine, unless you have 100 of these
T 1567596236 18<[Terra]18>	So I have hundreds of connections that do nothing.
T 1567596282 18<Myon18>	can you ask them to make that number configurable, and then set it to 2 in the config?
T 1567596325 18<Berge18>	And/or consider using a proxy such as pgbouncer.
T 1567596348 18<Berge18>	If you have considerable more connections to postgres than you have CPU cores (or IO subsystem to support the load), things will slow down.
T 1567596393 18<[Terra]18>	I'd live to move them to pgbouncer, but in their current setup it won't work since they expect that several 'prepare statements' handlers do exist.
T 1567596431 18<Berge18>	[Terra]: Why wouldn't it work?
T 1567596470 18<[Terra]18>	When pgbouncer closes the connection, the prepared statement is gone.
T 1567596531 18<[Terra]18>	And yes, I've tried asking them to lower the intial number of connections too.
T 1567596544 18<Berge18>	[Terra]: Depends on the pooling mode.
T 1567596594 18<Berge18>	(Why are they using PREPARE?)
T 1567596635 18<[Terra]18>	I've also suggested that they'd use a function instead of prepare statement
T 1567596803 18<[Terra]18>	It is manageble now, but I feel that all those idle connections are a waste of resources on the postgres server.
T 1567596826 18<Berge18>	Not if they just idle.
T 1567596837 18<Berge18>	Prepared statements aren't used for the same things as functions, though?
T 1567596963 18<mobidrop18>	they do take up connections so if you hit the max it'll block
T 1567596967 18<[Terra]18>	Isn't it so that for every connection at least 'work_mem' is reserved? Of is that work_mem not allocated until it is used?
T 1567597055 18<Myon18>	the latter, and it's even freed again immediately after each query
T 1567597064 18<Zr4018>	work_mem is actually a limit for plan nodes, so one connection can in fact use work_mem multiple times
T 1567597074 18<Myon18>	the memory usage is for internal caches ("which tables have I seen yet")
T 1567597132 18<[Terra]18>	Myon: Ah, ok. In that case it is much less of a problem than I thought.
T 1567597156 18<Myon18>	a typical number would be something like 16MB per persistent backend
T 1567597165 18<Myon18>	but it could be more if there's many tables/objects
T 1567597175 18<[Terra]18>	And by query I assume you mean transaction.
T 1567597179 18<Myon18>	query
T 1567597188 18<[Terra]18>	really?
T 1567597190 18<[Terra]18>	ok
T 1567597196 18<Myon18>	there's per-transaction memory as well, but that's usually less
T 1567597217 18<Myon18>	"select from foo order by bar" needs memory to sort in the *query*
T 1567597227 18<Zr4018>	previous $work database, which had terrible partitioning and long-lived connections, saw the syscache grow to ridiculous size
T 1567597333 18<[Terra]18>	So, an idle connection only eats one connection out of max_connections, but doesn't hurt much otherwise?
T 1567597359 18<Zr4018>	right
T 1567597366 18<Myon18>	plus it's a process that eats host RAM
T 1567597385 18<Myon18>	if you don't have RAM to have 1000 processes of that size, it's a problem
T 1567597444 18<[Terra]18>	Yes, but the code of those processes should be shared as well.
T 1567597493 18<Myon18>	the shared libraries are, the syscache is not
T 1567597529 18<Myon18>	just look at a PostgreSQL server that has been running for some time and check the RES column in ps/top
T 1567597534 18<[Terra]18>	Ah yes, and the syscache is where "which tables have I seen yet" is stored?
T 1567597536 18<Myon18>	(or RSS, that's the same)
T 1567597541 18<Myon18>	yes
T 1567597916 18<[patrik]18>	lifeboy: if you have 5 functions ... f1-f5 that returns single values you should be able to create a function "fa() returns text[]" and do a select f1()::text,f2()::text,f3()::text,f4()::text,f5():text construct in your fa() function and return all of your function values in one return value. Or am I missing something?
T 1567598007 18<[Terra]18>	Oh wow. I see several postgres processes with a RSS of 7500000+
T 1567598047 18<Myon18>	that might also be shared memory from shared_buffers
T 1567598060 18<Myon18>	sometimes that's counted in RSS as well
T 1567598081 18<Myon18>	especially if the server has started recently
T 1567598089 18<[Terra]18>	Ah, yes. Of course.
T 1567598103 18<[Terra]18>	Well this server is been up for a wile.
T 1567598130 18<[Terra]18>	Too long actually..
T 1567598368 18<[Terra]18>	Anyway, thanks for the information.
T 1567599176 18<[patrik]18>	lifeboy: if you create your single value functions like this...say f1() to fn...
T 1567599202 18<[patrik]18>	create or replace function f1() returns text as
T 1567599203 18<[patrik]18>	$$
T 1567599203 18<[patrik]18>	begin
T 1567599203 18<[patrik]18>	  return 1;
T 1567599203 18<[patrik]18>	end;
T 1567599207 18<[patrik]18>	$$ language plpgsql;
T 1567599253 18<[patrik]18>	then you can create however many functions you want to return single values...and then create a function to return all the values...fa() for example...
T 1567599283 18<[patrik]18>	create or replace function fa() returns text[] as
T 1567599284 18<[patrik]18>	$$
T 1567599284 18<[patrik]18>	declare
T 1567599284 18<[patrik]18>	t text[];
T 1567599286 18<[patrik]18>	begin
T 1567599288 18<[patrik]18>	  select ARRAY[f1()::text,f2()::text,f3()::text] into t;
T 1567599291 18<[patrik]18>	  return t;
T 1567599293 18<[patrik]18>	end;
T 1567599296 18<[patrik]18>	$$ language plpgsql;
T 1567599308 18<[patrik]18>	and have all of them returned as an array from a single function.
T 1567599312 18<Myon18>	[patrik]: stopping to wonder about weird questions if the OP doesn't follow up is a great way to save a lot of time on IRC
T 1567599327 18<[patrik]18>	Myon: ok. Im done worrying.
T 1567599333 18<Myon18>	welcome :)
T 1567599340 18<[patrik]18>	:)
T 1567599342 18<ilmari18>	[patrik]: also, please don't paste multi-line things directly in the channel, use a paste site
T 1567599348 18<ilmari18>	??paste
T 1567599348 18<pg_docbot18>	https://explain.depesz.com/ :: https://pasteboard.co/
T 1567599348 18<pg_docbot18>	https://www.db-fiddle.com/ :: https://paste.depesz.com/
T 1567599348 18<pg_docbot18>	https://dpaste.de
T 1567600014 18<Intelo18>	  created_at timestamp without time zone NOT NULL,
T 1567600014 18<Intelo18>	  updated_at timestamp without time zone NOT NULL,
T 1567600028 18<Intelo18>	should they be auto filled? if I want it, what to do?
T 1567600035 18<Intelo18>	auto filled on insertion
T 1567600043 18<Myon18>	you should use WITH timestamp in most cases
T 1567600060 18<nickb18>	with time zone*
T 1567600061 18<Myon18>	on insertion, use DEFAULT now()
T 1567600073 18<Myon18>	err yeah. "timestamptz"
T 1567600081 18<Intelo18>	using rails. have to figure out syntax there then
T 1567600100 18<[patrik]18>	intelo: define them to have a DEFAULT value. Like my_timestamp timestamptz not null default now()
T 1567600143 18<Zr4018>	rails will provide the values itself without any assistance from the database
T 1567600242 18<enoq18>	hi I've got existing int ids and I want to migrate the data to postgres
T 1567600249 18<enoq18>	what datatype do I want
T 1567600265 18<[patrik]18>	enoq: probably bigint
T 1567600269 18<ilmari18>	integer or bigint, depending on how big/many there are
T 1567600286 18<enoq18>	and then use a sequence which starts at the highest number?
T 1567600310 18<enoq18>	I need to deal with both inserts using an id and inserts creating an id
T 1567600319 18<[patrik]18>	that would probably work, but it depends on your application design.
T 1567600347 18<[patrik]18>	create a sequence starting with the highest value+1 once you are done loading the table
T 1567600350 18<enoq18>	we will try how far we get with JPA and hibernate
T 1567600372 18<Myon18>	if you have a mix of inserts that use the sequence and that don't, it will be a huge mess
T 1567600373 18<[patrik]18>	and use that sequnce like nextval('my_id_sequence'); as a value on new inserts.
T 1567600437 18<enoq18>	Myon right, hm. we have some stupid frontend logic that generates IDs from unix timestamps and we need some time to migrate that
T 1567600471 18<enoq18>	especially because the code is a mess
T 1567600489 18<[patrik]18>	enoq: if those unix magic timestamp ids doesnt mean something to your datamodel, use a proper id column and add another column to store frontend converted unix magic.
T 1567600516 18<enoq18>	yeah, unfortunately they do
T 1567600519 18<[patrik]18>	enoq: dont share an id field for sequence generated id's and unix-magic-id's
T 1567600549 18<Myon18>	or start the generated IDs at 1, it'll be some time until they conflict with the timestamps
T 1567600564 18<Myon18>	somewhat ugly, but should work
T 1567600674 18<[patrik]18>	or you could decode those unix-magic timestamps...are they only epoch timestamps or are there some more magic applied?
T 1567600759 18<[patrik]18>	if they are just epoch timestamps you can keep generating such ids
T 1567600770 18<[patrik]18>	in postgres as well... SELECT extract(epoch from now());
T 1567600954 18<[patrik]18>	so then the id field would be declared as --- id bigint not null default extract(epoch from now())
T 1567601260 18<[patrik]18>	what might end up sucking though is uniqueness if you have 2 sources of generating ids.
T 1567601305 18<Myon18>	or insert two rows in one second
T 1567601365 18<[patrik]18>	it's hibernate so that won't happen ;)
T 1567601404 18<[patrik]18>	is there only second resolution on epoch timestamps?
T 1567601420 18<nickb18>	I'd actually use a sequence starting at 0 with negative increment to avoid collisions
T 1567601437 18<[patrik]18>	nickb: that would work.
T 1567601469 18<Myon18>	[patrik]: well it's stored in an int
T 1567601476 18<[patrik]18>	true
T 1567601507 18<Myon18>	pg_typeof(extract(epoch from now())) says double precision
T 1567601520 18<Myon18>	1567601514.54378
T 1567601539 18<Myon18>	I'll just stop worrying here
T 1567603513 18<enoq18>	are types in postgres global? can they clash with existing types from your db?
T 1567603520 18<enoq18>	asking because of my enum named role
T 1567603566 18<RhodiumToad18>	role is a keyword
T 1567603604 18<RhodiumToad18>	though it's not a reserved keyword, so you can use it as a type name
T 1567603633 18<RhodiumToad18>	types are schema-qualified and use the search path; the builtin types are in the pg_catalog schema which is typically first
T 1567603655 18<RhodiumToad18>	however, some type names are special syntax rather than just plain names
T 1567603853 18<enoq18>	so using role is fine?
T 1567603918 18<RhodiumToad18>	it will work, yes
T 1567603958 18<enoq18>	thank you
T 1567603985 18<enoq18>	is there a way to reuse common fields btw? stuff like Address where you don't necessarily want a relation
T 1567604041 18<RhodiumToad18>	domains?
T 1567604056 18<RhodiumToad18>	what exactly do you want to reuse?
T 1567604074 18<enoq18>	I don't want to type the fields 3 times in my CREATE TABLE statements
T 1567604097 18<enoq18>	fields such as: street, zip
T 1567604163 18<Myon18>	create table foo (LIKE bar), but no idea if you can combine that with other fields
T 1567604239 18<davidb211118>	Hi all! Anyone already did some rust client that connects to postgresql with Tls enabled?
T 1567604654 18<[patrik]18>	enoq: store them in a jsonb then. hehe.
T 1567604682 18<enoq18>	[patrik] not today :)
T 1567604688 18<[patrik]18>	oh
T 1567604698 18<enoq18>	migrating off nosql
T 1567604856 18<[patrik]18>	but but but...Postgres runs SQLs with operators to query jsonb attributes :) its either that or repeat your fields :)
T 1567605020 18<ilmari18>	davidb2111: https://crates.io/crates/postgres claims to support tls
T 1567608965 20*	Disconnected (20)
T 1567608990 19*	Now talking on 22#postgresql
T 1567608990 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567608990 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567608990 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567609010 18<anykey18>	I could even do multiple "api" schemas and restrict these according to these roles
T 1567609031 18<anykey18>	I just wanted to know whether that is frowned upon or not
T 1567609061 18<[patrik]18>	that is what i like with using views, and if your are using login roles you can limit data visibility in the views depending on which user is logged in.
T 1567609115 18<[patrik]18>	what is frowned upon by some, is hailed as beauty by others. like much of things it depends on when you were educated :)
T 1567609171 18<anykey18>	from interaction with colleagues, I got the impression that "api" schemas are not common. I have only begun doing SQL for real in 2014. Five years later, I find that very few people want to do SQL in depth
T 1567609179 18<[patrik]18>	some are "database is just a datastore" and hates db-based logic (becuase they dont understand it)..if you understand db-based logic, it makes all the difference, and a cleaner design if you ask me. So its all dependant on who you ask if its frowned upon :)
T 1567609226 18<anykey18>	yes, but I am in the "PostgreSQL is a data processing framework and offers amazing capabilities out of the box" camp of people
T 1567609258 18<[patrik]18>	anykey: then by all means go ahead and do your schema-based api design :)
T 1567609278 18<anykey18>	I will try my best.
T 1567609402 18<JordiGH18>	Oh, alright, I figured out how to use netstat. Found my processes hammering my db.
T 1567609415 18<[patrik]18>	anykey: what is the api for? or will DOD kill me if you tell? hehe
T 1567609428 18<rosterok18>	RhodiumToad: I finally got the explain up:  https://explain.depesz.com/s/cAuZ
T 1567609511 18<anykey18>	[patrik]: groupware, team management, and a glorified to-do list with some company specific fields
T 1567609534 18<anykey18>	[patrik]: some file mangement too
T 1567609535 18<RhodiumToad18>	rosterok: here's a trick to try: in the query, add a condition  AND refs.start_timestamp < '2019-09-01 00:00:00'
T 1567609562 18<RhodiumToad18>	rosterok: and try the explain again, with mergejoin still disabled
T 1567609594 18<anykey18>	[patrik]: stuff that is mostly boring, actually :)
T 1567609603 18<[patrik]18>	anykey: if you do  such schema based api, i would suggest you look into listen/notify to make your api-consumers aware of changes too.
T 1567609639 18<anykey18>	[patrik]: hm, never used that before. Is postgresql now doing job queueing as well?
T 1567609838 18<[patrik]18>	anykey: well i wouldnt say job queueing, but listen/notify can send a payload to a client. thing is if you read about it you'll see that it contains a few gotchas, but it might be useful in a scenario like yours.
T 1567609869 18<anykey18>	I have noted it for research.
T 1567609871 18<anykey18>	thanks.
T 1567610015 18<[patrik]18>	anykey: if your api-consumers can change stuff its a good way of doing the other api-consumers aware of that change. if your api is fed from some central thing, and the clients will only consume stuff its not as useful.
T 1567610073 18<ZackTech201918>	Thanks guys I have fixed the problem
T 1567610082 18<anykey18>	I think(!) it's just periodic connect-read
T 1567610115 18<anykey18>	but they can of course change their data as well
T 1567610134 18<anykey18>	I will look into that topic.
T 1567610139 18<[patrik]18>	anykey: then its not needed, since for listen/notify to work you need to stay connected and "subscribe to a channel"...if you connect-read-disconnect it wont work.
T 1567610166 18<anykey18>	ah, ok.
T 1567610176 18<[patrik]18>	yeah check it out. its a cool tool in your "out of the box package" :)
T 1567610190 18<rosterok18>	RhodiumToad: here's the new explain:  https://explain.depesz.com/s/Np5u.  sorry about the wait.  it takes me forever to go through someone to get these.
T 1567610199 18<anykey18>	I was happy to see it gain procedures with transaction control
T 1567610241 18<[patrik]18>	yeah transaction control is a big improvement. i didnt get to use it tho, since im still on older stuff.
T 1567670323 20*	Disconnected (20)
T 1567670346 19*	Now talking on 22#postgresql
T 1567670346 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567670346 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567670347 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567670840 18<enoq18>	asked yesterday as well: I have a REST API that allows you to create db entries with or without ID; is there a way to combine this with an automatic ID if none is provided?
T 1567670881 18<Myon18>	??identity
T 1567670882 18<pg_docbot18>	https://wiki.postgresql.org/wiki/Identity_Guidelines :: https://www.depesz.com/2017/04/10/waiting-for-postgresql-10-identity-columns/
T 1567670882 18<pg_docbot18>	https://wiki.postgresql.org/wiki/Logo :: https://blog.2ndquadrant.com/postgresql-10-identity-columns/
T 1567670882 18<enoq18>	what I've read so far is that people use sequences and set it to the maximum value of the table upon generation
T 1567670885 18<Renter18>	enoq: well, there are a lot of layers there, if you just want an unique identifier you can easily have a SERIAL field
T 1567670912 18<Renter18>	But if you want something like having an unique identifier that can be set by the user/remote client, that's slightly different
T 1567670921 18<Myon18>	enoq: the newer "identity" layer has better protection against messup than the plain "sequences" one
T 1567670929 18<Myon18>	but it's still a mess if you mix them
T 1567670975 18<enoq18>	people import older data over the rest API with existing ids
T 1567670985 18<enoq18>	some create new entries over a web ui
T 1567670995 18<Renter18>	uh
T 1567670998 18<peerce18>	so then you just need to set the sequence to the max(id) after the import operation
T 1567671015 18<Renter18>	I'd take the approach that you generate new IDs anyway, then have an old_id from the old system
T 1567671016 18<strk118>	I'm trying to understand _why_ my test finds pg_class.reltuples updated, when it does not run VACUUM or ANALYSE or CREATE INDEX on the target table, how to tell what's triggering an update of pg_class.reltuples ?
T 1567671019 18<Renter18>	because you will f things up otherwise
T 1567671039 18<enoq18>	peerce that makes sense
T 1567671062 18<enoq18>	thanks :)
T 1567671086 18<peerce18>	with newer versions of postgres, it *is* preferable to use the IDENTITY stuff instead of SERIAL.   SERIAL was the legacy way of doing an auto ID.
T 1567671139 18<Renter18>	peerce: oh, that's interesting, I went back to postre this year and haven't run into IDENTITY yet
T 1567671139 18<peerce18>	http://www.postgresqltutorial.com/postgresql-identity-column/
T 1567671152 18<peerce18>	its a new feature in pg10+
T 1567671161 18<Myon18>	it's still a sequence under the hood
T 1567671177 18<peerce18>	yeah
T 1567671211 18<Myon18>	strk: autovacuum instead of explicit vacuum?
T 1567671321 18<strk18>	how to tell when did autovacuum kick-in ?
T 1567671399 18*	strk enabling autovacuum logging
T 1567671543 18<strk18>	LOG:  parameter "log_autovacuum_min_duration" changed to "0"
T 1567671563 18<strk18>	I see a single log line about autovacuum after that: LOG:  automatic vacuum of table "postgres.pg_catalog.pg_shdepend": index scans: 1
T 1567671583 18<strk18>	nothing in between the statements that do see pg_class.reltuples changed
T 1567671619 18<strk18>	was it an extension that allowed me to see _all_ statements (even those from plpgsql functions) ?
T 1567672715 18<afancy18>	Hi, is it possible to hide "information_schema"? thanks
T 1567672815 18<Myon18>	basically no
T 1567672823 18<Myon18>	what problem do you want to solve?
T 1567673022 18<afancy18>	Myon: I am using Hue to let our customers to browser postgresql database. But, information_schema is showing there. It would be nice if it can be hidden.
T 1567673042 18<Myon18>	they will have to live with that
T 1567673067 18<Myon18>	if you use pgadmin/omnidb, it won't appear under "schemas" but separately under "catalogs"
T 1567673070 18<afancy18>	Myon: Okey. I have revoke the select permission on the tables
T 1567673131 18<afancy18>	Myon: well, i am building a data lake, the underlying database can be any, including Hive, PostgreSQL, MySQL, etc. That is why I am using Hue.
T 1567673445 18<peerce18>	'data lake' ?   :-/
T 1567673454 18<peerce18>	sounds more like a slough.
T 1567673546 18<afancy18>	peerce: well, whatever
T 1567673595 18<peerce18>	why is this "Hue" exposing the information schema ?
T 1567673612 18<peerce18>	thats, btw, something *all* SQL spec compliant databases are supposed to have.
T 1567676849 18<incognito18>	peerce: plpgsql expert ? how to make a holdable dynamic cursor ?
T 1567676974 18<peerce18>	I'm not even sure what you mean by a 'holdable dynamic cursor' ??
T 1567677203 18<peerce18>	you mean like when the plpgsql function returns a refcursor and the client uses FETCH etc to read from that ?
T 1567677285 18<peerce18>	pretty much all the allowable behavior is detailed here, https://www.postgresql.org/docs/current/plpgsql-cursors.html
T 1567677433 18<peerce18>	its the 'dynamic' part I'm not sure what you mean.
T 1567677492 18<peerce18>	holding onto a cursor outside the transaction is ugly, too, it requires the server to materialize the whole query and store it somewhere so you can fetch it at your leisure.   thats not terribly efficient
T 1567677804 18<incognito18>	peerce: yes, let's say : a holdable refcursor
T 1567677835 18<incognito18>	i cannot loop if there is a DML inside it
T 1567677860 18<incognito18>	no known snapshot before at the start of the 2nd iteration of the loop
T 1567677868 18<incognito18>	-before
T 1567677968 18<peerce18>	um, DML in a cursor??  a  cursor is a represntation of a recordset... bunch of rows with the same fields.   how could that contain DML ??
T 1567677992 18<incognito18>	peerce: insert in another table
T 1567678025 18<incognito18>	 the DML and COMMIT cause the transaction block to forget the cursor; so the 2nd loop hands
T 1567678027 18<incognito18>	hangs*
T 1567678028 18<peerce18>	how is an insert inside a cursor ?
T 1567678043 18<incognito18>	FOR rec in EXECUTE '' LOOP
T 1567678048 18<peerce18>	and frankly, if you're doing that level of insanity with functions, you deserve the dogbite
T 1567678053 18<incognito18>	insert into hop () values ();
T 1567678054 18<incognito18>	commit;
T 1567678056 18<incognito18>	end loop;
T 1567678077 18<incognito18>	it's a regular way of using procedural language
T 1567678107 18<incognito18>	if the cursor was not dynamic, it was not a problem, i think
T 1567678122 18<peerce18>	my last $job, we moved most of our complex business logic OUT of pl*** and into the app servers, and only used pl*** for performance reasons
T 1567678177 18<peerce18>	we'd been very tied to complex pl*** (mostly, Oracle pl/sql) for 10+ years and moving most of the logic OUT of the database and into the appserver layer gave us BETTER performance and better debugability
T 1567678179 18<incognito18>	it's not really my goal here
T 1567678221 18<incognito18>	peerce : and better interoperability with the IS
T 1567678221 18<peerce18>	just saying, getting too complicated bites you in the ass.
T 1567678263 18<incognito18>	well, if you see something like holdable refcursor, let me know
T 1567678267 18<incognito18>	thank you indeed
T 1567678297 18<peerce18>	yeah, don't wait up for that.
T 1567678305 18<incognito18>	i know
T 1567678325 18*	incognito will not wait up to thank you ^^
T 1567679492 18<rokshis18>	How do I make a statement of "UPDATE table SET something = var ON CONFLICT(constraint_name) DO NOTHING"
T 1567679499 18<rokshis18>	is that possible?
T 1567679503 18<RhodiumToad18>	no
T 1567679514 18<rokshis18>	what's the alternative
T 1567679572 18<peerce18>	an UPDATE that might trigger a contraint?   huh.     probably wrap it in a savepoint, if you need to continue with a larger transaction
T 1567679600 18<peerce18>	??savepoint
T 1567679601 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/sql-savepoint.html
T 1567679605 18<rokshis18>	it triggers a partial unique index
T 1567679625 18<rokshis18>	i don't know, if I said it correctly, by saying "constraint_name"
T 1567679628 18<deebo18>	isnt the default rollback, aka do nothing?
T 1567679646 18<peerce18>	well, rollback is do nothing for the whole transaction
T 1567679659 18<rokshis18>	deebo, yes. But I want to do nothing only with the rows, that actually trigger the constraint, and other ones to update
T 1567679666 18<rokshis18>	it's a batch update.
T 1567679674 18<peerce18>	ah, thats problematic.
T 1567679696 18<peerce18>	use a WHERE clause to honly update the rows that are ok to update this way?
T 1567679705 18<rokshis18>	i see, well, still thanks. Just wondered if there was any semantics for that
T 1567679711 18<rokshis18>	peerce, yep, will probably do something like that
T 1567679713 18<rokshis18>	will figure it out
T 1567679715 18<rokshis18>	thanks guys
T 1567690842 18<xocolatl18>	I'm trying to understand the buffer_mapping lock
T 1567690864 18<xocolatl18>	is it looking up what buffer has the page, or is it running clocksweep to find a buffer to put the page in?
T 1567690920 18<xocolatl18>	aside from the declarations, I don't see where the lock is actually taken.  I must be searching the wrong things (buffer_mapping, BufMappingLWLockTranche, and LWTRANCHE_BUFFER_MAPPING)
T 1567691074 18<ufk18>	Hi! :) how can I get a value from a timestamptz column without the +?? (+03 or +00) at the end ? I want to get the value in a readable format
T 1567691163 18<xocolatl18>	in which time zone?
T 1567691228 18<ufk18>	currently when I select the column i see that it has +03 at the end
T 1567691229 18<xocolatl18>	but if you think that's unreadable, you should probably just use to_char() and format it yourself
T 1567691250 18<Myon18>	::timestamp
T 1567691282 18<Myon18>	just *don't* change the datatype of the column, tz is the best default
T 1567691286 18<ufk18>	awesome thanks
T 1567692220 18<xocolatl18>	looks like it's for loading a new page
T 1567692241 18<xocolatl18>	so if I'm seeing buffer_mapping contention, that should indicate that shared_buffers is too small, correct?
T 1567692299 18<Myon18>	or too large maybe?
T 1567692324 18<Myon18>	the clock sweep algorithm isn't especially smart, from what I got
T 1567692360 18<xocolatl18>	I *think* the buffer_mapping lock is only held after the target buffer is found
T 1567693456 18<ufk_18>	is there some kind of select I can do to see the last query that was executed (not in that same connection) ?
T 1567693475 18<Myon18>	select * from pg_stat_activity;
T 1567693484 18<plujon18>	I've only ever used simple data types in my postgressing.  But I now interact with a server that returns largish json blobs to me with unique keys identifying each blob.  I'm contemplating whether I should stick this data in postgres, or use another database for it, oriented around key/value json data.
T 1567693484 18<ufk_18>	nah it's quick it's already finished
T 1567693529 18<plujon18>	I don't really need to query into the blobs; mainly I need to retrieve them by key and convert them to [ruby] objects on demand.
T 1567693546 18<plujon18>	How would a postgressor go about this?  hstore?
T 1567693553 18<Myon18>	then store them as jsonb
T 1567693570 18<Myon18>	no need to squeeze them into a normalized schema
T 1567693619 18<plujon18>	Myon: Thanks; I should have checked the manual.  I see 8.14 describes JSON types!
T 1567693650 18<Myon18>	if you absolutely don't care about the content, you could also use "text" or "bytea"
T 1567693677 18<plujon18>	Would there be any benefit to using text or bytea?  Faster/smaller?
T 1567693761 18<plujon18>	Meh; I'll just give jsonb a try; thanks!
T 1567694402 18<ratrace18>	Hi. Looking for options for selecting randomly ordered N rows from a set with: a) lots of gaps in pkey so pkey can't be used to "hack" random,  b) heavily toasted table, c) use case that results with hundreds of thousands of tuples scaned sequentially when "ORDER BY RANDOM()" is used. One idea is to
T 1567694432 18<ratrace18>	have a separate table with only two columns, a pkey/fkey into proper table, and a random number by which the table would be joined and orderd.
T 1567694445 18<Myon18>	??tablesample
T 1567694445 18<pg_docbot18>	http://www.dbazine.com/db2/db2-disarticles/zikopoulos2 :: https://wiki.postgresql.org/wiki/TABLESAMPLE_Implementation
T 1567694445 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/tablesample-method.html :: https://blog.2ndquadrant.com/tablesample-in-postgresql-9-5-2/
T 1567694453 18<Zr4018>	??random
T 1567694453 18<pg_docbot18>	http://blog.rhodiumtoad.org.uk/2009/03/08/selecting-random-rows-from-a-table/ :: http://www.depesz.com/2007/09/16/my-thoughts-on-getting-random-row/
T 1567694453 18<pg_docbot18>	https://blog.2ndquadrant.com/tablesample-and-other-methods-for-getting-random-tuples/ :: https://www.postgresql.org/docs/current/static/functions-math.html
T 1567694511 18<Myon18>	a separate table seems overcomplicated
T 1567694533 18<Myon18>	if the table is heavily toasted anyway, you basically already have a separate table
T 1567694721 18<ratrace18>	tablesample is of no use to me, I have very complex WHERE clauses and it apparently doesn't work with that
T 1567694758 18<ratrace18>	so anway, my idea was a separate table which I'd update this "random_order" column with a random value on each update. it's not true random ordering for each select, but close enough for the use case
T 1567694782 18<ratrace18>	and such a table with only two integer columns would pack a lot of rows per page so I could even periodically "shuffle" them
T 1567694788 18<Myon18>	you could have that column on the original table as well?
T 1567694812 18<ratrace18>	Myon: I could but updating rows would update whole 8k page just because one row occupies it
T 1567694835 18<ratrace18>	with a separate table, unless I'm totally misunderstanding how paging works with postgres, I could have many rows per page
T 1567694851 18<ratrace18>	and thus have lesser impact from periodically updating all of them to "reshuffle"
T 1567694857 18<Myon18>	there should be at least 4 rows by page
T 1567694873 18<Myon18>	check how large your rows are actually, you said there's toasting
T 1567694902 18<ratrace18>	lemme read through the second group of links first
T 1567694924 18<yoshie902a18>	Would like some help elminating an Or clause in this sql statement.. https://stackoverflow.com/questions/57808000/postgres-eliminate-multiple-conditions-and-reduce-to-one-without-an-or-stateme
T 1567694945 18<ratrace18>	yeah the second group is based on gapless ID... so no good either.
T 1567695041 20*	Disconnected (20)
T 1567695241 19*	Now talking on 22#postgresql
T 1567695241 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567695241 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567695241 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567695247 18<ratrace18>	well the separate table approach would simplify the query to only one ORDER BY random_column with that table joined to the main query.
T 1567695271 18<ratrace18>	with some fkeys applied, I don't foresee any bugs but the reason I came here was to get this idea shot down so pls, bring it on :) and thanks ;)
T 1567695312 18<ratrace18>	periodically reshuffling to avoid alwyas the same ordering of what's supposedly random ordering over a period of time
T 1567695323 18<ne2k18>	is there a way to get psql to pretty print a query without creating a view of it and inspecting the source?
T 1567695371 18<Myon18>	ne2k: pgformatter
T 1567695387 18<RhodiumToad18>	ratrace: but there'd be no benefit in join+order by compared to just order by random()
T 1567695413 18<ratrace18>	RhodiumToad: I'd avoid using random() at all
T 1567695420 18<RhodiumToad18>	so?
T 1567695441 18<RhodiumToad18>	you wouldn't gain any performance and you'd lose the actual randomness
T 1567695532 18<ratrace18>	RhodiumToad: not sure I follow. Right now I have order by random which breaks indexing and causes full table scan so the engine could sort through all the results by where clause and select top 4 with the limit
T 1567695544 18<ratrace18>	 /order by random()/
T 1567695562 18<RhodiumToad18>	order by random doesn't "break indexing" when you have a where clause
T 1567695571 18<ne2k18>	Myon, so not possible somehow while editing a query in psql?
T 1567695603 18<ne2k18>	might be quite nice to add as a feature, press some shortcut key and it does whatever gets done internally when you create a view and print it out
T 1567695631 18<ratrace18>	RhodiumToad: I expressd myself badly. What I meant was, I don't want it to scan through thousands of rows if I want only 4 out of the result set
T 1567695653 18<RhodiumToad18>	right, but you usually can't avoid that
T 1567695669 18<ratrace18>	I don't have the seq scan when I join this separate table.
T 1567695696 18<RhodiumToad18>	that doesn't mean the query is any faster
T 1567695749 18<ne2k18>	how can you have the top four of random order without finishing ordering all of them?
T 1567695754 18<RhodiumToad18>	if the where clause is selecting 10k rows from 10m, then on average you'd have to scan 1000 rows from your separate random table for every row returned, if the plan is using that table to drive the join
T 1567695852 18<ne2k18>	Myon, oh, I also see there is a thing on pg_formatter about using it to format within an editor, and there is a thing in psql to use an editor to edit a line, isn't there? so maybe those things could be combined
T 1567695857 18<ratrace18>	RhodiumToad: hrm...
T 1567695873 18<Myon18>	ne2k: I'd think so, yes
T 1567695890 18<ne2k18>	Myon, "some assembly required" -)
T 1567695894 18<Myon18>	ne2k: in vim, it's basically !Gpg_formatter<enter>
T 1567695901 18<ne2k18>	ARGH, my eyes!
T 1567695926 18<Myon18>	or perhaps try \setenv EDITOR pg_formatter
T 1567695969 18<ratrace18>	RhodiumToad: yea I see it'll still have to join 10k rows which is not too far performance wise from a simple random()
T 1567695980 18<ratrace18>	(called per row for each of the 10k)
T 1567696074 18<Myon18>	\setenv EDITOR pg_format
T 1567696076 18<Myon18>	\e
T 1567696084 18<Myon18>	ne2k: ^ prints the query in psql
T 1567696099 18<ratrace18>	RhodiumToad: I'll guess I'll have to solve this at a totally different layer. one idea was to cache the list of resulting IDs in, say, redis, for each where-clause combo (few thousands but not infinite), and then randomize 4 ids from the list and use it in query ala   where pkey in (1, 2, 3, 4);
T 1567696116 18<Myon18>	http://paste.debian.net/1098930/
T 1567696132 18<ilmari18>	doesn't \e expect the editor to modify the file in-place? whereas pg_format spits it out on stdout?
T 1567696147 18<Myon18>	yeah
T 1567696165 18<Myon18>	but maybe that's ok - the problem is that it re-executes the query as well
T 1567696167 18<ilmari18>	pg_format could do with an -i option to edit it in-place
T 1567696507 18<ne2k18>	ilmari: yes, I see that it runs the query again. nice, though, and fine for a simple select
T 1567696843 18<davidfetter18>	xocolatl, thought this might interest you.  https://github.com/ULB-CoDE-WIT/MobilityDB/
T 1567697897 18<xocolatl18>	Myon: it seems buffer_mapping is NOT held during clocksweep, so I still think contention on it only means shared_buffers is too small
T 1567703679 20*	Disconnected (20)
T 1567703706 19*	Now talking on 22#postgresql
T 1567703706 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567703706 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567703706 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567703707 18<Slade18>	nice :)
T 1567703730 18<Myon18>	select ('today'::timestamp at time zone 'America/Chicago') at time zone 'America/New_York';
T 1567703750 18<Myon18>	I guess that's what you wanted
T 1567703805 18<RhodiumToad18>	that's probably wrong
T 1567703823 18<Myon18>	the last one was just for testing here
T 1567703873 18<RhodiumToad18>	if you want to know what time it was in new york at the start of what is the current day in chicago, you have to do this:
T 1567703925 18<RhodiumToad18>	date_trunc('day', now() at time zone 'America/Chicago') at time zone 'America/Chicago' at time zone 'America/New_York'
T 1567703945 18<Slade18>	no i want to know the start of date x, at timezone y  where x and y come from different fields. in different tables :P
T 1567703977 18<RhodiumToad18>	x is what data type?
T 1567703995 18<Slade18>	x is a timestamptz (currently always set to UTC)
T 1567704008 18<Slade18>	but i think Myon's first query shows me exactly how to do it
T 1567704012 18<RhodiumToad18>	timestamptz values don't store a timezone
T 1567704044 18<Slade18>	they store a +00:00 dont they?
T 1567704054 18<RhodiumToad18>	so the question is: at absolute time X, in timezone Y, what is the absolute time of the start of the current day?
T 1567704056 18<Slade18>	whicih i guess is different from a timezone
T 1567704060 18<RhodiumToad18>	they do not
T 1567704082 18<RhodiumToad18>	date_trunc('day', X at time zone Y) at time zone Y   is what you need
T 1567704136 18<Slade18>	timestamp with time zone is the data type.. hum..
T 1567704143 18<Slade18>	ok sounds good :)
T 1567704190 18<RhodiumToad18>	pg's implementation of timestamp with time zone is not quite like how the sql spec defines it, but has the merit of being actually useful
T 1567704228 18<hoe`18>	"(report.customer_server_id IN $2) AND" hmm, why would that give me a syntax error?
T 1567704290 18<RhodiumToad18>	because IN expects a following list or subquery
T 1567704304 18<RhodiumToad18>	and parameters can't be lists (though they can be arrays)
T 1567704331 18<RhodiumToad18>	if $2 is an array, then you wanted  (report.customer_server_id = ANY ($2))
T 1567704349 18<Slade18>	so, generally if you're storing startdate/enddate. is storing it as a range preferable?
T 1567704370 18<RhodiumToad18>	depends how you plan to query it
T 1567704383 18<Slade18>	or is it mostly irrelevant
T 1567704430 18<Slade18>	are there advantages to keeping them separate?
T 1567704449 18<RhodiumToad18>	sometimes, yes
T 1567704471 18<RhodiumToad18>	remember you can always do a functional index on the range if you need one
T 1567704579 18<hoe`18>	thanks RhodiumToad!
T 1567704617 18<Slade18>	yea just trying to come up with a standard way of showing it all
T 1567704870 18<harks18>	Is there something like "any(t.col) ~ 'myre.*'"?
T 1567704897 18<RhodiumToad18>	where t.col is an array column?
T 1567704902 18<harks18>	yes
T 1567704919 18<RhodiumToad18>	not without defining a custom operator for commuted regexp match, or unnesting the array
T 1567704921 18<harks18>	Do I have to unnest?
T 1567704928 18<harks18>	I see
T 1567704966 18<RhodiumToad18>	the custom operator isn't hard
T 1567705272 18<Slade18>	heh "operator does not exist: text + text"  always forget whats allowed on various  database types :P
T 1567705392 18<[patrik]18>	|| is a better operator than + for text
T 1567705393 18<[patrik]18>	:)
T 1567712717 20*	Disconnected (20)
T 1567712743 19*	Now talking on 22#postgresql
T 1567712743 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567712743 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567712744 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567712806 18<localhorse18>	RhodiumToad: i found these outdated instructions: https://wiki.openstreetmap.org/wiki/PostGIS/Installation#openSUSE_13.1_2 and this https://download.opensuse.org/repositories/Application:/Geo/openSUSE_Leap_15.0/  after replacing the link it seems to work now
T 1567712825 18<RhodiumToad18>	what version of postgis are you getting from that though?
T 1567712853 18<muzakmonk1118>	I am doing reading on partitioning and would like to better understand how partition pruning is done, what role statistics has in the execution of queries that don't include the column(s) partitioned on, and other related information. Any recommended reading?
T 1567712858 18<localhorse18>	RhodiumToad: postgresql10-postgis-2.5.2-lp150.1.43.x86_64
T 1567712918 18<RhodiumToad18>	ok, so that's one point release behind
T 1567712946 18<RhodiumToad18>	(2.5.3 is the current 2.5 release)
T 1567712961 18<localhorse18>	RhodiumToad: i want to use the same postgis version as heroku because i'm deploying to that. but i can't find which version they use for postgres 10
T 1567712982 18<RhodiumToad18>	if you have it installed you can query pg_extension
T 1567713009 18<localhorse18>	ah right
T 1567713116 18<localhorse18>	it says extversion 2.5.2, too
T 1567713144 18<localhorse18>	extversion is the column that represents the version, right?
T 1567713178 18<RhodiumToad18>	yes
T 1567713250 18<muzakmonk1118>	Are the statistics for all child tables/partitions examined at planning time and different plans potentially used for each child table?
T 1567713291 18<RhodiumToad18>	yes
T 1567713335 18<localhorse18>	RhodiumToad: is the default template used for createdb using utf8?
T 1567713352 18<RhodiumToad18>	that depends how you ran initdb
T 1567713389 18<RhodiumToad18>	if initdb defaulted to UTF8 (based on locale environment vars) or you explicitly did initdb -E UTF8 then yes
T 1567713405 18<RhodiumToad18>	psql -l  and look at the encoding of template1 to see
T 1567713491 18<localhorse18>	ah yes, it's utf8
T 1567713497 18<localhorse18>	btw, when is template0 used?
T 1567713513 18<RhodiumToad18>	when restoring from dumps with -C or when you explicitly specify it
T 1567713531 18<RhodiumToad18>	which you're forced to do if you're changing the encoding or collation in createdb
T 1567713589 18<RhodiumToad18>	the intent is that template0 should remain pristine, whereas you're allowed to modify template1 in order to set up things (such as extensions) that you want to be available in all new dbs by default
T 1567713618 18<RhodiumToad18>	(and to encourage this, template0 disallows connections by default)
T 1567713691 18<localhorse18>	ah thx, makes sense. you're the best :)
T 1567713739 18<RhodiumToad18>	there's a special exception that allows you to use template0 as template even when changing the encoding or collation, which is not allowed for any other template
T 1567713763 18<RhodiumToad18>	this is because the pristine state of template0 does not contain anything which is encoding or collation dependent.
T 1567714440 18<localhorse18>	ah
T 1567714806 18<muzakmonk1118>	I've got many tables with (client_id, id) as the primary key. Postgres throws an error if I try to do a hash partition on site_id, as is documented, but is there any workaround for this?
T 1567714826 18<muzakmonk1118>	I can partition on client_id, but that won't help in some cases where there is a single client on the entire database. We already have 60-100 databases that have 1 to several hundred clients and want to keep standardized schema across client databases.
T 1567714878 18<peerce18>	so id isn't unique ?
T 1567715040 18<Myon18>	lechner: you still didn't understand how a hash function works
T 1567715058 18<Myon18>	lechner: this algorithm *is* implemented, but the problem is elsewhere
T 1567750304 20*	Disconnected (20)
T 1567750330 19*	Now talking on 22#postgresql
T 1567750330 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567750330 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567750330 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567751612 18<wkalt18>	RhodiumToad: thanks, will remember that
T 1567752521 18<Myon18>	lechner: from normalized version components, yes. That part is defined nowhere, but I already told you that dropping leading zeros is probably all that's needed
T 1567754996 18<mazula18>	hi what is wrong with my code? https://gist.github.com/minas-tirith/16ca484531cbbde3009c1f7c4624766e
T 1567755079 18<Myon18>	mazula: you are creating the PK twice
T 1567755097 18<Myon18>	but the ERROR in there is because you execute the INSERT twice
T 1567755146 18<peerce18>	yeah, if yoou declare somethign PRIMARY KEY, it createsa  unique index already, so creating another one is just redundant
T 1567755185 18<mazula18>	I created two insert ?
T 1567755187 18<mazula18>	at once?
T 1567755194 18<Myon18>	also, naming a non-PK index _pkey is eww
T 1567755230 18<Myon18>	mazula: there are two errors, one is the create index
T 1567755252 18<Myon18>	and the other ERROR is actually not an error at all, the INSERT works just fine
T 1567755291 18<mazula18>	the error pop when I try to insert
T 1567755297 18<mazula18>	line 15
T 1567755301 18<mazula18>	the other lines work fine
T 1567755304 18<Myon18>	because you already inserted
T 1567755318 18<Myon18>	drop the table and try again
T 1567755477 18<adsf18>	do triggers support multiple insert into arguments? I have a situation where a trigger on one table needs to insert a value into a few other tables.
T 1567755490 18<peerce18>	yes, you can do that
T 1567755502 18<Myon18>	you can run arbitrary code in the trigger function
T 1567755503 18<peerce18>	i prefer avoiding complicated automagic trigger activated stuff
T 1567755518 18<peerce18>	but thats a personal preference
T 1567755539 18<adsf18>	its basically entering a unique key into somewhat of a queue table
T 1567756081 18<peerce18>	well, if you'd executed those commands in an empty database, then I can see no way you would have gotten that error
T 1567756872 18<incognito18>	;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;:;::;;::123;;;;:1b:23
T 1567756874 18<incognito18>	b;:123
T 1567756874 18<incognito18>	1:3123
T 1567756876 18<incognito18>	1;;;;;:123123
T 1567756876 18<incognito18>	:bnbb:
T 1567756877 18<incognito18>	;;;123123
T 1567756878 18<incognito18>	:bn:
T 1567756879 18<incognito18>	2;;;12:23
T 1567756881 18<incognito18>	bn:231;;123
T 1567756881 18<incognito18>	12:3
T 1567756883 18<incognito18>	bn2;;:123
T 1567756885 18<incognito18>	bn:b;;;123
T 1567756887 18<incognito18>	::bn:;;;123
T 1567756889 18<incognito18>	::b:bn;123
T 1567756889 18<incognito18>	:bnn
T 1567756891 18<incognito18>	123;;;123
T 1567756893 18<incognito18>	::bn:12;;;123
T 1567756893 18<incognito18>	::bnbn:
T 1567756895 18<incognito18>	123;;123123
T 1567756897 18<incognito18>	:bn!vb;123
T 1567756899 18<incognito18>	:bnb3
T 1567756901 18<incognito18>	;123
T 1567756903 18<incognito18>	:bnbnVC:
T 1567756905 18<incognito18>	23;b;:12
T 1567756907 18<incognito18>	3b:nv!b:;;;VBN./
T 1567756909 18<incognito18>	vb:
T 1567756911 18<incognito18>	;123
T 1567756940 18<peerce18>	whats this a faceplant?
T 1567757020 18<kayront18>	:D
T 1567757053 18<fleetfox18>	a cat?
T 1567757255 18<peerce18>	i'm more a dog person, never much cared for cats.
T 1567757472 18<lavalike18>	obviously the fox blames the cat
T 1567757675 18<peerce18>	https://www.youtube.com/watch?v=HYvnFdLqVL8   cat?
T 1567757678 18<peerce18>	MEOW!
T 1567757912 18*	incognito killed a keyboard right now ... :'( crime weapon : hot coffee
T 1567758057 18<elmcrest18>	incognito sorry to hear ... throw it into a box of rice! :D
T 1567758128 18<incognito18>	elmcrest: hehe, but i dont have a box of rice enough big; and don't want to make my keyboard a new criminal
T 1567758136 18<elmcrest18>	haha
T 1567758143 18<elmcrest18>	well ... replace it with a water resistant :P
T 1567758149 18<incognito18>	yep
T 1567758174 18<incognito18>	i try to hide the proof and the first criminal actually
T 1567758187 18<incognito18>	(and don't want a civil war btw)
T 1567758218 18<elmcrest18>	you have to stand up for your right to coffee
T 1567758291 18<coffeeAttorney18>	peerce: sorry, keyboard problem
T 1567758312 18<peerce18>	black coffee shoudln't crash a keyboard
T 1567758323 18<incognito18>	big milk coffee ?
T 1567758327 18<peerce18>	but coffee with milk and sugar, all bets re off
T 1567758346 18<xocolatl18>	pumpkin spice
T 1567758349 18<incognito18>	we'll see, it goes to the sun  :)
T 1567758355 18<peerce18>	if its good coffee, its evil to put dairy in it.   if its bad coffee, dairy doesn't really ehlp much
T 1567758385 18<incognito18>	peerce: the badest one : instant coffee + milk + too much sugar
T 1567758422 18<fleetfox18>	that is not coffee anymore :)
T 1567758440 18*	incognito tries like a dumb to type his password during 10 minutes with 123bbb;;;;;;bbb;;;123 going on the middle of it
T 1567758447 18<peerce18>	i don't think I've made or drunk 'instant' coffee in like 30+ years
T 1567758453 18<peerce18>	maybe 40+
T 1567758476 18<peerce18>	when I go camping, I bring whole beans and a hand crank grinder.
T 1567758485 18<peerce18>	and I either make Aeropress, or hand pour drip
T 1567758549 18<peerce18>	at home, we use an electric mill, and a Bonavita electric drip pot w/ melita #4 filters.
T 1567758672 18<vlt18>	*Melitta
T 1567758680 18<peerce18>	yah, that.
T 1567758723 18<vlt18>	https://en.wikipedia.org/wiki/Melitta_Bentz
T 1567758724 18<peerce18>	for grinding w/o electricity, this works great! https://handground.com/
T 1567758726 18<incognito18>	s, filter coffee
T 1567758757 18<incognito18>	filter coffee is the best; but it disturb people who likes expresso
T 1567758775 18<peerce18>	oh, i like a /good/ espresso but that requires a lot of work
T 1567758794 18<incognito18>	and a lot of noise
T 1567758800 18<peerce18>	we even have a very nice swiss made espresso maker, an Olympia Express
T 1567758809 18<peerce18>	but its SO much work
T 1567758866 18<vlt18>	Doing the lot of work to make an espresso actually *is* my coffee break. I only drink like 20 % of the coffees I make :D
T 1567758924 18<peerce18>	our Bonavita drip machine at home has a thermos carafe, and I drink 2-3 mugs daily.
T 1567758953 18<flux[m]18>	incognito: it seems actually that quality coffee grinders don't make _that_ much noise
T 1567758998 18<incognito18>	flux[m]: maybe, but it's not the case when i go drink a coffee (i'm in Paris) in a "Bistrot"
T 1567759118 18<peerce18>	good coffee still tastes good if its not hot.  we mostly use kenya aa, ethiopian harrar, various indonesians, amd evem a few select south american beans.
T 1567759126 18*	lluad discovers that an identity column generated by default doesn't have a default, in a pg_attributes.atthasdef sense
T 1567759131 18<peerce18>	had some really good nicaraguan last week
T 1567759136 18<lluad18>	This makes sense to me. To my code, not so much.
T 1567759151 18<peerce18>	light to medium roast.
T 1567759172 18<YmrDtnJu18>	i have the following setup: node db1 uses streaming replication to sync with db2. another node uses logical replication to stream some data from db1. i added db1 and db2 hostnames to host= in connection string and additionally added target_session_attrs=read-write. this way, libpq automatically uses db1. will the third node use db2 if it is promoted to master and db1 is shutdown?
T 1567759254 18<peerce18>	that makes my head hurt to even read.
T 1567759442 18<peerce18>	if you want to do master/slave promotion kinda stuff, you probably should put a bouncer/pooler in front that you reconfigure when the servers failover.
T 1567759475 18<YmrDtnJu18>	peerce: why?
T 1567759488 18<YmrDtnJu18>	libpq uses the same logic.
T 1567759562 18<indrek18>	Looking for ideas: I have approximately 300M rows for currency rates (cur, timestamp, value). About 75 different currencies. I need to start querying rates with specific dates. Im thinking about 3 options: Partition by cur (75 tables), look into timescaledb or separate table daily rates (doesn't help when asking rate date and time). How would you make queries faster? More index, mode used
T 1567759562 18<indrek18>	gb. Data itself is for half a year and about 20gb
T 1567759629 18<indrek18>	what would be the most efficent way?
T 1567759665 18<incognito18>	indrek: what are your performance requirement ?
T 1567759687 18<indrek18>	Not many queries, but fast
T 1567759695 18<indrek18>	maybe 10 queries per day
T 1567759700 18<indrek18>	but i need to get them in ms
T 1567759708 18<indrek18>	but i need to get them in milliseconds
T 1567759729 18<incognito18>	indrek: the 10 queries are only about recent data ?
T 1567759750 18<indrek18>	No, abut whole table
T 1567759766 18<indrek18>	It might be today or it might be last year
T 1567759775 18<indrek18>	and i have about 3 updates per second
T 1567759786 18<indrek18>	so far it was only used for "just in case logging"
T 1567759803 18<holi031718>	Having some issue with row-level security. I am using a column deleted to indicate deleted status of a row in some table. So I've added deleted = FALSE in RLS select policy. But when I update a record to deleted = TRUE, the query fails as new row violates select policy.
T 1567759811 18<holi031718>	Is there any way to disable select policy checking new row record?
T 1567759842 18<incognito18>	indrek : how much rates do you have by day ?
T 1567759872 18<fleetfox18>	is it growing proportionally across all currencies?
T 1567759897 18<indrek18>	about 2 rates per currency per sec
T 1567759938 18<incognito18>	173k per currency per day ?
T 1567759979 18<indrek18>	let me check exact number
T 1567760006 18<incognito18>	indrek : the 300M rows are only for 23days ?
T 1567760053 18<incognito18>	(i think we are trying to know the actual timestamp range of values)
T 1567760060 18<indrek18>	2 557 203 Row per day
T 1567760064 18<indrek18>	Approximately
T 1567760096 18<YmrDtnJu18>	indrek: does your query use more than one currency? does one query use a wide or small range of timestamps?
T 1567760103 18<incognito18>	ok so, around 60M row / month ?
T 1567760144 18<incognito18>	YmrDtnJu: +1, indrek; let us know the typical most consuming query
T 1567760165 18<indrek18>	No, about 77M rows per month
T 1567760196 18<indrek18>	Query that i need first last rate per currency per deal
T 1567760207 18<xocolatl18>	deal?
T 1567760210 18<indrek18>	Query that i need first is "last rate per currency per deal"
T 1567760213 18<incognito18>	i bet on : partition by week + hash index on cur column
T 1567760214 18<indrek18>	Query that i need first is "last rate per currency per day"
T 1567760257 18<indrek18>	incognito: with timescale? or pure pg?
T 1567760259 18<xocolatl18>	I would do a partition per month with an appropriate btree index
T 1567760275 18<incognito18>	pure pg
T 1567760289 18<incognito18>	i don't like aggregated data
T 1567760368 18<indrek18>	And then index on (timestamp, cur)
T 1567760379 18<xocolatl18>	no, (cur, timestamp)
T 1567760399 18<indrek18>	why in that order ?
T 1567760427 18<incognito18>	well, the indexes after viewing the queries :)
T 1567760451 18<indrek18>	that's true also :)
T 1567760505 18<indrek18>	i expect probably query like "WHERE date = xxx"
T 1567760531 18<xocolatl18>	actually (cur, (date_trunc('day', ts)))
T 1567760535 18<incognito18>	indrek: could be function based
T 1567760571 18<indrek18>	xocolatl: thanks
T 1567760576 18<indrek18>	incognito: thanks
T 1567760588 18<indrek18>	i will try to divide it into smaller tables and create indexes
T 1567760716 18<YmrDtnJu18>	you can also partition over more than one column and create a hierarchy of partitions. just to mention that.
T 1567760825 18<peerce18>	OTOH, imho, I don't think you ever want to create more than 20 or 50 partitions of a given table or things will get really ugly when you make queries that can't be pruned
T 1567760830 18<incognito18>	indrek: if you have only 3-4 monthes of data and you are not willing to store more than a year (~1MM rows); you can also partition by day
T 1567760852 18<incognito18>	oups, peerce is right
T 1567760865 18<incognito18>	i was agree for 365 partitions here  :)
T 1567760924 18<peerce18>	we mostly partitioned by week with 6 montsh retention
T 1567761463 18<indrek18>	too many tables will be eventually
T 1567761475 18<indrek18>	my plan is to keep it maybe for 10 years or so
T 1567761805 18<mazula18>	it's normal this error on an id PK with serial? duplicate key value violates unique constraint "dynamic_bundle_types_pkey"
T 1567761834 18<xocolatl18>	if you insert values yourself, yes
T 1567761842 18<mazula18>	why
T 1567761847 18<mazula18>	the id is not auto incremented?
T 1567761881 18<xocolatl18>	it is, and if the increment is the same as something you inserted yourself, you get a conflict
T 1567761945 18<mazula18>	I juste insert something without manually set the "id"
T 1567761971 18<xocolatl18>	something set an id manually (or reset the sequence)
T 1567761999 18<mazula18>	what is the difference between serial and auto_increment?
T 1567762010 18<xocolatl18>	auto_increment doesn't exist
T 1567762025 18<peerce18>	did you import some existing data into this table?
T 1567762032 18<peerce18>	before doing this INSERT in question ?
T 1567762038 18<mazula18>	there is 4 row
T 1567762041 18<mazula18>	in the table
T 1567762046 18<mazula18>	and I want insert 80 new rows
T 1567762080 18<peerce18>	what is the currval() of the sequence prior to doing this additional 80 inserts ?
T 1567762206 18<mazula18>	I don't know it's my first time with postgreslq
T 1567762231 18<peerce18>	select curval('seqname');
T 1567762246 18<peerce18>	where seqname is the sequence associated with this tables primary key
T 1567762280 18<peerce18>	err, maybe its currval('seqname') ...   i forget
T 1567762294 18<mazula18>	I use pg-format
T 1567762300 18<peerce18>	whats that ?
T 1567762889 18<indrek18>	perce: you mentioned that you partition by week. Do you create partitions automatically? Cron ?
T 1567762899 18<indrek18>	peerce: you mentioned that you partition by week. Do you create partitions automatically? Cron ?
T 1567771225 18<localhorse18>	when i have postgres running in docker but also want to run it outside of docker, it fails with this: http://dpaste.com/1XSMHNJ
T 1567771231 18<localhorse18>	how can i make it work?
T 1567771311 18<Xelnor18>	localhorse: allocate different TCP ports to the one in docker and to the system one :)
T 1567771568 18<localhorse18>	Xelnor: but how can i make it listen on NO port at all, just file socket?
T 1567771649 18<localhorse18>	both `listen_addresses` and `port` are commented out (by default it seems), but somehow it still tries to bind to that port
T 1567771664 18<localhorse18>	even though this says "Omit both of these options to disable TCP/ IP connections." https://www.jamescoyle.net/how-to/3019-how-to-change-the-listening-port-for-postgresql-database
T 1567771712 18<RhodiumToad18>	listen_addresses defaults to 'localhost', you can explicitly set it to '' to disable tcp
T 1567771730 18<RhodiumToad18>	port is required even for local sockets, that defaults to 5432
T 1567771762 18<RhodiumToad18>	and remember that 90-99% of the things you read from random websites on the internet are wrong
T 1567771836 18<RhodiumToad18>	("90% of the internet is crap, and nothing guarantees that the other 10% isn't crap as well")
T 1567771856 18<RhodiumToad18>	(to slightly misquote and paraphrase spaf)
T 1567771945 18<nbjoerg18>	to slightly extend on that: unix domain sockets include the port in the path
T 1567772237 18<localhorse18>	ah thx, works now
T 1567773794 18<afidegnum18>	if i unerstand, schema is what groups tables in the database, right ?
T 1567773824 18<RhodiumToad18>	it's a logical grouping of tables, yes
T 1567773833 18<afidegnum18>	ok, thanks,
T 1567774996 18<Siecje18>	I had a deadlock from this code. https://dpaste.de/dQYL Why would a transaction depend on another transaction?
T 1567775127 18<Myon18>	the typical situation is that you are updating rows 1 and 2, and some other transaction is updating rows 2 and 1
T 1567775135 18<Myon18>	both will then have to wait for each other
T 1567775177 18<Myon18>	this section hash sounds like it would update several rows
T 1567775423 18<RhodiumToad18>	transactions wait for other transactions as part of the row-level locking mechanism
T 1567775469 18<RhodiumToad18>	in order to allow unlimited numbers of row locks to be used, row locks are recorded in the tuple header of the locked row, by the transactionid of the locking transaction (or a multixact id for a group of transactions)
T 1567775495 18<RhodiumToad18>	when one transaction waits on a row lock, it finds the id of the blocking transaction and waits for it to end
T 1567775502 18<Siecje18>	Is row level locking the default?
T 1567775537 18<Siecje18>	Myon: Updating the hash reads fields but only modifies one column for each section.
T 1567775640 18<RhodiumToad18>	row-level locking is used in all update and delete operations, all FK checks, and all select for [key share|share|no key update|update] operations
T 1567775753 18<Siecje18>	So two updates to the same record in different transactions will lock forever?
T 1567775791 18<Myon18>	on a single row, the second update will wait for the first to finish
T 1567775807 18<Myon18>	the deadlock happens if you have several rows updated in different order
T 1567775836 18<Siecje18>	So I should update sections in the same order?
T 1567775847 18<Siecje18>	What if one has more records?
T 1567775894 18<RhodiumToad18>	what exactly are you updating?
T 1567776201 18<dob118>	hi, is there an index that can I apply to a text column to improve search via ~* operator?
T 1567776210 18<dob118>	I have the index on lower(col) now
T 1567776335 18<RhodiumToad18>	what sort of regexp are you searching on?
T 1567776346 18<RhodiumToad18>	a pg_trgm index will help a bit in some cases
T 1567776372 18<dob118>	RhodiumToad, I am just asking, it's not slow it performs good
T 1567780936 20*	Disconnected (20)
T 1567780960 19*	Now talking on 22#postgresql
T 1567780960 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567780960 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567780961 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567847128 20*	Disconnected (20)
T 1567847150 19*	Now talking on 22#postgresql
T 1567847150 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567847150 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:06 2019)
T 1567847151 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1567847463 18<incognito18>	tangara: \dt in psql client doesn't work ?
T 1567847487 18<incognito18>	\d
T 1567847538 18<tangara18>	er...let me copy down the command so that I don't have to ask you guys again and again
T 1567847674 18<tangara18>	@incognito...it shows only the table I have just created
T 1567847685 18<tangara18>	but my other existing table is not showing up
T 1567847690 18<tangara18>	when I typed that command
T 1567847742 18<tangara18>	@incognito what do you think it's happening?
T 1567847923 18<incognito18>	it could be many things; another schema not in search_path; another database
T 1567847950 18<incognito18>	request not sent (if no feedback)
T 1567848553 18<bmintz18>	running `pg_restore -O /var/backups/psql/2019-09-07/ec -t emotes | sudo -u bots psql ec` reports that ERROR:  duplicate key value violates unique constraint "emotes_id_key" DETAIL:  Key (id)=([redacted]) already exists.
T 1567848554 18<bmintz18>	however
T 1567848591 18<bmintz18>	sudo -u bots psql ec -c 'select count(*) from emotes' reports 0
T 1567848602 18<bmintz18>	oh it's under a schema ok
T 1567848621 18<bmintz18>	nvm
T 1567848795 18<xocolatl18>	a redacted id XD XD XD
T 1567848871 18<tangara18>	@incognito do i need to specify the name of database before I created any table or ?
T 1567848885 18<bmintz18>	xocolatl what's so funny
T 1567848900 18<tangara18>	why is that that using psql I can see the table but not inside pgAdmin4 ?
T 1567848903 18<incognito18>	tangara: you only need to connect to the right database; psql --help
T 1567848924 18<incognito18>	i'm not aware of pgAdmin4 UI atm
T 1567848945 18<tangara18>	so now how do I put the new table with the other table together under the same database name?
T 1567849007 18<tangara18>	it seems like i am the only one that run into million of problems...even finding a table can be so difficult
T 1567849012 18<incognito18>	tangara: identify the different databases created and in this databases, identify the schema created
T 1567849093 18<incognito18>	to identify the databases \l -- exclude from the result the builtin templates
T 1567849121 18<tangara18>	what do you mean ?
T 1567849151 18<xocolatl18>	tangara: very few people here use pgadmin
T 1567849165 18<tangara18>	yes. i want to learn psql also
T 1567849173 18<incognito18>	to identify the schema \dn
T 1567849187 18<xocolatl18>	you will be MUCH faster with psql (unless you're on windows)
T 1567849194 18<tangara18>	since now I can access the psql can you tell me how to put all tables together in the same schema or database ?
T 1567849210 18<tangara18>	yes. i hate windows also
T 1567849238 18<xocolatl18>	same schema: alter table foo.bar set schema baz;
T 1567849238 18<tangara18>	@incognito it shows me public postgres
T 1567849245 18<xocolatl18>	same database: use pg_dump
T 1567849305 18<tangara18>	so in this case is the schema postgres ?
T 1567849338 18<xocolatl18>	how could we possibly know that?
T 1567849339 18<incognito18>	and \d and \l
T 1567849469 18<tangara18>	                                                 List of databases    Name    |  Owner   | Encoding |          Collate           |           Ctype            |   Access privileges------------+----------+----------+----------------------------+----------------------------+----------------------- membership | postgres | UTF8     | English_United
T 1567849469 18<tangara18>	States.1252 | English_United States.1252 | postgres   | postgres | UTF8     | English_United States.1252 | English_United States.1252 | template0  | postgres | UTF8     | English_United States.1252 | English_United States.1252 | =c/postgres          +            |          |          |                            |                            |
T 1567849470 18<tangara18>	postgres=CTc/postgres template1  | postgres | UTF8     | English_United States.1252 | English_United States.1252 | =c/postgres          +            |          |          |                            |                            | postgres=CTc/postgres
T 1567849476 18<xocolatl18>	don't paste here please
T 1567849479 18<xocolatl18>	??paste
T 1567849479 18<pg_docbot18>	https://explain.depesz.com/ :: https://pasteboard.co/
T 1567849480 18<pg_docbot18>	https://www.db-fiddle.com/ :: https://paste.depesz.com/
T 1567849480 18<pg_docbot18>	https://dpaste.de
T 1567849484 18<tangara18>	\l shows me membership and also parents
T 1567849494 18<tangara18>	so parents is supposed to be in membership
T 1567849502 18<tangara18>	how do i make parents go into membship ?
T 1567849665 18<tangara18>	https://dpaste.de/ULWd
T 1567849688 18<xocolatl18>	you need to dump and restore it
T 1567849735 18<xocolatl18>	pg_dump -t parents postgres | psql membership
T 1567849738 18<tangara18>	@xocolatl are you talking to me?
T 1567849750 18<xocolatl18>	you are the only one asking questions
T 1567849761 18<tangara18>	ok. let me try. i hope i won't lost the tables in membership
T 1567849776 18<tangara18>	because i really hate to go thru the trouble to create them again..it's so scarey
T 1567849779 18<tangara18>	and i am tired
T 1567849792 18<tangara18>	@xocolatl ok
T 1567849793 18<xocolatl18>	you won't lose anything
T 1567849842 18<tangara18>	i did what you said but it is still not showing up
T 1567849857 18<xocolatl18>	so what errors did you get?
T 1567849861 18<incognito18>	tangara: prepare your different sql clients (pgAdmin4, psql, jdbc) to use only this membership db if you plan to only work on it
T 1567849863 18<tangara18>	no error
T 1567849877 18<tangara18>	that's what I plan to do
T 1567849891 18<xocolatl18>	if you didn't get an error then it worked
T 1567849898 18<tangara18>	but the parents which I have created using sql script doesn't appear inside the membeship
T 1567849911 18<tangara18>	and hence I asked do I need to do it like in MYSQL
T 1567849924 18<tangara18>	where I have to state use membership before creating the table
T 1567849945 18<xocolatl18>	you just need to connect to the right database
T 1567849948 18<tangara18>	xocolatl...it doesn't
T 1567849958 18<xocolatl18>	you can't "use database" like in other engines
T 1567849960 18<tangara18>	cos it is still not showing inside membership
T 1567849982 18<xocolatl18>	how are you looking?
T 1567850006 18<tangara18>	i refreshed my pgadmin and the table doesn't appear inside with other tables
T 1567850073 18<tangara18>	how ?
T 1567850086 18<xocolatl18>	I don't know anything about pgadmin
T 1567850128 18<tangara18>	yeah but your psql is not helping me to put membership together with parents in the same schema
T 1567850148 18<xocolatl18>	pg_dump -t parents postgres | psql membership
T 1567850155 18<tangara18>	i need parents to be the same schema or bringing the tables from mebership to be same as parents either way
T 1567850174 18<tangara18>	i already did as you said but it is not working
T 1567850187 18<tangara18>	i already told you not working
T 1567850194 18<xocolatl18>	show me the full output it gave you
T 1567850216 18<tangara18>	you mean \l /
T 1567850216 18<incognito18>	tangara: you still have your prompt ?
T 1567850221 18<tangara18>	yes
T 1567850225 18<tangara18>	i did the \l
T 1567850234 18*	xocolatl cares not about \l
T 1567850240 18<tangara18>	but it is not showing membership inside the schema as the parent
T 1567850243 18<xocolatl18>	show me the output of  pg_dump -t parents postgres | psql membership
T 1567850253 18<incognito18>	tangara: try psql membership -c "select * from parents"
T 1567850266 18<incognito18>	also
T 1567850287 18<tangara18>	postgres-#
T 1567850295 18<tangara18>	it just give me back the prompt
T 1567850310 18<tangara18>	same as when I typed the command xoclatl told me
T 1567850318 18<tangara18>	it just give me a prompt back
T 1567850329 18<xocolatl18>	you don't run that inside psql, you run it from the command line
T 1567850334 18<xocolatl18>	ctrl+d to get out
T 1567850392 18<tangara18>	can't get out
T 1567850415 18<xocolatl18>	ctrl+c and then ctrl+d
T 1567850445 18<tangara18>	no
T 1567850455 18<tangara18>	it asked me to go out of the batch job
T 1567850470 18<xocolatl18>	what batch job?
T 1567850474 18<tangara18>	i remember someone helped me and asked me to type a command and then i can use psql
T 1567850479 18<tangara18>	again
T 1567850484 18<tangara18>	from the prompt
T 1567850499 18<xocolatl18>	what operating system is this?
T 1567850546 18<incognito18>	tangara: the message is when you exit the psql prompt or the shell prompt ?
T 1567850559 18<tangara18>	postgres=#
T 1567850572 18<tangara18>	so from here how do i get out and go to psql
T 1567850583 18<xocolatl18>	you're already in psql there
T 1567850586 18<tangara18>	I am running psql using this runpsql.batch
T 1567850605 18<xocolatl18>	what operating system is this?
T 1567850606 18<tangara18>	and it links to windows command i think
T 1567850613 18<tangara18>	windows 10
T 1567850614 18<xocolatl18>	so you're on windows?
T 1567850618 18<tangara18>	yes
T 1567850626 18<xocolatl18>	oh well
T 1567850642 18<tangara18>	it doesn't work like your system ?
T 1567850657 18<tangara18>	am i able to use psql ?
T 1567850678 18<tangara18>	cos I googled then i came to know I have to invoke psql.runbtach then I an use
T 1567850720 18<tangara18>	so how now ?
T 1567850737 18<tangara18>	why is my membership not showing it is in postgres ?
T 1567850837 18<incognito18>	tangara: i dont know this batch file; are you able to ask the person who helped you to type the command writed by xocolatl; maybe there are some security in this batch
T 1567850864 18<tangara18>	this is my personal laptop
T 1567850944 18<tangara18>	@incognito ?
T 1567850946 18<tangara18>	how ?
T 1567850963 18<incognito18>	tangara: yes, but i don't know this batch file
T 1567851078 18<tangara18>	so how can i use psql ?
T 1567851092 18<incognito18>	from a cmd prompt
T 1567851099 18<tangara18>	is it be'cos i am running on windows 10 it is not possible to use psql ?
T 1567851122 18<incognito18>	but you need to know the environment able to do so
T 1567851133 18<tangara18>	ok. what environment ?
T 1567851171 18<incognito18>	the environement required in the cmd prompt in order to launch psql
T 1567851218 18<incognito18>	it sometime includes some personal information, that's why i asked you to call the person who helped you
T 1567851253 18<tangara18>	what personal infor ? i already siad this is my laptop
T 1567851288 18<incognito18>	user,password,folders,windowspasswd, etc...
T 1567851300 18<tangara18>	i have all the passwords
T 1567851315 18<tangara18>	can u tell me why you guys can use psql ?
T 1567851346 18<tangara18>	i am not sure why i can't invoke psql.exe inside the postgresql folder
T 1567851697 18<tangara18>	seems like nobody can help me here ?
T 1567851709 18<tangara18>	i am tired i need to take a break noe
T 1567858659 18<pgwhatever18>	is there a way to validate a function when creating it?
T 1567858697 18<RhodiumToad18>	validate how much of it?
T 1567858699 18<pgwhatever18>	For instance, if I create a function that calls another function, but that other function is not defined yet, it is created successfully, but will return an error at runtime when it is subsequently executed.
T 1567858709 18<RhodiumToad18>	that's intentional
T 1567858750 18<pgwhatever18>	Be nice if there was a way to mark that function as INVALID when created.
T 1567858760 18<pgwhatever18>	Oracle does that
T 1567858798 18<RhodiumToad18>	pg doesn't track dependencies of function body content. (hard to do sanely with arbitrary pluggable language modules)
T 1567858817 18<RhodiumToad18>	most function validators just check the syntax
T 1567858833 18<pgwhatever18>	makes sense, but how about this:  is there a tool that would check it after it is created?
T 1567858857 18<RhodiumToad18>	only running the function could do that
T 1567858880 18<pgwhatever18>	gotcha, thanks
T 1567858886 18<RhodiumToad18>	and even then there's the issue of coverage
T 1567858906 18<pgwhatever18>	dag, RodiumToad, you are soooo freakin helpful at this site!
T 1567858922 18<pgwhatever18>	thanks for all your past input as well
T 1567865049 18<pi__18>	no voice?
T 1567865065 18<RhodiumToad18>	hm?
T 1567866163 18<gajus18>	https://docs.timescale.com/latest/api#add_dimension
T 1567866187 18<gajus18>	anyone have a clue if add_dimension would sub-partition the existing dimensions or create a partition map using the new dimension?
T 1567866197 18<gajus18>	it is not clear from reading the documentation what is the behaviour
T 1567866400 18<gajus18>	having a second read, it sounds like it is the former
T 1567869072 20*	Disconnected (20)
T 1567869094 19*	Now talking on 22#postgresql
T 1567869094 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1567869094 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1567869106 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568013689 20*	Disconnected (20)
T 1568013710 19*	Now talking on 22#postgresql
T 1568013710 22*	Topic for 22#postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568013710 22*	Topic for 22#postgresql set by 26Snow-Man!~sfrost@tamriel.snowman.net (24Thu Aug  8 15:05:07 2019)
T 1568013711 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568013998 18<schinckel18>	arrity42: The one we've used in the past was pganalyse
T 1568014014 18<schinckel18>	Probably with a z tho', because 'muricans
T 1568014021 18<arrity4218>	how'd it work for you?
T 1568014021 18<schinckel18>	;)
T 1568014046 18<schinckel18>	Yeah, it was pretty good. It showed us things that didn't have indexes, but also monitored queries, and showed us the slow ones.
T 1568014062 18<arrity4218>	neato i'll bookmark and check it out
T 1568014075 18<schinckel18>	The aggregation wasn't perfect though: it didn't do a good job with some of our functions that have variadic args
T 1568014089 18<arrity4218>	luckily i don't have anything like that at the moment
T 1568014293 18<schinckel18>	Oh, we are on a free plan with them, and still getting some potentially actionable information, as it turns out.
T 1568014323 18<arrity4218>	nice!
T 1568016716 18<indrek18>	hello
T 1568016768 18<indrek18>	im inserting one table into another and to INSERT INTO newtable SELECT * FROM oldtable ORDER col_x.
T 1568016781 18<indrek18>	when i select not select * ,ctid from newtable
T 1568016833 18<incognito18>	indrek: what is your question ?
T 1568016836 18<indrek18>	i have rows 1,3,2 with ctid-s (0,23)(0,24)(0,25)
T 1568016854 18<incognito18>	the CTIDs are changing when you are changing relation
T 1568016868 18<indrek18>	the order by is wrong
T 1568016889 18<incognito18>	in the new table ?
T 1568016892 18<indrek18>	if input is order by, then why isn't the rows in new table in order 1,2,3
T 1568016894 18<indrek18>	yes
T 1568016913 18<incognito18>	was that an empty table ?
T 1568016920 18<indrek18>	yes
T 1568016981 18<indrek18>	55960695	(0,32)
T 1568016981 18<indrek18>	55960705	(0,33)
T 1568016981 18<indrek18>	55960699	(0,34)
T 1568016981 18<indrek18>	55960685	(0,35)
T 1568016981 18<indrek18>	55960684	(0,36)
T 1568016981 18<indrek18>	55960719	(0,37)
T 1568016985 18<incognito18>	maybe a truncate table newtable; and vacuum full newtable; before retrying the insert select could change the order of the CTIDs
T 1568017005 18<indrek18>	after insert ?
T 1568017029 18<incognito18>	no, to replay the whole INSERTIONs
T 1568017033 18<incognito18>	with good CTIDs
T 1568017049 18<indrek18>	i even dropped table
T 1568017053 18<indrek18>	nothing
T 1568017328 18<xocolatl18>	indrek: why do you care what order the rows are in?
T 1568019891 18<[patrik]18>	order is not a property of the data. so no relational databases guarantees order of inserted data.
T 1568019971 18<[patrik]18>	you need to use order by in the select retrieving the data
T 1568020252 18<dreinull18>	I have a Join like this: JOIN schueler ON (infos.klasse = schueler.klasse) or infos.klasse is null
T 1568020292 18<dreinull18>	but I want something like: ....(infos.klasse ILIKE '%schueler.klasse%')...
T 1568020297 18<dreinull18>	how can I do that?
T 1568020976 18<dreinull18>	so that it includes that particular string. I expect to have infos.klasse to have a comma separated list
T 1568021026 18<Myon18>	dreinull: you can put whatever boolean into the join claus
T 1568021030 18<Myon18>	e
T 1568021097 18<Myon18>	your schema violates 1NF, btw, you should expand that comman list into separate rows in a new table
T 1568021100 18<Zr4018>	that said, this reeks of bad schema design
T 1568021102 18<Zr4018>	... well, that
T 1568021117 18<[patrik]18>	dreinull: would such a construct suit your need? select s1.* from s1 join s2 on (s1.data LIKE s2.data) and s1.data like '%Foo%';
T 1568021216 18<Myon18>	also, that NULL check smells like a outer (left?) join
T 1568021233 18<Myon18>	(which also smells like bad schema design)
T 1568022074 18<enoq18>	is this good practice? CREATE DOMAIN unsigned_bigint AS bigint CHECK(VALUE >= 0);
T 1568022198 18<Zr4018>	do you expect to get attempts to insert negative values?
T 1568022285 18<enoq18>	it would be bad if it happened
T 1568022512 18<Myon18>	I'd call it nonnegative, but yeah
T 1568022641 18<enoq18>	thank you
T 1568025447 18<jotauve18>	Hi
T 1568025496 18<jotauve18>	I've two tables (every table have a customer_id column), I want to create a view  with a sum of some rows from table A and some rows from B, grouped by customer_id , how can I do it?
T 1568025612 18<Myon18>	jotauve: do you have a query that does what you want?
T 1568025873 18<harks18>	Is there a way to get this simple operation faster? https://paste.depesz.com/s/yE
T 1568025919 18<incognito18>	harks intersect
T 1568026043 18<harks18>	incognito How does intersect help here?
T 1568026126 18<harks18>	Are you suggesting (A union all B ) except (A intersect B ) ?
T 1568026181 18<incognito18>	harks: i was suggesting but this will not be faster
T 1568026267 18<incognito18>	maybe a select count(coalesce(a.str,b.str)) FROM ... FULL OUTER JOIN ON a.str=b.str where a.str is null or b.str is null
T 1568026353 18<incognito18>	harks: btw, it could be interesting to view the diff. of the explain plan of the 2 queries
T 1568026385 18<harks18>	I'm not sure what you are suggesting.
T 1568026462 18<incognito18>	the 2 sets except where the intersect (a=b)
T 1568026626 18<harks18>	Do you mean (A union all B ) except (A INNER JOIN B ) ?
T 1568026686 18<incognito18>	no
T 1568026736 18<incognito18>	(a full outer join b) where (a.str is null or b.str is null)
T 1568026750 18<incognito18>	where the join condition is not satisfied
T 1568027600 18<harks18>	That's a speedup my nearly 30 %. Thanks!
T 1568027638 18<arrity4218>	ohhhh i finally understand lateral joins
T 1568027652 18<arrity4218>	damn i should write an article i have the perfect before and after for the perfect way of understanding them
T 1568028105 18<avances12318>	hi, I have a db with a simple
T 1568028108 18<avances12318>	hi, I have a db with a simple  usage, each 10min, a big (11MB) row is inserted into a table. I have a SSD disk. Which checkpoint/fsync parameters can I tune, to speed to the best, this write?, I mean, y have 9:58 min without any writes,
T 1568028110 18<avances12318>	and I can split this row or change any aplication logic. Thanks for the advices!
T 1568028167 18<Myon18>	avances123: you aren't even close to having any problems
T 1568028173 18<Myon18>	doesn't make sense to optimze that
T 1568028389 18<tuxinator18>	For postgresql using physical replication, is it possible to pull and to push?
T 1568028405 18<Myon18>	it's basically already that
T 1568028414 18<Myon18>	what problem do you have?
T 1568028654 18<tuxinator18>	the current setup comes from a former employee and it is "external server" connects to "internal server" for replication, however it would make the whole thing easier and more secure if it would be like "internal server" connects to "external server" to repicate
T 1568028676 18<tuxinator18>	especially as "internal server" is the rw master and "external server" only the readonly copy
T 1568028781 18<mage_18>	that's how streaming replication works
T 1568028781 18<Myon18>	the connection is initiated from the standby side (put changes are pushed out over that connection)
T 1568028866 18<tuxinator18>	as the master "knows whats written at the moment" doesn't it make sense to send write replicas directly from master to readonly slave?
T 1568028884 18<Myon18>	"put changes are pushed out over that connection"
T 1568028890 18<Myon18>	but*
T 1568029144 18<tuxinator18>	so it is a requirement that the slave initiates the connection?
T 1568029160 18<Myon18>	yes
T 1568029190 18<Myon18>	the primary server doesn't even know the address of the standbys
T 1568029364 18<tuxinator18>	strange, i don't get it why technicaly this should be required
T 1568029388 18<Zr4018>	a replication client is just that, a client
T 1568029424 18<enoq18>	if you don't expect many writes/updates, how good is this solution to handle both autoincrement ids and existing ids https://stackoverflow.com/a/22411460
T 1568029452 18<Zr4018>	theoretically there's nothing fundamentally preventing the primary server from initiating the connection, but nobody was bothered enough to implement it that way
T 1568029497 18<Myon18>	the plain simple explanation is that it isn't implemented
T 1568029507 18<tuxinator18>	Zr40 haha :D
T 1568029521 18<tuxinator18>	Myon: thanks for all that explanations!
T 1568029537 18<enoq18>	tbh I'm kinda wondering how people handle REST PUT and POST calls with sequences
T 1568029567 18<tuxinator18>	in our Setup, master is internal, slave external, so that feature qould make sense
T 1568029569 18<Berge18>	enoq: Handle?
T 1568029581 18<enoq18>	Berge it's a common use case to have both PUT and POST
T 1568029591 18<Zr4018>	what's there to handle?
T 1568029597 18<Berge18>	enoq: sure, but what's the issue with it?
T 1568029601 18<enoq18>	both can create a new entry in your db
T 1568029603 18<enoq18>	ids
T 1568029608 18<enoq18>	POST needs to generate ids
T 1568029616 18<enoq18>	PUT has an id per REST spec
T 1568029617 18<Myon18>	INSERT ... RETURING id
T 1568029636 18<Berge18>	enoq: Yes, sort of akin to UPDATE and INSERT
T 1568029649 18<enoq18>	which isn't the problem
T 1568029656 18<ufk__18>	i
T 1568029658 18<enoq18>	the problem is sequence conflicts
T 1568029658 18<ufk__18>	Hi
T 1568029670 18<Berge18>	enoq: If clients are allowed to create their own IDs?
T 1568029682 18<enoq18>	Berge they are for PUT
T 1568029690 18<Berge18>	Then a sequence likely isn't what you're looking for
T 1568029692 18<enoq18>	since a client can just pass a new id in the URL
T 1568029693 18<Zr4018>	tuxinator: we have a case like that. We use ssh to tunnel the connection, so the external server can just connect to localhost, carrying the connection over the ssh session established by the internal server
T 1568029711 18<Myon18>	nice hack
T 1568029717 18<Berge18>	enoq: It's not common for clients to be able to create their own surrogate IDs like that, is it?
T 1568029739 18<enoq18>	Berge what do you mean?
T 1568029761 18<ilmari18>	either the entity has a natural key provided by the client, or it has a surrogate key generated by the server
T 1568029765 18<ilmari18>	usually not a mix
T 1568029768 18<Berge18>	Exactly
T 1568029772 18<ilmari18>	the entity _type_ that s
T 1568029775 18<ilmari18>	*that is
T 1568029803 18<Zr4018>	enoq: don't use PUT when you meant POST
T 1568029824 18<enoq18>	Zr40 you mean only allow PUT for updating records
T 1568029827 18<Berge18>	enoq: Perhaps you can provide an example
T 1568029852 18<Zr4018>	enoq: replacing records, or creating one for which the identifier is determined externally
T 1568029888 18<blip9918>	hey guys.  I'm trying to learn and use some code to do an upsert.  I don't get what a constraint is when it comes to ON CONFLICT ON CONSTRAINT <constraint+name> DO UPDATE <...>
T 1568029903 18<blip9918>	The field I want to upsert to is jsonb
T 1568029927 18<blip9918>	there's no unique property set on the field
T 1568029931 18<tuxinator18>	Zr40: we do the same, however SSH-Tunnels from a insecure Zone back to a secure one, don't solve a problem, they actually make it worse
T 1568029936 18<tuxinator18>	:D
T 1568029945 18<enoq18>	the real issue there is that these ids then become database internal and you need to find an additional unique key for every record
T 1568029947 18<blip9918>	and I don't know what the constraint name should be for that field?
T 1568029947 18<Berge18>	blip99: UPSERT only makes sense if you have a conflict
T 1568029962 18<Zr4018>	enoq: that makes no sense
T 1568029966 18<Berge18>	blip99: Do have a conflict, you need some set of things that can be in conflict.
T 1568029969 18<blip9918>	am i supposed to manually create a constraint?  I guess I don't get what constraints are properly :D
T 1568029969 18<enoq18>	so what should GET /shops/1 return
T 1568029970 18<Berge18>	(-:
T 1568029972 18<perry12300718>	Hi all, question about INHERITS.  I have a table that I fill every hour with hunderds of entries. Old entries are all deprecated and deleted. Is it a good solution to have two inherited tables and fill table_1, truncate table_2 and then swap them, so I always read form table_1? Both tables with disabled vacuum.
T 1568029989 18<Berge18>	blip99: Typically a UNIQUE contraint or primary key
T 1568029999 18<blip9918>	Berge, ok - how can I say "if there's any value in my 'data' column, consider that a conflict and overwrite it'?
T 1568030002 18<enoq18>	should it return a record with a database generated id "1" or with your second id called external_id "1"
T 1568030024 18<blip9918>	Berge, exactly, my data column isn't a key, nor has UNIQUE set
T 1568030036 18<Berge18>	blip99: Then how will you know which row to UPDATE?
T 1568030039 18<Myon18>	perry123007: prefer partitioning over inheritance
T 1568030058 18<Myon18>	but both have downsides, like renaming will lock the entire hierarchy
T 1568030059 18<blip9918>	unique_id  |  data (jsonb)
T 1568030063 18<Zr4018>	enoq: same thing. The difference is semantic - either you (the database) assigns all ids and you create new records using POST and replace existing ones with PUTH, or the client assigns all ids and you don't use a sequence
T 1568030076 18<Zr4018>	s/PUTH/PUT/
T 1568030083 18<blip9918>	Berge, ^   Inserting into that table, when data already exists - replace (upsert)
T 1568030099 18<enoq18>	Zr40 absolutely
T 1568030123 18<Berge18>	blip99: What's unique_id here?
T 1568030129 18<Berge18>	The jsonb field?
T 1568030154 18<blip9918>	id (unique primary key) | data (jsonb)
T 1568030158 18<Zr4018>	enoq: don't mix the two models for the same resource
T 1568030172 18<Berge18>	blip99: Then your constraint is the primary key
T 1568030179 18<blip9918>	aaaaaah
T 1568030188 18<perry12300718>	Myon: from what I understood, inheritance is way of partitioning
T 1568030190 18<blip9918>	my bad
T 1568030224 18<Myon18>	perry123007: the old way, PG10+ has a better one
T 1568030303 18<Myon18>	perry123007: btw, if you never delete anything, it doesn't make sense to disable autovacuum, so better leave it on (because footgun)
T 1568030308 18<blip9918>	Berge, and in that case should I manually create a constraint? I saw online examples where they do ON CONFLICT ON CONSTRAINT tablename_columnname_key
T 1568030324 18<Myon18>	(and if you do delete things, you also want autovacuum to be on)
T 1568030325 18<blip9918>	I guess this is special syntax tablename_columnname_key
T 1568030330 18<Berge18>	blip99: A primary key _is_ a constraint
T 1568030367 18<Berge18>	The syntax isn't special. Postgres will autogenerate constraint names if they're not given.
T 1568030371 18<ufk18>	i need to group by on one column while postgresql asks me to add all the other columns.. is there a way to overcome this ? I have a shared id on each group of rows from the resulted query and I want the first row for each group of rows that share the same id
T 1568030379 18<Berge18>	On the form tablename_columnname_pkey, iirc
T 1568030383 18<blip9918>	Berge, ah I see.  but how come my DB client doesn't show any constraints defined, it's automatic somehow?
T 1568030403 18<Berge18>	blip99: \d tablename i psql and pastebin the results
T 1568030407 18<perry12300718>	Myon: Ok, thanks
T 1568030408 18<enoq18>	Zr40
T 1568030420 18<enoq18>	Zr40 thank you I think I've figured it out
T 1568030449 18<enoq18>	we will migrate from client ids to database generated ids at some point, init the sequence with the highest id and only use PUT for updates
T 1568030474 18<enoq18>	db imports will be done over separate API calls that rewrite the ids
T 1568030501 18<enoq18>	basically replace ids given by the client with ids generated by the db
T 1568030504 18<Berge18>	enoq: Your clients can easily DoS your service by providing an ID that's the largest that can fit in your datatype, then
T 1568030569 18<enoq18>	Berge right, but that shouldn't happen (it's in local network)
T 1568030585 18<enoq18>	company internal use only
T 1568030586 18<Berge18>	If you can trust your clients, then sure
T 1568030899 18<ufk18>	I want to use window functions for using first_value and let_value based on the data that I ordered and paritioned by it. so lets say if the 'partition by' is returning 6 rows, I only want to handle the first row of each parititon since I use first_value and last_value() functions anyways so the rows are just duplicated. how can I ovecome this ?
T 1568031096 18<Myon18>	maybe you want GROUP BY instead of OVER (PARTITION BY...) ?
T 1568031142 18<arrity4218>	or a subquery join limit 1
T 1568031153 18<arrity4218>	order by desc etc.
T 1568031186 18<arrity4218>	maybe lateral to avoid doing 2 subqueries
T 1568031208 18<Myon18>	your new tool :)
T 1568031229 18<arrity4218>	it's actually interesting the ufk just asked that
T 1568031242 18<arrity4218>	i had to do something almost identical 10 minutes ago
T 1568031318 18<arrity4218>	for a list of threads, i needed either the last reply of the thread, or the thread title, for each
T 1568031385 18<arrity4218>	well my use-case doesn't matter so i won't elaborate too much on it, but needless to say ufk my situation was very close to yours, and i ended up doing a subquery that did order by desc limit 1
T 1568031403 18<arrity4218>	bed time night night
T 1568031704 18<Gibheer18>	is there a way to cast a macaddr type to a bytea to get the last 3 bytes of the mac address?
T 1568031894 18<Gibheer18>	I want to run % on the mac address to let the same mac address be handled by the same thread. But somehow I can't find a function to extract the last 3 bytes from the mac address to maybe get an integer out of it
T 1568032119 18<gcbirzan18>	Gibheer: If you just want an identifier, you can do & '0:0:0:ff:ff:ff'::macaddr.
T 1568032445 18<Gibheer18>	gcbirzan: my plan was to start n threads and then look for mac addresses where the address % n = id
T 1568033163 18<azuri518>	is it safe to run pg_stat_statements_reset() on a production DB?
T 1568033210 18<azuri518>	I just upgraded our DB to postgres 11 and the read IOPS are high even after running "analyze verbose"
T 1568033218 18<Gibheer18>	thank you gcbirzan, your hint gave me a different idea select (substring('00:11:22:33:44:55' from 16))::int % 8;
T 1568033392 18<Myon18>	azuri5: if you don't need the old stats, it's safe
T 1568033408 18<azuri518>	Myon: thanks
T 1568033745 18<ufk18>	Myon, I can't do group by.. cause i need data from first and last row in the partiiton
T 1568033859 18<ufk18>	i can't do limit 1 because then i will get only one line.. and i need one line per shared id
T 1568033901 18<MatheusOl18>	ufk: depending on what you want exactly, you can use row_number (or rank)
T 1568033903 18<Myon18>	then you should maybe show an example
T 1568033911 18<jerware18>	hello
T 1568033927 18<MatheusOl18>	ufk: one doing ORDER BY ... ASC and another doing ORDER BY ... DESC, then get `rn_asc = 1 OR rn_desc = 1`
T 1568033937 18<MatheusOl18>	Unless I misunderstood (quite possible)
T 1568034114 18<ufk18>	ok i have a list of calls.. each call got its own unique id. each call can have more then one line with direction 1 or 2 (related to routing). in each group of call ids, i sort the rows by direction, call time and then for each group i need to get info from the first and last line
T 1568034116 18<jerware18>	SELECT foo UNION SELECT bar ORDER BY id;  -- The ORDER BY doesn't work here.
T 1568034136 18<ufk18>	so i do PARTITION BY a.call_id order by a.direction, a.call_time RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
T 1568034176 18<ufk18>	since I already use first_value and last_value functions, for each call_id  i need to return only one line, and it returns the same amount of lines that I had to begin with. how can I limit it to one line ?
T 1568034201 18<Myon18>	jerware: this should work, but you can't have a ORDER BY on the foo part
T 1568034238 18<jerware18>	(SELECT foo UNION SELECT bar) ORDER BY id;  -- What I'm trying to do.  I want to order the resulting table
T 1568034263 18<Myon18>	that's SELECT foo UNION SELECT bar ORDER BY id;
T 1568034344 18<MatheusOl18>	ufk: you want the fist and last line per call_id? Or just one line?
T 1568034414 18<MatheusOl18>	ufk: e.g. Would that solve your problem? SELECT * FROM (SELECT ..., row_number() OVER(PARTITION BY a.call_id ORDER BY a.direction, a.call_time) AS rn_asc, row_number() OVER(PARTITION BY a.call_id ORDER BY a.direction DESC, a.call_time DESC) AS rn_desc FROM ... ) t WHERE (t.rn_asc = 1 OR t.rn_desc = 1) ??
T 1568034515 18<ufk18>	actually yeah
T 1568034531 18<ufk18>	:) thanks
T 1568034585 18<MatheusOl18>	yw
T 1568034769 18<tuxinator18>	Myon: an idea how much such a replication feature the direction i asked for would cost to develope?
T 1568034877 18<Myon18>	tuxinator: first you'd need to teach the primary to know the addresses of all standby servers
T 1568034900 18<Zr4018>	tuxinator: you've explained what you want, but not why you want it
T 1568034918 18<Myon18>	tuxinator: drafting that, and implementing... a few weeks, spread over several months
T 1568034995 18<Myon18>	tuxinator: a viable option would be to put a pg_receivewal instance into your DMZ, and have the standby server "connect" there
T 1568035014 18<Myon18>	aka cascading replication
T 1568035020 18<Myon18>	that's already a thing now
T 1568035039 18<tuxinator18>	Myon, still the Server in DMZ would need to connect back
T 1568035065 18<Myon18>	that's what DMZs are good for
T 1568035110 18<tuxinator18>	Myon, not really, always better to not connect back and actually we do not need to as we do no writes externaly
T 1568035121 18<tuxinator18>	Myon: always a question of security
T 1568035141 18<tuxinator18>	Myon: we have a Budget for Opensource Developement so we could consider to sponsor
T 1568035165 18<Myon18>	tuxinator: post a draft on pgsql-hackers and ask if anyone is interested
T 1568035177 18<tuxinator18>	Myon: good idea
T 1568035213 18<tuxinator18>	Myon: and thanks for the suggestions, however at our site it's a term of hight security so we try to allways avoid connecting back
T 1568035262 18<Myon18>	also, what works now is to push the wal *files* out, and have the standby read from there
T 1568035273 18<Myon18>	then it's file-based instead of streaming
T 1568035350 18<tuxinator18>	Myon: File-Based = Logical replication right?
T 1568035376 18<Myon18>	you asked for "physical" replication
T 1568035413 18<tuxinator18>	Myon: so what is file-based?
T 1568035442 18<tuxinator18>	Myon: found some doc's , will read through
T 1568035503 18<Myon18>	that's replication in 16MB increments
T 1568035580 18<blip9918>	hi all, i'm trying to understand some postgres code, they do a: DO UPDATE set (product_id, data) = (EXCLUDED.product_id, EXCLUDED.data)
T 1568035600 18<blip9918>	what does the EXCLUDED accomplish?  (this is part of code that should be upserting)
T 1568035649 18<blip9918>	I see excluded in many examples online
T 1568035675 18<Myon18>	you are looking for "on conflict do what I mean"
T 1568035684 18<Myon18>	in practise, you have to spell that out
T 1568035696 18<blip9918>	yes
T 1568035724 18<blip9918>	ON CONFLICT ON CONSTRAINT product_pkey DO UPDATE set (product_id, data) = (EXCLUDED.product_id, EXCLUDED.data)
T 1568035757 18<blip9918>	Myon, ^   I just don't understand the do update set - is it over-writing? Who is this EXCLUDED table/schema
T 1568035782 18<Myon18>	that's the data that failed to INSERT
T 1568035789 18<blip9918>	aaah
T 1568035795 18<blip9918>	ok perfect.
T 1568035806 18<blip9918>	it failed to insert because of a conflict, so we FORCE inserted it :)
T 1568035812 18<blip9918>	great
T 1568035816 18<Myon18>	it's not automatic because you might want to increment some count instead
T 1568035960 18<blip9918>	ah I see. makes sense
T 1568035967 18<blip9918>	thanks
T 1568038657 18<c355e3b18>	Has anyone ever seen pg_dump not include `DEFAULT ...` when dumping tables?
T 1568038792 18<ilmari18>	c355e3b: only if the columns don't have defaults
T 1568038827 18<c355e3b18>	Ok, ill keep digging into it
T 1568038843 18<ilmari18>	are you sure it's dumping the table you think it is?
T 1568038849 18<ufk18>	is there something like timezone() function that does inverse timezone ? :)
T 1568038850 18<c355e3b18>	Yeah its including the table
T 1568038859 18<ilmari18>	are you sure it's the _right_ table?
T 1568038868 18<c355e3b18>	and psql shows me the columns
T 1568038893 18<c355e3b18>	I'm telling it to dump the entire schema without data
T 1568038895 18<ilmari18>	what does \d <schema>.<table> show a default value?
T 1568038935 18<c355e3b18>	`nextval('dcv.batch_estimate_closures_id_seq'::regclass)`
T 1568038941 18<c355e3b18>	So its there
T 1568038978 18<ilmari18>	pg_dump might be setting the default separately using ALTER TABLE ... ALTER COLUMN ... SET DEFAULT
T 1568039232 18<rzglx18>	i didn't see a good explanation for it in the docs-- is there a page explaining the storage characteristics of text data type?  or any data type in general?
T 1568039266 18<rzglx18>	for example, in sql server a varchar(n) data type is defined as n bytes + 2 bytes.  all the things i see for pgsql only describe the maximum for text
T 1568039267 18<Myon18>	rzglx: the keyword is "varlena" storage
T 1568039291 18<Myon18>	varchar and text are the same
T 1568039298 18<rzglx18>	oh ok
T 1568039317 18<Myon18>	also, it's not "n", but whatever number of bytes are required for n *characters* (think utf-8)
T 1568039355 18<Myon18>	and it's not 2, but 1 or 4, and optional compression of the data
T 1568039369 18<Myon18>	??varlena
T 1568039370 18<pg_docbot18>	http://www.varlena.com/GeneralBits
T 1568039372 18<Myon18>	??toast
T 1568039372 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/storage-toast.html
T 1568039377 18<rzglx18>	right sorry, that was for single byte encoding sets only
T 1568039385 18<rzglx18>	thanks for the handy links i'll look through them
T 1568039446 18<RhodiumToad18>	in short it's 1 byte + content length for lengths up to 126 bytes of content, and 4 bytes + content length otherwise
T 1568039464 18<Myon18>	the first link looks like vanity, I'm pondering deleting it
T 1568039484 18<RhodiumToad18>	the short varlenas do not have to be aligned, but the long ones are aligned to 4 bytes
T 1568039565 18<RhodiumToad18>	varlena values in columns of storage types other than "plain" can be toasted (replaced with a pointer to data chunks in a separate toast table) but the threshold for this is based on the size of the whole row, not individual columns
T 1568039647 18<Myon18>	?forget http://www.varlena.com/GeneralBits
T 1568039648 18<pg_docbot18>	Forgot 1 url
T 1568039713 18<rivyn18>	RhodiumToad:  My apologies - I got pulled away last week after I'd asked about the malloc error I have been running into.  Did you have any further insight after I pasted t he meminfo?
T 1568039725 18<RhodiumToad18>	no
T 1568039738 18<RhodiumToad18>	oh, maybe I did
T 1568039749 18<RhodiumToad18>	are you monitoring the committed memory amount at all?
T 1568039768 18<RhodiumToad18>	when you have overcommit off you really need to do that, because trying to monitor "free" memory is misleading
T 1568039781 18<RhodiumToad18>	you can have large amounts of committed memory that is not free
T 1568039794 18<RhodiumToad18>	er, that is not removed from the "free" count
T 1568039915 18<harks18>	Is there a flag to tell pg_dump to omit the schema name?
T 1568039933 18<RhodiumToad18>	no
T 1568040916 18<CalimeroTeknik18>	can this 'natural sort' be used without "create operator class"? http://www.rhodiumtoad.org.uk/junk/naturalsort-hack.sql
T 1568040932 18<RhodiumToad18>	use the other one, not that one
T 1568040993 18<RhodiumToad18>	and no, it can't
T 1568041016 18<CalimeroTeknik18>	I guess the other one is http://www.rhodiumtoad.org.uk/junk/naturalsort.sql (C locale only)
T 1568041046 18<RhodiumToad18>	these days there might be equivalents in ICU, I haven't checked
T 1568041122 18<xocolatl18>	yes, icu is the way I would do this
T 1568041131 18<xocolatl18>	it's even the example in our docs, I believe
T 1568041191 18<xocolatl18>	"CREATE COLLATION numeric (provider = icu, locale = 'en@colNumeric=yes')"
T 1568041263 18<CalimeroTeknik18>	the reason why I'm looking at this, is that naturalsort-hack can't work on amazon RDS
T 1568041280 18<RhodiumToad18>	do they support ICU?
T 1568041313 18<RhodiumToad18>	(needs pg 10+)
T 1568041349 18<CalimeroTeknik18>	I have a 10.6 server on their infrastructure ready to test this [currently reading docs to figure out what icu is]
T 1568041380 18<RhodiumToad18>	I think the  CREATE COLLATION numeric (provider = icu, locale = 'en-u-kn-true');  is the preferred form?
T 1568041466 18<CalimeroTeknik18>	how does this all relate to natural sort in the end?
T 1568041492 18<RhodiumToad18>	you'd use COLLATE "numeric" in the column definition, or  ORDER BY foo COLLATE "numeric"
T 1568041538 18<CalimeroTeknik18>	why the name "numeric" rather than say, "natural"?
T 1568041573 18<RhodiumToad18>	call it whatever you choose
T 1568041596 18<RhodiumToad18>	the name after CREATE COLLATION is the name that appears in the COLLATE clause, but it's otherwise arbitrary
T 1568041621 18<RhodiumToad18>	it's the locale= parameter that defines how it actually behaves
T 1568041701 18<CalimeroTeknik18>	oh, I see, and this 'en-u-kn-true' locale does the weird right thing of ordering by numerical / non-numerical chunks then! wonderful
T 1568042169 18<RhodiumToad18>	it's the -kn-true tag which specifies the numeric behavior
T 1568042276 18<dreinull18>	Myon, [patrik] thanks for your help. I haven't thought of putting it on different rows instead of comma separation. That's a good idea. Why is the null check bad? Do you have a better pattern for instead?
T 1568042278 18<CalimeroTeknik18>	and I take it that 'en@colNumeric=yes' is an alternative, more explicit notation
T 1568042344 18<Myon18>	dreinull: not necessarily bad, depends on the schema and the query (and if the schema is good for that kind of query)
T 1568042372 18<rivyn18>	RhodiumToad, how can I check committed memory?
T 1568042386 18<dreinull18>	well, I decided that no setting is valid for all.
T 1568042390 18<rivyn18>	Is it better to have overcommit enabled?
T 1568042407 18<rivyn18>	I thought that overcommit disabled was supposed to be safer for PostgreSQL
T 1568042424 18<RhodiumToad18>	rivyn: check the committed_as value from meminfo
T 1568042425 18<rivyn18>	to avoid oom-killer
T 1568042441 18<RhodiumToad18>	right, but the downside is that you need to watch committed memory usage vs. swap space
T 1568042459 18<rivyn18>	I see committed_AS as ~9GB
T 1568042465 18<RhodiumToad18>	also you probably want to increase overcommit_ratio to 90
T 1568042490 18<RhodiumToad18>	with overcommit off, malloc fails when it would cause committed_as to exceed commitlimit,
T 1568042507 18<rivyn18>	ok, how does swap factor in?
T 1568042510 18<RhodiumToad18>	which by default is swap size + RAM*(overcommit_ratio/100)
T 1568042523 18<rivyn18>	I could allocate more swap easily enough
T 1568042539 18<RhodiumToad18>	so by default, with overcommit_ratio at 50%, you're limited to swap + half of ram
T 1568042578 18<RhodiumToad18>	I would increase swap size to at least RAM size, as well as increasing overcommit_ratio
T 1568042602 18<indrek18>	What should be reasonable speed to get data from those tables, if you expect to have total around 300M rows of data. Lets say average developer machine. With tables and query like this
T 1568042602 18<indrek18>	https://pastebin.com/19me7dDU
T 1568042607 18<RhodiumToad18>	yes, that means you'll have a ton of swap space allocated that you'll never, ever, use - that's the penalty of disabling overcommit
T 1568042635 18<indrek18>	Here is the explain also of that query
T 1568042636 18<indrek18>	https://explain.depesz.com/s/AHF7
T 1568042640 18<rivyn18>	so would it be recommended to enable overcommit?
T 1568042652 18<rivyn18>	what's the best practice for dedicated PG servers?
T 1568042680 18<RhodiumToad18>	rivyn: safest way is disable overcommit, provide plenty of swap, monitor committed space usage
T 1568042710 18<RhodiumToad18>	also increase overcommit_ratio unless you are short of RAM
T 1568042780 18<rivyn18>	ok
T 1568046755 18<susenj18>	I have a linux host running postgresql. What command should I run to list the existing databases without going to the psql prompt?
T 1568046772 18<ilmari18>	susenj: psql -l
T 1568046801 18<susenj18>	it says `psql: FATAL:  role "root" does not exist`
T 1568046843 18<depesz18>	susenj: try not to work on root.
T 1568046844 18<ilmari18>	sudo -u postgres psql -l
T 1568046856 18<ilmari18>	susenj: which linux distro?
T 1568046871 18<susenj18>	ilmari: Centos 7
T 1568046899 18<susenj18>	depesz, Agree but I was going to automate something and I don't want to go into psql prompt
T 1568046971 18<ilmari18>	susenj: add -l to whatever psql invocation you use to get to the prompt
T 1568046977 18<ilmari18>	then it'll just output the list of databases and exit
T 1568046997 18<susenj18>	perfect. Exacly what I wanted. Thanks a lot
T 1568047007 18<ilmari18>	you possibl want -AXt as well, if you're processing the output
T 1568047405 18<davidfetter_work18>	hi
T 1568047415 18<susenj18>	Wow.. thanks ilmari. so, it removes the formatting..right?
T 1568047567 18<ilmari18>	susenj: -X stops it reading .psqlrc, which might set all sorts of output format switches, -A switches to unaligned mode, -t removes the column header row
T 1568047632 18<susenj18>	Awesome, how about tables? If I want to list the table names in a particular db? Inside psql prompt, we have `\dt;` but not sure about outside
T 1568047796 18<davidfetter_work18>	psql -AtqX '\dt'
T 1568047850 18<susenj18>	davidfetter_work `psql: FATAL:  database "\dt" does not exist`
T 1568047868 18<davidfetter_work18>	susenj, or if you're in an application, SELECT table_name FROM information_schem.tables WHERE table_type='BASE TABLE' AND table_schema::text !~ '^(pg_*|information_schema)'
T 1568047879 18<davidfetter_work18>	psql -AtqXc '\dt'
T 1568047881 18<davidfetter_work18>	sorry
T 1568047895 18<davidfetter_work18>	and that's just for the default db, so
T 1568047904 18<davidfetter_work18>	psql -AtqX -d your_db -c '\dt'
T 1568048006 18<susenj18>	Thanks davidfetter_work. it worked like a charm. Where is this flags cheatsheet? I think I would need this
T 1568048012 18<xocolatl18>	if we're playing golf, I submit  psql -AtqXc '\dt' your_db
T 1568048029 18<ilmari18>	\\dt is shorter than '\dt'
T 1568048054 18<ilmari18>	psql -AXtqc\\dt your_db
T 1568048057 18<breinbaas18>	space betwee c and ' can go
T 1568048062 18<breinbaas18>	right :)
T 1568048112 18<davidfetter_work18>	ugh
T 1568048121 18<davidfetter_work18>	golf is an anti-pattern :P
T 1568048140 18<breinbaas18>	fun-pattern is shorter!
T 1568048197 18<davidfetter_work18>	susenj, the man page has all of them, unfortunately in alphabetical order, which makes them handy as a reference, less so when you need several and don't know what they are.
T 1568048270 18<susenj18>	cool. I will check that too. and thanks ilmari for the shortcut
T 1568048324 18<ilmari18>	susenj: if this is going in a longer-lived script, use the long form of the options
T 1568048343 18<davidfetter_work18>	legibility++ :)
T 1568048459 18<susenj18>	@ilma
T 1568048473 18<susenj18>	ilmari, noted
T 1568048513 18<ilmari18>	golf and clever short option combos are handy for muscle memory and everyday use, not "real" code
T 1568048524 18*	ilmari uses 'ls -lart' and 'ps faux' a lot
T 1568049023 18<davidfetter_work18>	same here, although I've stopped using "lart" in all other contexts
T 1568050040 18<davidfetter_work18>	??cdc
T 1568050040 18<pg_docbot18>	http://debezium.io/
T 1568050074 18<davidfetter_work18>	anybody used change data capture utilities other than debezium?
T 1568050123 18*	davidfetter_work was pretty underwhelmed by AWS DMS, and is a little gun-shy as a result
T 1568050150 18<lluad18>	I'm consuming change data from PG into my own app, using a tiny Go library.
T 1568050169 18<davidfetter_work18>	is the library free software?
T 1568050178 18<lluad18>	It is, yeah.
T 1568050187 18*	davidfetter_work looking to redact a logical copy of a DB in real time
T 1568050192 18<davidfetter_work18>	linky?
T 1568050195 18<lluad18>	Let me see if I can find it (it's not my lib, I'm just using it)
T 1568050328 18<lluad18>	https://github.com/kyleconroy/pgoutput
T 1568050480 18<davidfetter_work18>	?learn cdc https://github.com/kyleconroy/pgoutput
T 1568050480 18<pg_docbot18>	Access denied
T 1568050490 18<davidfetter_work18>	ugh. can somebody with permission do that? :)
T 1568050513 18<RhodiumToad18>	?learn cdc https://github.com/kyleconroy/pgoutput
T 1568050513 18<pg_docbot18>	Successfully added URL with 1 keyword
T 1568050517 18<davidfetter_work18>	thanks!
T 1568050554 18<davidfetter_work18>	so about that parsing hex characters into binary...is this to make COPY work faster on byteas?
T 1568050649 18<RhodiumToad18>	it was just because I was experimenting with parsing the file
T 1568050669 18<davidfetter_work18>	:)
T 1568050670 18<RhodiumToad18>	whether COPY (or byteain generally) needs improvement is another matter
T 1568050696 18<RhodiumToad18>	I suspect that byteain's performance is lost in the noise, but I haven't profiled it
T 1568050712 18<lluad18>	You might find some useful stuff at https://wiki.postgresql.org/wiki/Logical_Decoding_Plugins too
T 1568050718 18<davidfetter_work18>	what would you guess is causing the slowness?
T 1568050732 18<davidfetter_work18>	thanks, lluad!
T 1568050737 18<RhodiumToad18>	what slowness?
T 1568050741 18<davidfetter_work18>	in COPY
T 1568050751 18<RhodiumToad18>	uh
T 1568050756 18<RhodiumToad18>	who said it was slow?
T 1568050789 18<davidfetter_work18>	"I suspect that byteain's performance is lost in the noise, but I haven't profiled it" seems to imply that there's something dominating the time it takes that's not byteain
T 1568050840 18<RhodiumToad18>	well copy has to do a whole bunch of stuff
T 1568050861 18<RhodiumToad18>	reading lines, splitting into fields, calling the input functions, forming tuples, writing them to the table
T 1568050901 18<whartung18>	but thats (mostly) CPU dominated which is less of a problem on modern systems, where I/O is still the primary barrier, yes?
T 1568050923 18<davidfetter_work18>	turns out a lot of DB loads are now CPU-bound
T 1568050942 18<davidfetter_work18>	people are often not on spinning rust
T 1568050994 18<davidfetter_work18>	RhodiumToad, so at least in theory, it should be possible to skip a lot of that by making stuff in the form COPY BINARY expects
T 1568051009 18*	davidfetter_work looks at the docs for this
T 1568051055 18<RhodiumToad18>	in theory, but what I found for COPY TO was that the BINARY path actually had higher overheads (that shouldn't be so much of an issue for COPY FROM but who knows)
T 1568051087 18<RhodiumToad18>	in particular, Ryu made COPY TO of a table with a lot of float columns run faster in text mode than in binary mode (!)
T 1568051098 18<whartung18>	!!!
T 1568051110 18<davidfetter_work18>	that is so cool :)
T 1568051110 18<whartung18>	Thatis not intuitive.
T 1568051123 18<davidfetter_work18>	have you seen the ryu code?
T 1568051223 18<RhodiumToad18>	whartung: float output in some cases is now faster than bigint output
T 1568051238 18<whartung18>	nice
T 1568051239 18<RhodiumToad18>	though that might say more about bigint output than anything else
T 1568051254 18<whartung18>	by bigint you mean 64b or > 64b int?
T 1568051262 18<RhodiumToad18>	64 bit
T 1568051284 18<whartung18>	thats interesting
T 1568051294 18<whartung18>	floats are 80 bit (right?)
T 1568051297 18<RhodiumToad18>	64
T 1568051301 18<whartung18>	youd think theyd be competeitive
T 1568051303 18<whartung18>	ok
T 1568051325 18<RhodiumToad18>	80 bit was for i387-style long doubles, used for intermediate values
T 1568051390 18<whartung18>	just youd think that the FP to ascii conversion would be comutationlly more complex (however marginally) than INT -> ascii
T 1568051398 18<RhodiumToad18>	ryu gets its speed from a very clever algorithm plus a bunch of micro-optimizations
T 1568051462 18<whartung18>	I wondr at waht point its cheaper to look up values than calculate them when rendering numbers.
T 1568051475 18<RhodiumToad18>	the algorithm combines the process of finding the shortest decimal fraction value within the rounding interval with the process of multiplying by powers of 10 to convert to decimal
T 1568051497 18<RhodiumToad18>	it does use a lookup table, but not a huge one
T 1568051513 18<whartung18>	right, you wouldt need a huge one  diminishing returns very quickly
T 1568051524 18<DrEeevil18>	whartung: you might enjoy reading libgmp ;)
T 1568051556 18<whartung18>	is that the big decimal library?
T 1568051560 18<RhodiumToad18>	ryu's lookup tables are (for 64-bit) 292 and 326 entries of 128 bits each
T 1568051572 18<whartung18>	nice
T 1568051727 18<whartung18>	yea I had never heard of ryu
T 1568051749 18<RhodiumToad18>	https://dl.acm.org/citation.cfm?id=3192369
T 1568051813 18<RhodiumToad18>	thing is, other fast floating-point output functions have tended to be a whole lot more complex
T 1568051841 18<davidfetter_work18>	egad
T 1568051846 18<davidfetter_work18>	here I thought ryu was complex
T 1568051853 18<RhodiumToad18>	not at all
T 1568051870 18<RhodiumToad18>	I guess it's complex by comparison to integer output, but by float standards it's not
T 1568052087 18<davidfetter_work18>	you kinda implied that integer output might be improved, too. probably not worth doing, unless people start to complain
T 1568052222 18<RhodiumToad18>	some of the optimizations that ryu uses, like 2-digit output tables, would work well for integers
T 1568052242 18*	davidfetter_work reading https://tia.mat.br/posts/2014/06/23/integer_to_string_conversion.html
T 1568052274 18<RhodiumToad18>	I focused on floats because there was evidence that it was an actual pain point for users with large volumes of float data
T 1568052287 18<davidfetter_work18>	right
T 1568052295 18<RhodiumToad18>	as in, result transfers being 3x slower than they should have been
T 1568052316 18<davidfetter_work18>	ouch
T 1568053590 18<Orez18>	is it possible to return raw bytes from a `psql -Atf` call? right now i'm `encode(bytea, 'base64')`-ing my result and then piping through base64 -D
T 1568053961 18<davidfetter_work18>	Orez, is it just one column?
T 1568053978 18<Orez18>	yes, one column one row
T 1568053990 18<davidfetter_work18>	so you don't need it NULL-terminated
T 1568053994 18<xocolatl18>	COPY can do that, I think
T 1568054091 18<davidfetter_work18>	I tried COPY...BINARY, and it includes more stuff
T 1568054099 18<davidfetter_work18>	what should I be doing instead?
T 1568054152 18<xocolatl18>	what was your test case?
T 1568054164 18<davidfetter_work18>	davidfetter=# COPY (SELECT * FROM pwnd WHERE passwd = digest('', 'sha1')) TO stdout BINARY;
T 1568054167 18<davidfetter_work18>	PGCOPY
T 1568054170 18<davidfetter_work18>	
T 1568054172 18<davidfetter_work18>	o M:[2!GDTime: 108.037 ms
T 1568054188 18<davidfetter_work18>	I have a few sha1's expressed as byteas in that DB
T 1568054245 18<xocolatl18>	ah right, it has the whole varlena info in there
T 1568054256 18<xocolatl18>	Orez: I don't think it's possible
T 1568054260 18<RhodiumToad18>	binary format includes a bunch of stuff like field lengths and so on
T 1568054267 18<RhodiumToad18>	it's not expected to print well on a tty
T 1568054359 18<RhodiumToad18>	Orez: you'd have to either post-process the copy-binary format, or stick with base64 or hex
T 1568054377 18<RhodiumToad18>	psql isn't exactly aimed at binary work
T 1568054460 18<Orez18>	dang, that's a shame. i dont know a whole lot about this, but i wonder if there's some encoding i could set on the db to return a "text" of raw bytes?
T 1568054481 18<RhodiumToad18>	how raw?
T 1568054488 18<RhodiumToad18>	do they include null bytes?
T 1568054496 18<Orez18>	ah yeah potentially
T 1568054499 18<RhodiumToad18>	then no
T 1568054521 18<RhodiumToad18>	do you have a specific need to use psql rather than some other client?
T 1568054592 18<davidfetter_work18>	how about decode() ?>
T 1568054655 18<RhodiumToad18>	post-processing the COPY binary format should be pretty simple btw
T 1568054673 18<davidfetter_work18>	byteaout ?
T 1568054674 18<Orez18>	RhodiumToad: no real *need*, just trying to see how far i can push this idea heh. was curious if i could write an `imgcat` function in psql
T 1568054693 18<RhodiumToad18>	ahh
T 1568054718 18<RhodiumToad18>	if you want to take a bytea value from the db and write it to a file on the client, that's actually easier
T 1568054771 18<RhodiumToad18>	(if you're not worried about absolute performance)
T 1568054788 18<RhodiumToad18>	the trick is to create a lo object from the bytea and use \lo_export to write it to a file
T 1568054816 18<RhodiumToad18>	something like,
T 1568054956 18<RhodiumToad18>	begin;
T 1568054984 18<RhodiumToad18>	select lo_from_bytea(img) as o_oid from images where id='whatever' \gset
T 1568054997 18<RhodiumToad18>	\lo_export :o_oid filename.jpg
T 1568054999 18<RhodiumToad18>	rollback;
T 1568055017 18<RhodiumToad18>	(or replace the rollback with  select lo_unlink(:'o_oid'); commit;
T 1568055039 18<RhodiumToad18>	er sorry
T 1568055045 18<RhodiumToad18>	lo_from_bytea(0,img)
T 1568055204 18<Orez18>	ah interesting. that's good to know
T 1568055231 18<Orez18>	thanks all!
T 1568055546 18<gfree7618>	help buffer
T 1568055556 18<gfree7618>	lol
T 1568058444 18<theseb18>	Why does this simple view def give a syntax error?  https://pastebin.com/izHEeWEA
T 1568058489 18<test9142318>	hi. I'm running into an issue where postgres claims 'ERROR:  null value in column "senderid" violates not-null constraint'. however, if I do "SELECT * from table WHERE senderid IS NULL;" I don't get any hits. what am I doing wrong?
T 1568058524 18<theseb18>	Here is the error: https://pastebin.com/ez0t6ep9
T 1568058673 18<nbjoerg18>	theseb: create view ... as select ...
T 1568058702 18<nbjoerg18>	theseb: do you want to create a *table*?
T 1568058713 18<theseb18>	nbjoerg: yea....what if i have data in database to do a select with?  i just want to define a table and then add some rows!? ;)
T 1568058727 18<theseb18>	nbjoerg: i mean...what if i DO NOT have data
T 1568058730 18<theseb18>	to do select with?
T 1568058760 18<nbjoerg18>	theseb: a view by nature is always a query
T 1568058772 18<nbjoerg18>	theseb: if you want a place to temporarily stash data, that's a temp table
T 1568058813 18<theseb18>	nbjoerg: i'm on Apache Spark and when i tried create temp table it said "those aren't supported...try views instead!?!!?!?"
T 1568058814 18<theseb18>	Ahhh!
T 1568058866 18<nbjoerg18>	I can only help you with the Mechanicsburg kind of sparks, sorry
T 1568059224 18<jdv7918>	i have a table with an pgarray of ids for which i have a seperate lookup table.  how could i go about mapping the pgarray over that lookup?
T 1568059272 18<theseb18>	nbjoerg: thanks
T 1568059275 18<theseb18>	again
T 1568060306 18<jdv7918>	nevermind - figured it
T 1568062040 18<theseb18>	help! How can i store and access values WITHOUT touching the database? Why do i care?... i want to do the equivalent of a for loop (i.e. run some SQL code and change some values for each loop)....problem is I don't have perms to create a temporary table! errr.
T 1568062078 18<nbjoerg18>	theseb: pl/pgsql?
T 1568062089 18<theseb18>	perhaps SQL has something like named constants?.... MY_VAR = "foo"; ?
T 1568062113 18<peerce18>	plpgsql does
T 1568062114 18<nbjoerg18>	what's wrong with using your language of the day? e.g. python or whatever?
T 1568062128 18<peerce18>	and change some values, meaning, update stuff ?
T 1568062136 18<peerce18>	or just return modified values ?
T 1568062412 18<theseb18>	nbjoerg: actually....i do have a python script involved here....that isn't a bad idea
T 1568062471 18<peerce18>	theseb; again, when you say, 'store and access' and 'change some values for each loop', do you mean, to do an UPDATE of an existing table with these changed values ?  or what?
T 1568062584 18<theseb18>	peerce: : i have an SQL script that grabs data from a database....i want to get data for say...company X, then company Y, then company Z, etc.
T 1568062625 18<theseb18>	peerce: the database is not storing the specific stuff for companies X, Y and Z....but i don't want to create 3 massive SQL scripts that only vary in a few values that are different for each company
T 1568062631 18<theseb18>	peerce: does that clear it up?
T 1568062681 18<peerce18>	not really.    why can't this just be a SELECT statement that fetches stuff IN (companyX, companyY, companyZ)  ?
T 1568062790 18<theseb18>	peerce: does sql let you do a select from raw data?....e.g. select name, ticker from ( ("GOOG" 333.333) ("IBM" 22.22) ...) etc.?
T 1568062800 18<theseb18>	peerce: something like that WOULD do the trick nicely
T 1568062818 18<peerce18>	theseb; so the data isn't even in the database??
T 1568062835 18<theseb18>	peerce: that's the problem....i have a few values that i can't add to the DB
T 1568062855 18<peerce18>	so where is this data if its not in the database ?
T 1568062903 18<theseb18>	peerce: i have to search and get it from other sources
T 1568062922 18<peerce18>	so why do yuou need to use a sELECT, you've already got it in your app?  just use it.
T 1568063014 18<theseb18>	peerce: is it possible to do a select from a tuple of values instead of a table like my example above?.. select name, ticker from ( ("GOOG" 333.333) ("IBM" 22.22) ...) etc.?
T 1568063025 18<theseb18>	peerce: or some similar goo?
T 1568063033 18<peerce18>	why would you send data to the database just to read it back  ?
T 1568063051 18<peerce18>	[if you're not storing it in the database, I mean...]
T 1568063053 18<RhodiumToad18>	theseb: yes
T 1568063084 18<RhodiumToad18>	select name,ticker from (values ('GOOG',333.333),('IBM',22.22),...) as v(name,ticker);
T 1568063096 18<Xgc18>	<table value constructor>
T 1568063099 18<RhodiumToad18>	though arrays are more efficient
T 1568063106 18<RhodiumToad18>	or json
T 1568063148 18<theseb18>	RhodiumToad: nice...beautiful
T 1568063176 18<theseb18>	peerce: there are more complicated SQL scripts i need to run for various changing values
T 1568063184 18<RhodiumToad18>	select key as name, value as ticker from json_each('{"GOOG":333.333, "IBM":22.22, ...}');
T 1568063235 18<RhodiumToad18>	the advantage of using json is that the data can be passed in as a single literal; long VALUES clauses are quite inefficient to parse
T 1568063264 18<RhodiumToad18>	er, probably want json_each_text there
T 1568063292 18<RhodiumToad18>	or you can do things like,
T 1568063337 18<RhodiumToad18>	select name, ticker from json_to_recordset('[{"name":"GOOG", "ticker":333.333}, {...}, ...]') as j(name text, ticker numeric);
T 1568063718 18<peerce18>	so you're doing joins with this app supplied data and other existing tables?     hopefully there's not too much of this supplied data, json or otherwise, as that sort of construct is not indexed, so complex operations on it may not perform very well
T 1568063894 18<theseb18>	peerce: here is my SQL code... https://pastebin.com/YydDrUgD ... see those hex strings? I basically want to run the same code and just modify those hex strings each time .. and store the result of each run
T 1568063900 18<theseb18>	peerce: that's the deal....nothing hidden
T 1568063986 18<RhodiumToad18>	you have "..." there which should be '...'
T 1568064018 18<RhodiumToad18>	if these are binary values why are you storing them as hex rather than bytea?
T 1568064055 18<theseb18>	RhodiumToad: dunno..i didn't set up the database
T 1568064276 18<peerce18>	why not just pass those as arguments to the queries then?     WHERE .... = LOWER($1) AND ... = LOWER($2) AND ...
T 1568064398 18<theseb18>	peerce: because i've never learned the WHERE before? but i think that will be perfect
T 1568064407 18<theseb18>	time to learn it...;)
T 1568064412 18<theseb18>	i <3 SQL
T 1568064418 18<theseb18>	SQL forevar
T 1568071152 22*	26ChanServ gives channel operator status to 18xocolatl
T 1568071156 22*	26xocolatl has changed the topic to: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568071161 22*	26ChanServ removes channel operator status from 18xocolatl
T 1568071384 18<davidfetter_work18>	w00t!
T 1568074509 18<ningu18>	is there a guide somewhere to dos and don'ts for working with json data?
T 1568074537 18<ningu18>	I don't particularly _want_ to use json for an upcoming project but we'll be dealing with a certain thing that comes in a variety of formats so it's kind of the only/best choice
T 1568074560 18<ningu18>	but I'd like to be sure I understand better what tradeoffs I'll be making, like, if I want to query it a certain way, if that will be really annoying or what
T 1568074566 18<ningu18>	or if there are ways to make it a little easier
T 1568074589 18<RhodiumToad18>	querying for containment is reasonably flexible with a gin index
T 1568074609 18<RhodiumToad18>	querying for things like inequality comparison on deeply nested keys is a pain
T 1568074669 18<ningu18>	RhodiumToad: how about "full text search" within a json record? meaning like match any value (not key)?
T 1568074684 18<RhodiumToad18>	that's extremely painful
T 1568074692 18<ningu18>	what about if it's not nested?
T 1568074701 18<RhodiumToad18>	still painful
T 1568074706 18<ningu18>	hrm ok
T 1568074741 18<xocolatl18>	hey RhodiumToad, do you remember a while ago I asked about C-level scanning through an index or seqscan?  you said only CLUSTER made a clumsy decision about that and I should go through SPI.  I just found systable_beginscan (in src/backend/access/index/genam.c) that seems to do what I was looking for.
T 1568074748 18<ningu18>	so how would you handle that case? you have records that are basically freely defined key/value pairs, and you need to store them (associated with some more regularly structured data), and you need to query to see if any value matches
T 1568074764 18<ningu18>	RhodiumToad: I mean something like EAV can do it, I just know everyone says no (and I understand why)
T 1568074813 18<RhodiumToad18>	xocolatl: that only seqscans when forced to
T 1568074868 18<ningu18>	well honestly, it wouldn't be that hard to denormalize and add a field that has a concatenation of all the values
T 1568074896 18<ningu18>	and just use it for matching, then use the actual data (in json) for display
T 1568075013 18<ningu18>	RhodiumToad: is that maybe the least bad option for this?
T 1568075029 18<ningu18>	could use trgm index maybe
T 1568075033 18<RhodiumToad18>	depends what the objective is
T 1568075058 18<ningu18>	the objective is mainly to determine, "does any value of these key/value pairs contain the given search term(s)?"
T 1568075074 18<RhodiumToad18>	yes, but why
T 1568075148 18<ningu18>	each set of key/value pairs is a dictionary entry. there's a word in some language being described, and its translation into english or similar major language. but there can be a lot of fields like gloss, definition, etymology, notes, etc. the search will be for someone saying, give me all the words in the database that contain the word "drink" somewhere in their entry
T 1568075160 18<ningu18>	we need to preserve the original structure of the entry in case the user wants to inspect it manually
T 1568075180 18<ningu18>	we will extract as much regular data as possible but that will be limited by the way these are structured
T 1568075231 18<RhodiumToad18>	where "drink" is matches exactly as a word, or stemmed tsearch-style, or matched by substrings?
T 1568075257 18<ningu18>	yeah, I thought of that. matches exactly as a word is definitely the simplest case and probably all we need to support.
T 1568075276 18<ningu18>	it would be a regex where the word is flanked by \b, or similar.
T 1568075286 18<ningu18>	so like it would match "drink-water" but not "drinking"
T 1568075294 18<RhodiumToad18>	have a function that returns a flat list of all the words in the entry as an array, and index that too
T 1568075307 18<ningu18>	hmm... ok
T 1568075318 18<ningu18>	so it will come down to just whether the array contains the word, cool
T 1568075342 18<ningu18>	that would work well
T 1568075353 18<ningu18>	what about the stemming case, just curious?
T 1568075419 18<RhodiumToad18>	well then you'd return it as a tsvector not an array
T 1568075505 18<ningu18>	ah ok, right, but I guess you'd generate the tsvector from all the unique words in the record
T 1568076572 18<velix18>	Sorry for abusing you as doc, but what was the function to count NULLIF over columns?
T 1568076603 18<velix18>	oh wait, let's do it the easy way: COUNT(*) FILTER (WHERE a IS NULL) ;)
T 1568076688 18<velix18>	num_nulls()!
T 1568076767 18<peerce18>	count(1)-count(field)
T 1568076782 18<peerce18>	since count(field) is the number of not nulls...
T 1568076792 18<RhodiumToad18>	num_nulls is for multiple arguments, not an aggregate
T 1568076799 18<RhodiumToad18>	count(*)-count(field)
T 1568076805 18<peerce18>	yeah, thats why I suggested the other.
T 1568076827 18<RhodiumToad18>	or  count(nullif(field is null,false))
T 1568076833 18<velix18>	so num_nulls(col1, col2, col3) doesn't work?
T 1568076842 18<RhodiumToad18>	or  sum((field is null)::integer)
T 1568076857 18<RhodiumToad18>	velix: it does work, but I thought you wanted an aggregate?
T 1568076858 18<xocolatl18>	eww on that last one
T 1568076870 18<RhodiumToad18>	oh, you said "over columns"?
T 1568076884 18<RhodiumToad18>	so count() was the wrong approach anyway
T 1568076892 18<velix18>	RhodiumToad: Yes, not over rows. But since we're talking about it: can't I sum(num_nulls) ?
T 1568076904 18<RhodiumToad18>	only by casting to integer
T 1568076907 18<velix18>	sure
T 1568076918 18<RhodiumToad18>	oh wait, it is integer
T 1568076921 18<velix18>	:D
T 1568076954 18<velix18>	PostgreSQL has solutions for problems I don't have. Now that's cool.
T 1568079980 18<maybefbi18>	has anyone used postgrest? what do you think of it?
T 1568080252 18<peerce18>	I haven't, and I think its a solution in search of a problem.   I think the REST thing belongs between the clients and the appserver where you implemnt your business logic.    the app server should use direct SQL connections since it can and they give the best performance.
T 1568080301 18<maybefbi18>	peerce: by direct SQL connections do you mean no ORM?
T 1568080317 18<peerce18>	I haven't found an ORM yet that didn't end up pissing me off.
T 1568080323 18<maybefbi18>	i agree.
T 1568081775 18*	wfpkhc wanders on in
T 1568081777 18<wfpkhc18>	hello
T 1568081799 18<RhodiumToad18>	good evening
T 1568081807 18*	wfpkhc hugs RhodiumToad!
T 1568081812 18<wfpkhc18>	your still around! yaay!
T 1568081880 18<wfpkhc18>	may i please ask the following: if i have a left join on a table  - and im using a "Case when COLUMN is null"  - is there anyway to tell if it "is  null and the column id is a certain id?
T 1568081923 18<RhodiumToad18>	er, yes?
T 1568081928 18<RhodiumToad18>	what is the whole query?
T 1568081958 18<wfpkhc18>	errrm let me write it out in a simple form its a ugly query  like its creator - 1 second please
T 1568082224 18<wfpkhc18>	https://dpaste.de/m8zy
T 1568082273 18<wfpkhc18>	https://dpaste.de/rxAh
T 1568082283 18<wfpkhc18>	there is a more cleaneded up one
T 1568082350 18<wfpkhc18>	what i would like to try and do is - if the alias is null and the id == 5 then display the word "system" else display unknown
T 1568082609 18<RhodiumToad18>	hm
T 1568082637 18<wfpkhc18>	i have to go
T 1568082640 18<wfpkhc18>	im sorry
T 1568082644 18<wfpkhc18>	will come back later
T 1568082654 18*	wfpkhc hugs RhodiumToad and runs off hoping he remembers me
T 1568082658 18<RhodiumToad18>	easy enough with another CASE
T 1568085862 18<Exuma18>	how can I combine 2 table results (from 2 matviews) into another view
T 1568085871 18<Exuma18>	ideally i could map columns
T 1568085880 18<Exuma18>	or somehow exclude columns from one
T 1568085886 18<Exuma18>	but i want A + B results
T 1568085962 18<FunkyBob18>	you mean like a UNION ?
T 1568086038 18<Exuma18>	FunkyBob hmm perhaps
T 1568086046 18<Exuma18>	?? union
T 1568086046 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/queries-union.html
T 1568086152 18<Exuma18>	oh awesome, yeah jus tlike that. thanks FunkyBob
T 1568086423 18<Exuma18>	UNION ALL actually
T 1568088323 18<davidfetter18>	hi
T 1568088456 18<davidfetter18>	in a system where there's a lot of Ruby on Rails, does it make sense to wrap a simple pwnd(sha1 bytea) function that returns bool in a https endpoint, or a special-purpose DB endpoint?
T 1568092556 18<indrek18>	How do you understand what is postgersql actually doing? I have table A and daily summary table B. I inserted rows to table A from old table A (i did partitions) and now when i look at \dt+ i see that size is not changing any more in Tables A, but it's table B size is changing. Table B gets updated with after trigger. So my question is what is pg actually doing?
T 1568092594 18<indrek18>	Is it triggering all after insert triggers after all data is inserted? Or it's actually doing an insert and triggering insert to table B?
T 1568093769 18<davidfetter18>	indrek, with native partitioning, only the leaves can have data
T 1568093801 18<davidfetter18>	is there some reason the current query to pg_stat_activity shows up in pg_stat_activity by default?
T 1568093806 18<indrek18>	yes, only the leaves have data
T 1568093826 18*	davidfetter constantly putting pid <> pg_backend_pid() in WHERE clauses, and it's kinda tedious
T 1568093844 18<indrek18>	the query i executed shows up in pg_backend_pid
T 1568093882 18<indrek18>	with a state active
T 1568093891 18<indrek18>	so i suppose, it's running
T 1568093895 18<davidfetter18>	the query on pg_stat_activity in the current session? yes, and that's pretty useless as far as I've seen so far
T 1568093906 18<indrek18>	yes
T 1568093911 18*	davidfetter thinks it shouldn't be part of that view
T 1568093918 18<indrek18>	why?
T 1568093943 18<davidfetter18>	what conceivable use would seeing the query on pg_stat_activity that you just wrote have?
T 1568094011 18<indrek18>	to see it it's waiting on some locks etc
T 1568094043 18<davidfetter18>	have you ever seen such a query wait on locks in its own output?
T 1568094072 18<indrek18>	not on it's own
T 1568094082 18<indrek18>	but others waiting on your query
T 1568094157 18<davidfetter18>	you've seen this?
T 1568094180 18<davidfetter18>	by the time it produces rows, such conflicts are by definition resolved :P
T 1568094252 18<indrek18>	actually i have
T 1568094268 18<indrek18>	because years ago someone made a script on update
T 1568094272 18<indrek18>	and it didn't have filter
T 1568094284 18<indrek18>	so with each row it updated all rows and it was like a snowball
T 1568094294 18<indrek18>	finally i had so many rows to update that it took minutes
T 1568094873 18<davidfetter18>	ok
T 1568094887 18*	davidfetter still thinks it wouldn't hurt to leave it out
T 1568095947 18<davidfetter18>	hrm. I've got a hash index and a unique b-tree index on the same column. before, the hash index was built, queries like SELECT EXISTS(SELECT 1 FROM pwnd WHERE passwd=digest('foobar','sha1')) would use the b-tree index and run in <= 3ms
T 1568095970 18<davidfetter18>	with the hash index there, such queries use it instead and run in <= 5ms :/
T 1568095988 18<aypea[0]18>	wow. that said, why both?
T 1568095999 18<davidfetter18>	i wanted to see which was faster
T 1568096003 18<aypea[0]18>	ah :)
T 1568096010 18<davidfetter18>	there's only that query that's going to run against this DB
T 1568096058 18<aypea[0]18>	only place I've used hash indexes in was where it made inserts a ton faster (eventually, once the table got big enough). not sure if it was due to structure or size (the hash index was around 1/3rd the size of btree)
T 1568096086 18<aypea[0]18>	didn't care about search speed as that, at least, made the system work and saved a few TB of disk
T 1568096115 18<davidfetter18>	theoretically, this is perfect for hash indexes--equi-joins on fixed strings only, the hash index is ~3/4 of the size of the B-tree index
T 1568096155 18<davidfetter18>	practically, though, the b-tree index is faster
T 1568096177 18<aypea[0]18>	my use-case was equality comparisons of fixed-width strings (generally) :)
T 1568096205 18<davidfetter18>	pretty similar here. I'm comparing byteas (sha1s of pwnd passwords)
T 1568096219 18<aypea[0]18>	hahahaha. same except I went with sha512 :)
T 1568096223 18<davidfetter18>	they are of course fixed length
T 1568096241 18<davidfetter18>	I got this big DB of sha1s of leaked passwords
T 1568096254 18<davidfetter18>	three wits used "correct horse battery staple"
T 1568096269 18<aypea[0]18>	xkcd passwd :)
T 1568096273 18<davidfetter18>	exactly
T 1568096359 18<aypea[0]18>	it was a cunning plan but, I believe, the list of 4 likely human friendly words is a lot smaller than the list of 4 words.
T 1568096421 18*	davidfetter wonders how the de-duplicated b-tree indexes Anastasia and Peter are working on would do in this case
T 1568096439 18<aypea[0]18>	de-duplicated?
T 1568096472 18<davidfetter18>	per the latest commit message, "Add deduplication to nbtree"
T 1568096521 18<davidfetter18>	at least in theory, this shouldn't benefit from that particular optimization, but it might benefit from other ones they're tucking into this patch
T 1568096568 18*	davidfetter just a little lazy about recompiling, reloading 20something GB of stuff, then indexing it
T 1568096643 18<aypea[0]18>	hrm. can't see the commit in gitlandia.
T 1568097780 18<davidfetter18>	patch hasn't landed yet
T 1568097819 18<davidfetter18>	Message-ID: <CAH2-Wzm=9TnAFGCDfvsBVC5zYonQqeLMmYpnx=xZ3nyXOeHjNA@mail.gmail.com>
T 1568097861 18<davidfetter18>	https://www.postgresql.org/message-id/flat/CAH2-Wzm%3D9TnAFGCDfvsBVC5zYonQqeLMmYpnx%3DxZ3nyXOeHjNA%40mail.gmail.com#c0c84272254c067ac467a5c75ab32dd9
T 1568097863 18<aypea[0]18>	ah
T 1568097969 18<aypea[0]18>	thanks for the url :)
T 1568098016 18<davidfetter18>	most of what's in that thread is *way* over my head. I follow these things just enough to put them in the weekly newsletter, as best I understand them
T 1568098026 18<davidfetter18>	sometimes, my best isn't quite good enough
T 1568098467 18<aypea[0]18>	same :)
T 1568101832 18<ysch18>	davidfetter: Which PostgreSQL version was it (for hash indexes testing), just curious? And approx. index sizes, RAM, and were those index-only scans for b-tree?
T 1568102539 18<johnny_b18>	hello
T 1568102540 18<davidfetter18>	ysch, 13 as of a couple of weeks ago
T 1568102567 18<davidfetter18>	16GB for hash, 21GB for b-tree
T 1568102593 18<davidfetter18>	16G RAM
T 1568102600 18<davidfetter18>	and yes
T 1568102619 18<davidfetter18>	johnny_b, good
T 1568102621 18<davidfetter18>	sorry
T 1568102637 18<johnny_b18>	np, it happens 8)
T 1568102781 18*	davidfetter bumps a couple of chuck berry tracks
T 1568102942 18<johnny_b18>	davidfetter: nice 8)
T 1568102978 18<ysch18>	davidfetter: "Yes" for IOS, right? Thanks for the info!
T 1568102993 18<johnny_b18>	i'd like to use the hungarian hunspell files for full text search but it can't be imported because the UTF-8 formatted affix file has non-UTF-8 chars (https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=737049). how can i make them work?
T 1568102994 18<davidfetter18>	right
T 1568103022 18<peerce18>	what charset are these non-utf8 chars in ?
T 1568103041 18<peerce18>	and is there some consistent way you can tell whats what?
T 1568103045 18<Myon18>	that file is totally crazy
T 1568103075 18<johnny_b18>	Myon: have you seen it?
T 1568103091 18<Myon18>	johnny_b: I'm the one commenting in the last message in that bug
T 1568103103 18<johnny_b18>	fabulous
T 1568103151 18<johnny_b18>	it seems that comments are still in ISO-8859-1 or 2
T 1568103151 18<Myon18>	I don't know the affix file format in detail, but I'd think you could just replace <ff> by some other character
T 1568103303 18<johnny_b18>	i see
T 1568103680 20*	Disconnected (20)
T 1568103704 19*	Now talking on 22#postgresql
T 1568103704 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568103704 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Tue Sep 10 01:19:16 2019)
T 1568103704 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568103841 18<Myon18>	"just" appears to be a horrible mess
T 1568103854 18<denaras18>	Hey - we are at 11.5 Postgresql, and we wonder how we can scale our database. Most usage is for read heavy workloads. We are thinking to setup stream replications and have one or two replicas. I just wonder is there anything better for 11.5 (compared to 9.3)?
T 1568103857 18<Myon18>	REP ^orr orv # orrvadsz->orvvadsz
T 1568103857 18<Myon18>	AF 1478
T 1568103857 18<Myon18>	AF Vj<D7>L<D3>n<F2><E9><E8><B3><C4><E4>TtYc<B8><BC>l # 1
T 1568103882 18<Myon18>	denaras: that'd be the way for read scaling, yes
T 1568103899 18<Myon18>	or simply put in more cores and more RAM
T 1568103974 18<denaras18>	Myon: thanks, we are taking right now backups from primary server for 10 hours :(. So yeah replica will solve this. I just wonder few more questions
T 1568103998 18<denaras18>	How we know what replica is in sync? How we can resync? What will happen if let say I turn off replica for 1 day and start again ?
T 1568104008 18<Myon18>	denaras: pg_stat_replication
T 1568104016 18<Myon18>	??replication
T 1568104016 18<pg_docbot18>	http://momjian.us/main/writings/pgsql/replication.pdf :: http://wiki.postgresql.org/wiki/Streaming_Replication
T 1568104016 18<johnny_b18>	Myon: yes, unfortunately it is 8(
T 1568104016 18<pg_docbot18>	http://wiki.postgresql.org/wiki/Binary_Replication_Tutorial :: http://repmgr.org/
T 1568104016 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/protocol-replication.html :: https://wiki.postgresql.org/wiki/Trigger-based_Replication
T 1568104016 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/high-availability.html :: https://www.postgresql.org/docs/current/static/warm-standby.html#SYNCHRONOUS-REPLICATION
T 1568104019 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/standby-settings.html#RECOVERY-MIN-APPLY-DELAY
T 1568104030 18<denaras18>	thanks
T 1568104031 18<Myon18>	johnny_b: is aspell any better?
T 1568104134 18<johnny_b18>	Myon: i dunno , hunspell was the first choice because it's up-to-date
T 1568104143 18<denaras18>	Myon: what would happen if we stop replica for 1 day and we start it again? Will we need to start over - clear data folder, do pg_basebackup and setup everything again ?
T 1568104161 18<Myon18>	denaras: you need to keep WAL for one day around
T 1568104172 18<Myon18>	using "replication slots" or a "wal archive"
T 1568104813 18<peerce18>	denaras; note that a day of WAL couold ee a lot of storage ona busy write intensive server
T 1568104863 18<peerce18>	with replication slots, that day would be stored in your pg_wal folder under the pgdata main data directory.
T 1568104890 18<peerce18>	with a wal archive, it could be on a seperate storage server
T 1568105310 18<blip9918>	guys how do i reference fields in a jsonb field?
T 1568105329 18<blip9918>	tablename.data->name doesn't cut it
T 1568105345 18<Myon18>	->'name'
T 1568105348 18<incognito18>	is there are a builtin tool a parallelize the copy of pg backup ? (with : compression)
T 1568105365 18<Myon18>	pg_dump -Fd -j8
T 1568105370 18<davidfetter18>	pg_dump -Fd -j $(nproc)
T 1568105379 18<incognito18>	network copy also
T 1568105385 18<Myon18>	add -h
T 1568105394 18<incognito18>	yes, i only see pg_dump -j8 && rsync ...
T 1568105395 18<blip9918>	Myon, ah. perfect. and seems like ->> removes the quotes from the string
T 1568105401 18<Myon18>	the compression is on the local side only, though
T 1568105415 18<davidfetter18>	blip99, ->> gets text instead of JSON[B]
T 1568105423 18<blip9918>	thanks david
T 1568105451 18<incognito18>	no extension that are automatically doing : pg_start_backup() && rsync .. compressed ... && pg_end_backup() ?
T 1568105510 18<Myon18>	pgbackrest
T 1568105653 18<xocolatl18>	it would be nice if pg_basebackup had a -j option
T 1568105705 18<davidfetter18>	^^^^^^
T 1568105715 18<davidfetter18>	aiui, people are working on it :)
T 1568105759 18<incognito18>	note for myself: read pg_backrest doc :)
T 1568105778 18<xocolatl18>	incognito: barman can also do it
T 1568106114 18<pascalou18>	hello
T 1568106199 18<pascalou18>	max conn is 200, all roles connlimit is -1 ; all db are datallowconn =t, why would  get a to many connection with a user?
T 1568106229 18<xocolatl18>	because you have 200 connections already?
T 1568106234 18<pascalou18>	no , 2
T 1568106249 18<xocolatl18>	what is the exact error?
T 1568106316 18<pascalou18>	user=sssss,db=nnnnn,app=[unknown],client=xxxxx FATAL:  too many connections for database "zzzzz"
T 1568106542 18<ysch18>	pascalou: What's datconnlimit for the database "zzzzz" (in pg_database)?
T 1568106596 18<davidfetter18>	I was hoping to make printing integers more efficient. make goes without errors, but "make check" bombs out immediately with a segfault. http://paste.ubuntu.com/p/x3w7BmrCG3/
T 1568106599 18<pascalou18>	ok , thanks
T 1568106609 18<pascalou18>	that was it
T 1568106633 18<Myon18>	I'm not sure if I knew about that parameter
T 1568106663 18<pascalou18>	we usualy never set it , usually we set datallowconn=f
T 1568106755 18<xocolatl18>	you used to have to update the catalogs to set it
T 1568106761 18<xocolatl18>	so I wrote a patch :)
T 1568106960 18<davidfetter18>	anybody seeing something obvious that I missed?
T 1568107971 18<johnny_b18>	Myon: there are other problematic byte sequences too, not just 0xff
T 1568108021 18<Myon18>	johnny_b: there must be source code for that file, that's definitely not written by hand
T 1568108041 18<Myon18>	johnny_b: maybe fixing the "compiler" not to use funky bytes is feasible
T 1568108053 18<Myon18>	given that only Hungarian has that problem
T 1568108082 18<johnny_b18>	yeah 8(
T 1568109400 18<incognito18>	
T 1568111648 18<zerowalker_w18>	hi, anyone on windows or know how to make ssl work? I am self signing a certificate and giving it a ssl_key_file and ssl_cert_file file and ssl=on. But it won't start and i get no logs, it will start if i don't give nay key/cert but with unknown authority which make sense. Any idea what i can be missing, i am using openssl. "openssl req -new -x509 -days 365 -nodes -text -out server.crt   -keyout server.key -subj "/CN=localhost""
T 1568111684 18<zerowalker_w18>	it's probably obvious, i always have problems with certificates, especially on postgres, so i have probably asked this before:(
T 1568111810 18<Myon18>	zerowalker_w: try launching postgres.exe directly
T 1568111825 18<Myon18>	somethings that gives some error messages that don't appear in the normal log
T 1568111835 18<Myon18>	(are you looking into the correct log file?)
T 1568111863 18<zerowalker_w18>	i am just looking in the logs folder, i wil ltry opening it directly
T 1568111878 18<zerowalker_w18>	though thing is it's a service
T 1568111923 18<zerowalker_w18>	log* folder
T 1568111923 18<Myon18>	postgres.exe -D c:\whatever\data\directory
T 1568112093 18<zerowalker_w18>	hmm i may have fixed it, doesn't make any sense though unless i failed and then messed it up, fixed it, but didn't re-messit lol xd
T 1568112159 18<zerowalker_w18>	okay it seems to work now, the error was i used " instead of ', which i didn't at first, but i later changed the crt/key file and destination
T 1568112177 18<zerowalker_w18>	so i am guessing some of those changes actually fixed it, but cause i used the " all those times it never worked
T 1568112195 18<zerowalker_w18>	using postgres.exe helped identify that the config was bad in this case, thanks!
T 1568112208 18<zerowalker_w18>	gotta go, cya
T 1568112759 18<Myon18>	cheers
T 1568113202 18<enoq18>	hi, how different is postgis to internal spatial stuff?
T 1568113208 18<enoq18>	asking because I saw that JPA has spatial support via postgis
T 1568113210 18<enoq18>	and I'd like to avoid it because I think it will make issues when updating it and running it inside docker
T 1568113240 18<capitol18>	what is "internal" in this context?
T 1568113246 18<[patrik]18>	hi guys. when im meassuring replication lag it seems to be mostly 0 which is perfect, but sometimes i see a small lag (under 100kb) that goes towards zero. How much replication lag is good threshold before saying "this is a problem"?
T 1568113297 18<enoq18>	internal as in: built in Point type and ST_Distance
T 1568113307 18<enoq18>	which is basically all that I need
T 1568113348 18<Myon18>	enoq: the builtin types don't have a ST_Distance function
T 1568113375 18<Myon18>	enoq: there's some very simple implementation in the earthdistance extension, though
T 1568113406 18<enoq18>	point <@> point
T 1568113408 18<Myon18>	[patrik]: we can't tell how much one transaction is worth at your side
T 1568113418 18<[patrik]18>	oh true
T 1568113446 18<Myon18>	enoq: that's straight distance. Germany-New Zealand is about 13000km. Is that ok?
T 1568113490 18<enoq18>	Myon: you mean on a sphere?
T 1568113506 18<[patrik]18>	Myon: i'll just save a table and see where it trends. Some kbs of replication lag is fine. But if it keeps trending upwards instead of towards 0 then that indicates a problem.
T 1568113507 18<enoq18>	as opposed to on earth
T 1568113522 18<Myon18>	enoq: in plain space, no surfaces involved
T 1568113534 18<enoq18>	oh, 2d space?
T 1568113546 18<Myon18>	3d space. right through the middle of the planet
T 1568113553 18<enoq18>	ah, crap
T 1568113586 18<enoq18>	Myon: same issues for cubic stuff?
T 1568113595 18<Myon18>	it'll be ok for small areas
T 1568113600 18<[patrik]18>	Myon: thanks.
T 1568113625 18<Myon18>	(postgis does the same, btw, if you use the "geometry" type, it's all planar coordinates)
T 1568113644 18<Myon18>	only the "geography" type has real 3d distances
T 1568113694 18<Myon18>	[patrik]: there is synchronous replication, but don't use it until you really understand the availability restrictions
T 1568113751 18<KekSi18>	if you're going to read "a shitload" and never want reads to be stale then that's your ticket
T 1568113781 18<enoq18>	Myon: and I suppose the geography type also assumes that the earth is a perfect sphere
T 1568113784 18<KekSi18>	inserts are going to be hella slow with it anyhow
T 1568113794 18<Myon18>	enoq: WGS84
T 1568113830 18<enoq18>	are there performance implications when choosing that type?
T 1568113837 18<Myon18>	it's slower, sure
T 1568113845 18<enoq18>	is there some rough estimate?
T 1568113856 18<enoq18>	order of magnitudes
T 1568113867 18<Myon18>	depends on what kind of queries you are doing
T 1568113882 18<enoq18>	select a table and order by distance from a given point
T 1568113896 18<enoq18>	table has 2000k entries atm
T 1568113901 18<enoq18>	ehm
T 1568113904 18<enoq18>	2000*
T 1568113918 18<enoq18>	we don't expect anything over 10000 really
T 1568113920 18<Myon18>	I have no idea, but in many cases the pure algorithmic cost will be much smaller than the overall query cost (round trip time etc)
T 1568113932 18<enoq18>	ok, good to hear
T 1568113990 18<enoq18>	would you recommend going for postgis then?
T 1568114033 18<Myon18>	avoiding it and inventing new code instead of using whatever your app might already have for postgis sounds like a bad plan
T 1568114096 18<enoq18>	area in question will probably be germany, switzerland and austria (maybe italy)
T 1568114143 18<Myon18>	maybe (flat) UTM coordinates are good enough then
T 1568114206 18<Berge18>	enoq: Test with your own data
T 1568114225 18<Berge18>	We're using the geography type wherever we can, and its performance is adequate for what we do
T 1568114268 18<enoq18>	the meh thing is that JPA spatial only seems to support geometry https://docs.jboss.org/hibernate/orm/5.4/userguide/html_single/Hibernate_User_Guide.html#spatial-configuration-dialect
T 1568114352 18<Myon18>	geometry has much more operators
T 1568114360 18<Myon18>	but you can cast around
T 1568114393 18<enoq18>	would be interesting if using geography on schema level works just fine with the geometry stuff
T 1568114414 18<Myon18>	possibly, but not all functions/operators will work
T 1568114491 18<enoq18>	another possiblity of course would be to order in application code
T 1568114763 18<enoq18>	is there a difference in precision regarding "point <@> point" and ST_Distance for Geometries
T 1568114772 18<enoq18>	since postgis and docker seems like a major pain
T 1568114888 18<Myon18>	using the same coordinate system, it should be the same
T 1568115051 18<enoq18>	thanks, will talk to coworkers
T 1568115105 18<mazula18>	it's the correct way to use enum ? https://gist.github.com/minas-tirith/d552d2b2b43e0e41ffac360f4e2d9726
T 1568115146 18<ilmari18>	mazula: you need to specify both the column name and the type name (which should probably not be the same) in the CREATE TABLE statement
T 1568115195 18<ilmari18>	mazula: also, only use enums if you know that a) you'll never want to remove a value (you can only add and rename) b) you'll never want to attach any metadata to the values
T 1568115214 18<mazula18>	types VARCHAR(255), like that?
T 1568115248 18<ilmari18>	mazula: no the type name is the name you used in CREATE TYPE ... AS
T 1568115259 18<Myon18>	.oO(length limits are for dbase users)
T 1568115374 18<mazula18>	ilmari Can i do that? https://gist.github.com/minas-tirith/d552d2b2b43e0e41ffac360f4e2d9726
T 1568115387 18<enoq18>	is there a reason why postgis is separate btw?
T 1568115409 18<Myon18>	PostgreSQL is extensible, so it doesn't have to be in core
T 1568115437 18<Myon18>	and postgis has different release cycles and a huge stack of geo libs as dependencies
T 1568115451 18<Myon18>	it really wouldn't fit into one tarball
T 1568115458 18<enoq18>	ah so the geo libs are the main issue
T 1568115486 18<Myon18>	plus the people working on it don't have to be database experts (and the PostgreSQL developers don't have to be geo experts)
T 1568115541 18<enoq18>	mysql actually ships geography now
T 1568115550 18<ilmari18>	mazula: no. use the type name you used in the CREATE TYPE statement in the CREATE TABLE statement
T 1568115576 18<ilmari18>	mazula: create type my_enum as enum (...); create table my_tabl (my_column my_type);
T 1568115858 18<mazula18>	ilmari ok like that ? https://gist.github.com/minas-tirith/d552d2b2b43e0e41ffac360f4e2d9726
T 1568116250 18<ilmari18>	mazula: what happened when you tried?
T 1568116262 18<ilmari18>	mazula: that's two type names.
T 1568116267 18<ilmari18>	also, don't use varchar(255), use text
T 1568116355 18<ysch18>	??dont
T 1568116356 18<pg_docbot18>	https://wiki.postgresql.org/wiki/Don%27t_Do_This
T 1568116357 18<Myon18>	"types types_values not null"
T 1568116372 18<ysch18>	mazula: Better read the whole link above.
T 1568116420 18<mazula18>	https://gist.github.com/minas-tirith/d552d2b2b43e0e41ffac360f4e2d9726
T 1568116460 18<Myon18>	did you try that?
T 1568116480 18<ilmari18>	mazula: you have two type names on the `types` column
T 1568116481 18<mazula18>	"types types_values not null" ?
T 1568116525 18<[patrik]18>	Myon: I dont need the synchronus replication. I'm fine with the replication that i have, but as you say its important to understand - and I'm still learning things. Thanks for the info.
T 1568116530 18<Myon18>	mazula: that's what line 6 should look like
T 1568116540 18<mazula18>	yes
T 1568116545 18<mazula18>	it's good?
T 1568116551 18<Myon18>	did you try it?
T 1568116610 18<mazula18>	not yet
T 1568148708 20*	Disconnected (20)
T 1568148732 19*	Now talking on 22#postgresql
T 1568148732 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568148732 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Tue Sep 10 01:19:16 2019)
T 1568148732 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568148902 18<ningu18>	this seems to work: https://pastebin.com/sgSujpJt
T 1568148960 18<davidfetter_work18>	cool
T 1568149000 18<ningu18>	davidfetter_work: how would one normally index all the words? I guess it's the same case as indexing all the words in a regular text value?
T 1568149044 18<davidfetter_work18>	yep. I'm pretty rusty on how text search works, but I believe that's the idea.
T 1568149058 18<ningu18>	yeah, I'm rusty too, but I know where to look
T 1568149633 18<velix18>	https://i.imgur.com/RJvc1BX.png
T 1568150213 18<axsuul18>	If I have a compound index XYZ on three columns foo,bar,baz and then I need to query only on bar,baz columns, will it use index XYZ or should I create another compound index on bar,baz?
T 1568150256 18<xocolatl18>	it will likely not be used
T 1568150344 18<axsuul18>	thanks
T 1568150418 18<xocolatl18>	if that's your access pattern, you could just create the index on bar,baz,foo
T 1568150762 18<axsuul18>	xocolatl : ah interesting, so I actual have the following queries:   (1) bar,baz    (2) foo      .. so does that mean i can get away with just bar,baz,foo (unique) and foo   indexes?
T 1568150785 18<xocolatl18>	yes
T 1568150804 18<axsuul18>	nice, thanks!
T 1568150838 18<xocolatl18>	remember, tables don't need to be indexed, queries do.  you have to look at your queries to determine the best indexes
T 1568150972 18<axsuul18>	thanks, will keep that mind, finding it kind of hard to know which queries use what indexes. if I try to use EXPLAIN, it can give a false positive if my dataset isnt large enough =/
T 1568151008 18<xocolatl18>	yeah, you can't optimize a toy dataset
T 1568151017 18<xocolatl18>	I mean, you *can*, but you get toy optimizations
T 1568151524 18<davidfetter_work18>	??mfa
T 1568151524 18<pg_docbot18>	Nothing found
T 1568151526 18<davidfetter_work18>	hrm.
T 1568151528 18<davidfetter_work18>	??2fa
T 1568151528 18<pg_docbot18>	Nothing found
T 1568151547 18<davidfetter_work18>	anybody managed to get an MFA working to auth to pg?
T 1568151583 18*	mlt- still doesn't know what MFA is
T 1568151643 18<xocolatl18>	multi-factor authentication
T 1568151651 18<davidfetter_work18>	sorry, "multi-factor authentication", which usually means a time-based one-time password combined with some other credential
T 1568151674 18<xocolatl18>	or the museum of fine arts in boston, take your pick
T 1568151679 18<nbjoerg18>	davidfetter_work: any specific reason for that mechanism?
T 1568151695 18<nbjoerg18>	davidfetter_work: e.g. compared to rsa-based auth of a yubikey?
T 1568151706 18<nbjoerg18>	which might not work oob, but seems much easier to integrate
T 1568151707 18<mlt-18>	so...like using Google Authenticator app to login to PG?
T 1568151728 18<nbjoerg18>	I think you could do that with pam based though
T 1568151754 18<xocolatl18>	pam can pretty much do everything
T 1568151776 18<xocolatl18>	if you don't run up against authentication_timeout, of course
T 1568151798 18<nbjoerg18>	the primary question for me is what the goal is
T 1568151820 18<xocolatl18>	fun, I'd guess
T 1568151824 18<xocolatl18>	like the twitter fdw
T 1568151828 18*	davidfetter_work work sat a security company
T 1568151832 18<davidfetter_work18>	works at*
T 1568151874 18<nbjoerg18>	making sure that administrators are using 2FA: I think getting libpq to use ccid is much easier
T 1568151883 18<nbjoerg18>	e.g. pkcs#11
T 1568151936 18<davidfetter_work18>	I use the git FDW each week to write up the newsletter :)
T 1568151963 18<davidfetter_work18>	well, I hacked on it a bit because I haven't yet figured out how to improve performance in a cleaner way
T 1568152044 18<mlt-18>	TIL...git fdw
T 1568152085 18<davidfetter_work18>	it's pretty fun
T 1568152122 18<davidfetter_work18>	it's possible that git alone could do what I want, but I like saying things like GROUP BY and agg(... ORDER BY ...)
T 1568152123 18<nbjoerg18>	hm. that would make a certain amount of sense for a use case I have
T 1568152360 18<davidfetter_work18>	I basically filed off some pieces of the git FDW that were slowing me down. I don't care about the diffs between commits for what I'm doing, and it was doing them ssllllllloooooooowwwwwwwwly
T 1568154673 18<berndj18>	xocolatl, do you know if the caveat about constants vs variables as mentioned in https://www.postgresql.org/message-id/20020807143850.GB30003%40gerf.org still applies? (re indexing inet column for use with <<=)
T 1568154753 18<RhodiumToad18>	it only applies partially.
T 1568154778 18<RhodiumToad18>	if the block is a parameter, then it is treated as a constant if the query is getting a custom plan
T 1568154814 18<berndj18>	my concern is that the <<=-using query i'm looking at taming is inside a stored procedure, comparing an inet-valued column to an input to the function
T 1568154827 18<RhodiumToad18>	(the query gets a custom plan unless it's a prepared statement, or statement inside a function, that gets executed more than 4 times and the plan cache comes up with a generic plan that doesn't look more expensive)
T 1568154852 18<RhodiumToad18>	procedure in what language?
T 1568154903 18<berndj18>	almost certainly plpgsql (it looked sql-ish when i was looking at it earlier at the office)
T 1568154930 18<RhodiumToad18>	the difference between language plpgsql and language sql can be critical
T 1568154939 18<RhodiumToad18>	it was a procedure with multiple statements?
T 1568154946 18<berndj18>	yes
T 1568154952 18<RhodiumToad18>	plpgsql then.
T 1568154956 18<berndj18>	hang on a sec i'll get it for sure
T 1568154975 18<berndj18>	yeah, plpgsql
T 1568154975 18<xocolatl18>	sql can have multiple statements...
T 1568154988 18<RhodiumToad18>	that means you are at some risk of getting a generic plan, but if the table is at all large and the ranges selective, it's likely that it'd stick to custom plans
T 1568154997 18<RhodiumToad18>	yes, but usually doesn't
T 1568155022 18<RhodiumToad18>	berndj: what specific evidence do you have for the current form being slow?
T 1568155030 18<berndj18>	200 kilorows in the table containing inet-valued cells
T 1568155111 18<berndj18>	RhodiumToad, a) the procedure takes of the order of a second to figure out what i think should be "fast", b) running the internal query i'm focusing on inside the proc takes of the order of a second to return the couple dozen rows derived from the ~200krow table
T 1568155146 18<RhodiumToad18>	what exactly was the query you ran manually?
T 1568155190 18<berndj18>	any other complications? this bit in focus is inside a CREATE TEMP TABLE ... AS (SELECT DISTINCT ... FROM (SELECT DISTINCT ... <<= ... UNION SELECT ... >>=))
T 1568155213 18<berndj18>	what's the canonical pastebin for this #?
T 1568155221 18<RhodiumToad18>	??paste
T 1568155221 18<pg_docbot18>	https://explain.depesz.com/ :: https://pasteboard.co/
T 1568155221 18<pg_docbot18>	https://www.db-fiddle.com/ :: https://paste.depesz.com/
T 1568155221 18<pg_docbot18>	https://dpaste.de
T 1568155244 18<RhodiumToad18>	dpaste.de best for non-sql, paste.depesz.com has sql formatting, explain.depesz.com is for explains
T 1568155361 18<berndj18>	ok, not such a long query after all. it is: SELECT lai.account FROM iplistip ili JOIN "lnkAccountIPList" lai ON lai.iplist = ili.iplist WHERE inet('104.47.33.58') <<= ili.ip;
T 1568155375 18<RhodiumToad18>	ahh
T 1568155386 18<RhodiumToad18>	so ili.ip is actually a range?
T 1568155387 18<berndj18>	oh, that fixed IP is just some example i grabbed from logs; in the proc it's actually a variable 'addr'
T 1568155436 18<berndj18>	yes, the table is formally ranges, but just eyeballing it the vast majority are /32 ranges
T 1568155484 18<RhodiumToad18>	that doesn't really matter
T 1568155493 18<berndj18>	and the only indexes covering the 'ip' column are "iplistip_pkey" PRIMARY KEY, btree (iplist, ip) and "iplistip_ip_idx" btree (ip) which seems sucky
T 1568155506 18<RhodiumToad18>	the optimization in the message you linked to only applies to column <<= constant, not to constant <<= column
T 1568155520 18<RhodiumToad18>	what pg version are you using?
T 1568155546 18<RhodiumToad18>	recent versions have some spgist support for ip range lookups, otherwise ip4r is what you need
T 1568155553 18<berndj18>	psql reports 9.3.24, with no whine about differing server version
T 1568155560 18<RhodiumToad18>	show server_version;
T 1568155574 18<berndj18>	yeah, it's the same
T 1568155587 18<RhodiumToad18>	ok. pretty sure that's too old for spgist inet support
T 1568155625 18<RhodiumToad18>	ip4r will work though, if you can install it (it's a separate module)
T 1568155677 18<berndj18>	i ran i managed to do CREATE INDEX ON iplistip USING GIST (ip inet_ops); without error on a 9.5.14 box though, but it doesn't have a 100krow iplistip table to make it interesting
T 1568156035 18<RhodiumToad18>	yeah, there's some support for gist but last I looked it was a lot less efficient than ip4r; the spgist support in more recent versions may be better
T 1568156068 18<RhodiumToad18>	9.3 lacks even rudimentary gist support for inet
T 1568156177 18<agohoth18>	Hello
T 1568156177 18<berndj18>	i could just punt until everything gets upgraded including PG to at least 9.5 (which is pending anyway), and keep firefighting until then
T 1568156201 18<blaenk18>	im trying to figure out the syntax for adding a column and setting the default in one go, is this ok: ALTER TABLE  ADD COLUMN x BIGSERIAL DEFAULT nextval('existing_serial'::regclass);
T 1568156212 18<blaenk18>	because the manual doesn't seem to show the DEFAULT clause or I'm misreading it
T 1568156222 18<agohoth18>	I have a developer at work who tuned pg 9.6  and then I used an online config page https://pgtune.leopard.in.ua/#/ and got vastly different results
T 1568156228 18<RhodiumToad18>	do you want the new column to be populated for existing rows?
T 1568156229 18<agohoth18>	mine has smaller values for almsot eveyrhtign
T 1568156241 18<blaenk18>	RhodiumToad: yes preferably, but i could do that separately if necessary
T 1568156243 18<agohoth18>	also she didnt enable any autovac
T 1568156248 18<blaenk18>	for now I'm wondering about going forward for new rows
T 1568156263 18<blaenk18>	(but also if you would be so kind I'm interested in knowing that too!)
T 1568156269 18<xocolatl18>	agohoth: autovacuum is enabled by default
T 1568156271 18<RhodiumToad18>	agohoth: what values for work_mem, effective_cache_size, shared_buffers, maintenance_work_mem?  and how much RAM and how many CPUs do you have?
T 1568156307 18<RhodiumToad18>	blaenk: bigserial implies a newly created sequence. if you want to use an existing sequence, the column type should be bigint
T 1568156332 18<RhodiumToad18>	blaenk: alter table ... add x bigint default nextval('blah');  works
T 1568156352 18<blaenk18>	RhodiumToad: ohh okay, this is for a table partition I'm creating un-attached, the partitioned table has a BIGSERIAL and before I can attach this table I need to have that column too, does that make sense? I'm not sure
T 1568156367 18<RhodiumToad18>	blaenk: if you want to avoid populating existing rows, then you add the column and the default separately.
T 1568156386 18<blaenk18>	so if I make it BIGINT then it won't match the schema for the partitioned table will it? or is BIGSERIAL still just  BIGINT but implies a new sequence
T 1568156390 18<blaenk18>	I vaguely remember reading something like that
T 1568156399 18<RhodiumToad18>	blaenk: bigserial and bigint are the same type
T 1568156406 18<blaenk18>	perfect thank you
T 1568156420 18<agohoth18>	16 g ram aws instance 4 cpu
T 1568156421 18<RhodiumToad18>	blaenk: bigserial is just shorthand for something like  bigint not null default nextval('newly created sequence');
T 1568156423 18<agohoth18>	I think ssd
T 1568156436 18<agohoth18>	I gave that config page 14g ram so 2 for os linux
T 1568156463 18<blaenk18>	RhodiumToad: thank you so much. if a table already had BIGSERIAL, how would I go about like redoing it with the existing serial? do I have to drop the column and re-add it?
T 1568156534 18<RhodiumToad18>	blaenk: no, you'd change the defaults and use alter sequence ... owned by ...
T 1568156546 18<blaenk18>	thank you I will read into that
T 1568156552 18<RhodiumToad18>	agohoth: so what are the values in question?
T 1568156654 18<agohoth18>	https://paste.pics/e2d48cd6e9ab4b1828f929e6f26291ec
T 1568156665 18<sgt_disco18>	If I want to create a unique index where a column is only unique based on equivalent values of another column, how would I go about adding that constraint?
T 1568156754 18<ash_worksi18>	is there a way for me to find out what's causing `RESET ALL` to show up in `pg_running` every so often?
T 1568156754 18<agohoth18>	it seems to optimize for a large effective cache size
T 1568156768 18<agohoth18>	10g or so
T 1568156856 18<RhodiumToad18>	effective_cache_size needs to be large, it doesn't actually allocate anything
T 1568156871 18<agohoth18>	oh autovac is enabled by default ? ok
T 1568156887 18<agohoth18>	now
T 1568156913 18<RhodiumToad18>	agohoth: what are the values from your developer, not from pgtune?
T 1568156925 18<agohoth18>	Damn I closed by work machine a mac
T 1568156927 18<agohoth18>	hold on
T 1568157262 18<localhorse18>	hey
T 1568157324 18<localhorse18>	i have a self-referencing table and i want to `select *, count(children)` (from the same table), how can i do that?
T 1568157336 18<localhorse18>	i tried with a self join but i only get rows that actually have children
T 1568157342 18<RhodiumToad18>	where "children" includes all the descendents?
T 1568157343 18<ningu18>	left join
T 1568157351 18<RhodiumToad18>	or just the immediate level of descent?
T 1568157357 18<localhorse18>	RhodiumToad: only immediate
T 1568157363 18<RhodiumToad18>	left join, then
T 1568157367 18<localhorse18>	i tried that
T 1568157396 18<RhodiumToad18>	select p.id, count(c.id) from mytable p left join mytable c on (c.parent_id=p.id) group by p.id;
T 1568157456 18<localhorse18>	ah, thanks :)
T 1568157565 18<localhorse18>	RhodiumToad: but now it gets tricky: the first occurrence of mytable should be a subselect and the right one is the full table
T 1568157584 18<RhodiumToad18>	why should it be a subselect?
T 1568157585 18<localhorse18>	what's the right syntax to use a subselect there instead?
T 1568157608 18<localhorse18>	RhodiumToad: because i'm filtering the rows first, who i want to return (including their childcount)
T 1568157622 18<localhorse18>	they should be filtered first, not afterwards
T 1568157623 18<RhodiumToad18>	just add a where clause then
T 1568157639 18<RhodiumToad18>	you can't filter by childcount before computing the childcount, obviously
T 1568157654 18<agohoth18>	https://paste.pics/6fa0f7a502c7a8487959a7fed92273eb
T 1568157660 18<localhorse18>	i don't want to filter by childcount tho, but by something else, involving a gist index
T 1568157671 18<agohoth18>	here are the developer configs
T 1568157673 18<ningu18>	localhorse: why can't you put it in the where clause?
T 1568157696 18<localhorse18>	i'll try
T 1568157715 18<localhorse18>	the subselect is a postgis query
T 1568157720 18<RhodiumToad18>	agohoth: I would say that your developer's values are mostly too high
T 1568157778 18<agohoth18>	on an amazon linux x5large 16 g ram 4 cpu  would you dedicate 14g of ram to postgresql? leaving 2 for the linux os?
T 1568157790 18<RhodiumToad18>	agohoth: however, pgtune's suggested work_mem is probably too small. I would have started with 224MB
T 1568157802 18<agohoth18>	I did a presentation about this config and the config page and the pgtunerl.pl  no one was impressed
T 1568157815 18<blaenk18>	RhodiumToad: so I'd ALTER TABLE  ALTER COLUMN  SET DEFAULT next('existing_sequence'), and then ALTER SEQUENCE  OWNED BY  for what? I'm not sure I understand the documentation for this situation
T 1568157823 18<RhodiumToad18>	agohoth: you can't really dedicate RAM to pg. pg wants a lot of the memory available to be available to the OS for disk caching
T 1568157827 18<agohoth18>	This is used in data analytics....I picked mixed type apps in that config page...
T 1568157833 18<agohoth18>	oh
T 1568157840 18<blaenk18>	to remind you, the situation is to change a column with existing rows to use a different sequence, so they should get new numbers rather than the ones it already had
T 1568157892 18<RhodiumToad18>	agohoth: the problem with the 4GB work_mem specified in the developer's configs is that it would take only one moderately complex query to drive the machine into swap
T 1568157933 18<localhorse18>	RhodiumToad, ningu: the subselect starts like this `SELECT p.*, ST_Distance(p.loc, r.loc)::float4 dist FROM points p, (select ST_SetSRID(ST_MakePoint($1, $2), 4326)::geography) as r(loc)` i don't know how to add the `count(c.id)` to this select because it already contains a subselect
T 1568157965 18<ningu18>	is that parent or child?
T 1568157973 18<localhorse18>	i mean i don't know where to add the `join`
T 1568157982 18<RhodiumToad18>	localhorse: you could always do  select ... from (select ...) as p left join ...
T 1568157997 18<blaenk18>	ok I think I would do ALTER SEQUENCE existing_sequence OWNED BY tbl.thecolumn
T 1568158048 18<agohoth18>	hmmm
T 1568158069 18<agohoth18>	she also has some values for variables the pgtune doesn't seem to care about
T 1568158079 18<RhodiumToad18>	pgtune's suggestions are far from ideal
T 1568158088 18<agohoth18>	oh really?  oh wow
T 1568158101 18<blaenk18>	maybe I was confusing in my question, basically I want another table to re-use the same sequence, but rows have already been written with the values from a different sequence, how can I wipe those and force them to take on values of the existing sequence?
T 1568158104 18<agohoth18>	How do I learn the proper configuration?
T 1568158108 18<blaenk18>	should i just drop the column and re-add it
T 1568158115 18<blaenk18>	seems straightforward that way
T 1568158123 18<RhodiumToad18>	1000 is the default for max_files_per_process, fwiw
T 1568158138 18<peerce18>	blaenk; 'values from an existing sequence', is there some relation between the rows in table 1 and table 2 ?
T 1568158164 18<blaenk18>	so it's a partitioned table that has the sequence, and i want child/partitions to use that same sequence, so that they all take on distinct values
T 1568158167 18<RhodiumToad18>	temp_buffers of 8GB is somewhat suspect - if you have more than one connection working with a large temp table that could well push you into swap again
T 1568158174 18<blaenk18>	otherwise if each partition has its own sequence then i'll probably have duplicate values
T 1568158197 18<RhodiumToad18>	however, if you know that only one connection at a time, at most, is making heavy use of temp tables then it might be justified
T 1568158204 18<peerce18>	if you created the partitions properly, they should all be using the same seuqnece from the beginning
T 1568158208 18<peerce18>	before you populated them
T 1568158253 18<RhodiumToad18>	agohoth: the most important settings are effective_cache_size (where the actual value is quite non-critical, just make it something like 75% of total RAM),
T 1568158267 18<RhodiumToad18>	shared_buffers, work_mem, and maintenance_work_mem
T 1568158276 18<blaenk18>	so what I'm doing is for a partition I create it separately to match the schema and then I ATTACH it, but I was using BIGSERIAL for this column which was creating its own sequence, what should I have used instead?
T 1568158300 18<RhodiumToad18>	for shared_buffers, opinions vary quite widely on the best value
T 1568158321 18<peerce18>	blaenk; https://www.postgresql.org/docs/current/ddl-partitioning.html#DDL-PARTITIONING-DECLARATIVE
T 1568158332 18<RhodiumToad18>	for work_mem and maintenance_work_mem, start with something like  work_mem=RAM/(16*cpus) and maintenance_work_mem=RAM/12
T 1568158346 18<localhorse18>	RhodiumToad: ningu: i get invalid reference to FROM-clause entry for table "p"
T 1568158362 18<RhodiumToad18>	localhorse: for what query exactly
T 1568158363 18<agohoth18>	hmm
T 1568158378 18<blaenk18>	peerce: so I'll omit the sequence column and it'll automatically be added when I ATTACH?
T 1568158384 18<peerce18>	blaenk; when you create the partitions via CREATE TABLE parent_xxx PARTIION OF parent
T 1568158392 18<peerce18>	it should inherit the same sequence.
T 1568158406 18<RhodiumToad18>	agohoth: for max_wal_size, a lot depends on the write load on the db and your intended recovery time after a crash - 2GB is not unreasonable
T 1568158442 18<blaenk18>	peerce: the thing is I'm working with a table that was imported by spark, and then just transforming that. I guess I can create a new table and copying into it but it just seemed unnecessary
T 1568158450 18<peerce18>	whats 'spark' ?
T 1568158452 18<RhodiumToad18>	agohoth: small random_page_cost (i.e. slightly more than 1) is reasonable for SSD storage, increasing cpu_tuple_cost to 0.1 or so may also be worthwhile
T 1568158476 18<localhorse18>	RhodiumToad: this query https://www.irccloud.com/pastebin/xq7al4UG/
T 1568158500 18<RhodiumToad18>	agohoth: large effective_io_concurrency can help reduce latency on cloud-type systems when you do a lot of bitmap indexscans
T 1568158510 18<blaenk18>	peerce: basically it got imported by a different process, the table already exists in the db by the point at which I'm trying to make it match the partitioned table schema so that I can attach it
T 1568158518 18<blaenk18>	http://spark.apache.org/
T 1568158539 18<RhodiumToad18>	localhorse: you didn't put all of the subquery inside where I told you to put it
T 1568158546 18<blaenk18>	I was kindly told how to make the table use the existing sequence, that's fine going forward
T 1568158550 18<peerce18>	if you're using manual partitioning, the child tables should have been created with INHERITS
T 1568158567 18<blaenk18>	manual partitioning? I'm using declarative partitioning
T 1568158567 18<RhodiumToad18>	localhorse: select p.id, count(c.id) from (/* put your entire original query here */) p left join ...
T 1568158583 18<peerce18>	then when you created it, specifying PARTITION OF ...   should have taken care of this.
T 1568158586 18<blaenk18>	and again I can't have created the tables, it just already exist out of my control, unless I create a new table and copy everything
T 1568158586 18<localhorse18>	ah
T 1568158589 18<agohoth18>	work meme 219M
T 1568158593 18<agohoth18>	hmm
T 1568158604 18<blaenk18>	the problem isn't how to solve it going forward, RhodiumToad helped me with that
T 1568158607 18<agohoth18>	tuner says only 9 wow big disparity
T 1568158624 18<agohoth18>	9.6 postgresql by the way
T 1568158627 18<blaenk18>	what I'm wondering is in general, how to 'redo' rows that have already used one sequence, to now use another. im thinking i should just drop the column and recreate it
T 1568158640 18<peerce18>	that won't update all the rows
T 1568158655 18<agohoth18>	want to see the pgtune for data ware house scenario?
T 1568158657 18<blaenk18>	oh really? if the column literally no longer exists?
T 1568158660 18<peerce18>	and if you do force a update of all the rows that will double the size of the table via churn
T 1568158667 18<agohoth18>	is 14 of 16 g ram ona aws vm ok for dedicated pg box linux?
T 1568158677 18<peerce18>	since 'update' is implemented as insert,delete
T 1568158692 18<blaenk18>	what 'update' are you referring to, UPDATE?
T 1568158705 18<peerce18>	anything that modifies a value in an existing row, but yes, UPDATE
T 1568158720 18<blaenk18>	I'm saying I'll ALTER TABLE  DROP COLUMN , and then I'll ADD COLUMN again but with the new DEFAULT sequence
T 1568158720 18<localhorse18>	RhodiumToad: the original query excluding ORDER BY and LIMIT, right?
T 1568158723 18<peerce18>	UPDATE sometable SET id=nextval('sequencename');
T 1568158731 18<peerce18>	that will do the same thing.
T 1568158731 18<agohoth18>	it chaged the max wal to 8g in data warehouse config vs the mixed app config
T 1568158733 18<localhorse18>	those go at the end of the whole query
T 1568158735 18<blaenk18>	ah
T 1568158755 18<blaenk18>	but it doesn't seem like I have an alternative, I'm in this situation now
T 1568158759 18<RhodiumToad18>	localhorse: right
T 1568158760 18<peerce18>	the UPDATE is actually a little more efficient, as deleted columns are never actually removed, they just get ignored in future transactions
T 1568158785 18<peerce18>	and the ADD COLUMN will always add the new column to the end of the tuples
T 1568158793 18<RhodiumToad18>	agohoth: the max wal size really only affects how frequent checkpoints are when you're writing large volumes of data
T 1568158814 18<RhodiumToad18>	agohoth: for things like bulk loading you want checkpoints to be reasonably infrequent
T 1568158831 18<blaenk18>	peerce: so given that I'm _already_ in this situation, what is the least bad approach?
T 1568158855 18<blaenk18>	I need to migrate tables to use a different sequence and redo their sequence values
T 1568158870 18<RhodiumToad18>	agohoth: for mixed or OLTP workloads you usually want them a bit more frequent to avoid long recovery times
T 1568158911 18<peerce18>	alter table alter column, change the default value to the new sequence, then UPDATE table SET column=nextval('seqname');
T 1568158912 18<agohoth18>	hmmmm
T 1568158926 18<blaenk18>	thanks peerce
T 1568158927 18<peerce18>	so nothing else in your database even references these sequences ?
T 1568158931 18<agohoth18>	Where did you learn howto tune?  the wiki? or testing? or?
T 1568158932 18<peerce18>	this data doesn't have a natural key ?
T 1568158943 18<blaenk18>	these sequences aren't keys
T 1568158948 18<peerce18>	huh?
T 1568158952 18<blaenk18>	what I said?
T 1568158957 18<blaenk18>	there are natural keys
T 1568158961 18<RhodiumToad18>	experience and knowing how the code works (also helping people here)
T 1568158963 18<peerce18>	normal use of sequence is to be a primary key
T 1568158969 18<blaenk18>	then this is not a normal use
T 1568158970 18<peerce18>	a surrogate key
T 1568158982 18<peerce18>	then whats the point of even having a random sequence unrelated to the data ?
T 1568158986 18<blaenk18>	it's for spark to read 'partitions' of the db
T 1568158993 18<blaenk18>	ranges of rows
T 1568158993 18<localhorse18>	RhodiumToad: now i get: error: column "p2.loc" must appear in the GROUP BY clause or be used in an aggregate function https://www.irccloud.com/pastebin/Be57P02T/
T 1568158999 18<blaenk18>	in parallel
T 1568159043 18<peerce18>	so you're partitioning by this sequence yet its not the PK nor is it even assigned as the partitioning value yet ?   huh.
T 1568159047 18<RhodiumToad18>	localhorse: you can't select p2.* like that
T 1568159056 18<localhorse18>	why not?
T 1568159073 18<localhorse18>	i also want to select `dist` in the overall query btw
T 1568159084 18<agohoth18>	I wonder about the number of connections too....
T 1568159113 18<localhorse18>	RhodiumToad: do i need to spell out all columns verbatim?
T 1568159117 18<RhodiumToad18>	localhorse: just group by p2.id, p2.dist  then (and select only those two plus the count)
T 1568159141 18<agohoth18>	I notice at the analytics group I work in now, they use a huge mishmash fo differnt java and dataabse tools
T 1568159144 18<localhorse18>	but i want all columns of p, dist and child count
T 1568159161 18<peerce18>	localhorse; but aren't there more than one column with the same id and dist ?
T 1568159164 18<agohoth18>	Are there shops that do data analytics with only postgresql?
T 1568159168 18<peerce18>	er, more than one row, I mean...
T 1568159179 18<localhorse18>	peerce: hence the group by p2.id
T 1568159211 18<agohoth18>	as a former linux admin I hate to see data in so many different places
T 1568159213 18<RhodiumToad18>	localhorse: is points.id the pkey of points?
T 1568159215 18<localhorse18>	so i need to group by <all columns of p2> ?
T 1568159218 18<localhorse18>	yes
T 1568159221 18<agohoth18>	I always suspect the devs are hiding bad performacne
T 1568159240 18<localhorse18>	can i do it with `distinct`?
T 1568159254 18<RhodiumToad18>	localhorse: no
T 1568159266 18<RhodiumToad18>	localhorse: it might be better to rearrange the query like this:
T 1568159323 18<RhodiumToad18>	select p2.*, c.count from (...) p2 left join (select q.parent_id, count(*) from points q group by q.parent_id) c on (p2.id=c.parent_id);
T 1568159337 18<RhodiumToad18>	note no group by at the outer level, though you can add ORDER BY
T 1568159358 18<RhodiumToad18>	er, use nullif(c.count,0) rather than just c.count there
T 1568159396 18<localhorse18>	RhodiumToad: but then it's potentially fetching a huge number of rows for the inner query, no?
T 1568159405 18<localhorse18>	it's joining the whole table with itself
T 1568159411 18<localhorse18>	not just my subselect
T 1568159555 18<theseb18>	Why does this given an error?... with foo (a, b, c) as (select 4, 5, 6)
T 1568159555 18<theseb18>	     select * from foo;
T 1568159585 18<theseb18>	i'm learning CTEs
T 1568159651 18<RhodiumToad18>	what error?
T 1568159688 18<RhodiumToad18>	localhorse: if you find it slower that way, then try this instead:
T 1568159707 18<theseb18>	RhodiumToad: it says where the "a" is....it wasn't expeting that there
T 1568159720 18<RhodiumToad18>	localhorse:  select p2.*, c.count from (...) p2 left join lateral (select count(*) from points q where q.parent_id=p2.id) c on true;
T 1568159728 18<localhorse18>	RhodiumToad: i'm getting: Unexpected null for non-null column https://www.irccloud.com/pastebin/dZdNovsK/
T 1568159731 18<Xgc18>	theseb: Make sure you're using a database which support this.  You've been asking general questions in every DB channel and #sql.
T 1568159765 18<RhodiumToad18>	localhorse: getting that from where?
T 1568159770 18<ningu18>	theseb: with foo as select (4, 5, 6)
T 1568159773 18<localhorse18>	from this query
T 1568159804 18<RhodiumToad18>	theseb: what's the exact error message
T 1568159816 18<RhodiumToad18>	localhorse: from what piece of software
T 1568159832 18<localhorse18>	postgres
T 1568159865 18<theseb18>	RhodiumToad: fixed thanks to ningu!.. with foo as select (4, 5, 6)
T 1568159866 18<theseb18>	works
T 1568159870 18<localhorse18>	or diesel rather
T 1568159884 18<localhorse18>	the rust lib to talk to postgres
T 1568159899 18<RhodiumToad18>	there's no such error in the postgresql source code
T 1568159924 18<RhodiumToad18>	with foo(a,b,c) as (select 4,5,6) select * from foo;   is valid syntax
T 1568159941 18<Xgc18>	theseb: The SQL you just showed is not valid.
T 1568159941 18<ningu18>	theseb: what database are you using?
T 1568159959 18<RhodiumToad18>	particularly useful in the form  with foo(a,b,c) as (values (1),(2),(3)) select * from foo;
T 1568159964 18<localhorse18>	RhodiumToad: then it's from diesel. it tries to deserialize the result of the queries into strongly typed structs
T 1568159974 18<theseb18>	ningu: Spark SQL
T 1568159977 18<localhorse18>	and it got a null for a non-null column
T 1568159981 18<localhorse18>	from this query
T 1568159992 18<theseb18>	Xgc: really?
T 1568159993 18<theseb18>	hmm
T 1568160015 18<ningu18>	theseb: well you can't have "with ..." without select after it
T 1568160016 18<RhodiumToad18>	localhorse: coalesce(c.count, 0)  <-- should have been that, not nullif
T 1568160021 18<Xgc18>	theseb: Yes, not in any database I know and not valid standard SQL either.
T 1568160025 18<localhorse18>	ah!
T 1568160032 18<RhodiumToad18>	localhorse: but if it's assuming that a nullif() result is not null, then it's broken
T 1568160057 18<localhorse18>	no, it's not assuming. i told it that the child_count is a non-nullable type
T 1568160059 18<Xgc18>	theseb: You must be using some odd database if that was the exact SQL you executed with no errors.
T 1568160070 18<RhodiumToad18>	Xgc: maybe you missed the second part of their original query?
T 1568160079 18<theseb18>	RhodiumToad: here is the command and error you wanted... https://pastebin.com/mu1MCAxb
T 1568160118 18<RhodiumToad18>	theseb: well that's clearly not postgres so it's not our problem, is it?
T 1568160145 18<RhodiumToad18>	theseb: I cut+paste your query into postgres and it runs fine:
T 1568160165 18<RhodiumToad18>	https://dpaste.de/mZoe
T 1568160176 18<Xgc18>	Right. He using something strange.
T 1568160189 18<Xgc18>	RhodiumToad's SQL is obviously fine.
T 1568160248 18<localhorse18>	RhodiumToad: thanks, with coalesce, it works :)
T 1568160266 18<localhorse18>	RhodiumToad: but am i right that it's slow because it joins the whole table with itself before filtering it down to my subset?
T 1568160285 18<RhodiumToad18>	localhorse: the lateral one or the non-lateral one?
T 1568160295 18<localhorse18>	the non-lateral one
T 1568160312 18<RhodiumToad18>	localhorse: whether it's actually slow will depend a lot on your data sizes, for which explain analyze is your friend
T 1568160316 18<theseb18>	RhodiumToad, Xgc: wait...SQL is supposed to be standard and the same everywhere innit?
T 1568160322 18<ningu18>	theseb: no
T 1568160328 18<RhodiumToad18>	theseb: HAHAHAHAHA
T 1568160336 18<ningu18>	no two implementations are identical
T 1568160357 18*	Xgc smiles
T 1568160363 18<theseb18>	good lord it was made in the 1970s...they haven't got their act together? didn't ANSI standard fix all this?
T 1568160366 18<RhodiumToad18>	theseb: there is a standard, but (a) something like 80% of the standard is "optional features", and (b) virtually nobody implements even the standard part the same way
T 1568160400 18<RhodiumToad18>	theseb: in particular WITH is one of those optional features
T 1568160439 18<theseb18>	RhodiumToad: wait...with other langs like C and Python etc......you need to follow very strict guidelines to be called a compiler for lang $X
T 1568160451 18<theseb18>	i want to complain to the management
T 1568160477 18<ningu18>	the history and ecosystem of databases is kind of different
T 1568160491 18<ningu18>	there are a small number of implementations, each with a long history and large number of users
T 1568160499 18<RhodiumToad18>	https://wiki.postgresql.org/wiki/PostgreSQL_vs_SQL_Standard  <-- there's an incomplete list of the ways that postgresql explicitly violates even the parts of the spec that it tries to implement in the standard way
T 1568160512 18<Xgc18>	theseb: C is another with implementations that vary widely, and almost none support the entire standard, usually not for at least 10 or 20 years after the standard is released.
T 1568160513 18<RhodiumToad18>	??comparison
T 1568160514 18<pg_docbot18>	http://troels.arvin.dk/db/rdbms/ :: http://en.wikipedia.org/wiki/Comparison_of_SQL_database_management_systems
T 1568160514 18<pg_docbot18>	http://www.wikivs.com/wiki/MySQL_vs_PostgreSQL :: http://www.intitec.com/varios/FabalabsResearchPaper-OSDBMS-Eval.pdf
T 1568160514 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/functions-comparisons.html#ROW-WISE-COMPARISON :: https://www.postgresql.org/docs/current/static/functions-comparison.html
T 1568160514 18<pg_docbot18>	http://www.sql-workbench.eu/dbms_comparison.html
T 1568160559 18<theseb18>	ok
T 1568160565 18<theseb18>	yikes
T 1568160567 18<localhorse18>	RhodiumToad: but the lateral one should be faster?
T 1568160572 18<localhorse18>	in theory?
T 1568160594 18<RhodiumToad18>	localhorse: assuming you have reasonable indexes, yes
T 1568160636 18<localhorse18>	i have these two indexes: create index on points (updated_at, id); create index on points using gist (loc);
T 1568160643 18<localhorse18>	are they reasonable enough?
T 1568160651 18<localhorse18>	or how to improve them for this query specifically?
T 1568160656 18<localhorse18>	for the lateral one
T 1568160719 18<RhodiumToad18>	you'll need an index on points(parent_id)
T 1568160742 18<localhorse18>	a separate one, or added to the first?
T 1568160750 18<RhodiumToad18>	separate
T 1568160765 18<localhorse18>	ok
T 1568160816 18<agohoth18>	ll
T 1568160849 18<ningu18>	theseb: databases have to perform in very specific ways, often to high performance requirements, and different implementations have added extensions of various sorts over the years
T 1568160871 18<ningu18>	but basically, choosing your database is an important choice
T 1568161236 18<theseb18>	ningu: ok...thanks
T 1568161466 18<localhorse18>	RhodiumToad: so you said `select p2.*, c.count from (...) p2 left join lateral (select count(*) from points q where q.parent_id=p2.id) c on true`, why `on true`?
T 1568161492 18<ningu18>	localhorse: all joins need an "on" condition specified
T 1568161504 18<ningu18>	even if effectively there is nothing to put there
T 1568161513 18<localhorse18>	then why are we even joining?
T 1568161539 18<ningu18>	lateral joins work differently from other joins (and are also different syntactically)
T 1568161545 18<localhorse18>	ok
T 1568161571 18<ningu18>	in this case, the count needs to be in the subquery and needs to be done once per joined row
T 1568161629 18<ningu18>	I don't think the count would come out right without lateral... you can try it
T 1568161640 18<localhorse18>	RhodiumToad: but is this really faster than the non-lateral version then?
T 1568165194 18<jhammerman18>	Hi Postgres IRC, does anyone have experience with HikariCP rapid recovery in a HA failover scenario? We have built both HA-AP and HA-CP architectures, and Im am trying to find specifc information regarding in-flight transactions (so that I can help our users safely retry the correct class of errors, with backoff, etc.)
T 1568165339 18<peerce18>	never even heard of hikari, but in general, if there's a failover, any pending transactions that ahven't been committetd have to be restarted from the beginning.
T 1568165567 18<dive-o18>	jhammerman: all I know about HikariCP as far as that goes is that it really, really doesn't like it when the database goes away.
T 1568165584 18<dive-o18>	I'm no java guy though so I can't really give any useful details other than that though
T 1568165666 18<jhammerman18>	OK, thanks yall
T 1568166170 18<Slade18>	i wonder if there is still much of an advantage of using int/enums instead of just storing the string  (gender  1/Male 2/Female etc)  (1/ok 2/canceled 3/noaccess)
T 1568166897 18<peerce18>	well, just storing the strings won't ensure you don't have random values there
T 1568166914 18<peerce18>	and string comparisions are much more work than integer comparisions
T 1568166929 18<peerce18>	plus the tuples are larger, as are the indicies
T 1568166942 18<peerce18>	when you multiply thtat by millions of rows it becomes quite significant
T 1568193167 20*	Disconnected (20)
T 1568193192 19*	Now talking on 22#postgresql
T 1568193192 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568193192 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Tue Sep 10 01:19:16 2019)
T 1568193192 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568194432 18<adsf18>	can windowing functions show you % over or % under the average?
T 1568194506 18<Myon18>	you can divide the result by something
T 1568194518 18<adsf18>	good shout
T 1568194520 18<adsf18>	thnx
T 1568195214 18<Berge18>	enoq: It doesn't. Generally, use the postgres docs, they're very good and accurate. Most things on the Internet aren't.
T 1568195227 18<enoq18>	Berge I see, thank you
T 1568195234 18<enoq18>	I had trouble finding it in the postgres docs
T 1568195852 18<Moonsilence18>	Hi! I am looking for a boolean expression to check wether a text value can be cast to type UUID. I need something like 'mytextval'::text -> false (no valid uuid) and '50dc6ce5-76db-7be8-59ba-b25553b8631e'::text -> true (valid uuid)
T 1568195876 18<Myon18>	~ 'regexp'
T 1568195900 18<Moonsilence18>	is there a regexp to match uuid v4 format?
T 1568195946 18<Moonsilence18>	ah yes, it's ugly but i found one ;-) thanks Myon
T 1568196021 18<Myon18>	alternatively, write a plpgsql function that tries the cast and catches the exception
T 1568197480 18<snatcher18>	\copy (select cn from tn where cnn=v) to '/tmp/fn'; what's the way to copy with evaluated escapes (newlines etc) in this case?
T 1568197586 18<ilmari18>	snatcher: what do you mean "evaluated escapes"? you can use CSV format, which quotes values containing newlines (and commas)
T 1568197701 18<snatcher18>	ilmari: output file contains  \\u0\n etc instead unicode symbols or newlines for example
T 1568197722 18<snatcher18>	i mean "\n" as  string
T 1568197762 18<Myon18>	there's also COPY BINARY
T 1568197790 18<Myon18>	or maybe you could just \o /tmp/fn, and run a plain SELECT
T 1568197795 18<Myon18>	(\a \t)
T 1568200556 18<snatcher18>	still contains PGCOPY header
T 1568200656 18<snatcher18>	maybe i misunderstand something, what's the correct way to write string "as is" to file?
T 1568200687 18<colo-work18>	snatcher, using what? a UNIX shell?     printf 'as is' > file
T 1568200697 18<colo-work18>	(that does NOT include a trailing newline)
T 1568200707 18<snatcher18>	colo-work: using pgsql
T 1568200750 18<colo-work18>	ah. what's your actual objective?
T 1568200790 18<snatcher18>	colo-work: parse that string on other host as file
T 1568200826 18<snatcher18>	\copy (select cn from tn where cnn=v) to '/tmp/fn' binary; includes PGCOPY header
T 1568200945 18<snatcher18>	>Binary COPY OUT files are only intended for consumption by COPY IN commands. There is no way to prevent Postgres from writing the file/row/field headers.
T 1568200961 18<snatcher18>	so there is no way
T 1568201990 18<doev18>	I have child tables, that inherits from a master table. In the master table, I need a field "label", that depends on the child table. What is best practise to do that? If I use a view, the code will be complex. what else is possible?
T 1568202192 18<doev18>	If there is no better solution, I will use a label-field in the master table, and control it by the insert/update trigger.
T 1568202565 18<moldy18>	doev: what does "depends on the child table" mean?
T 1568202581 18<moldy18>	doev: and why does the field not live on the child table?
T 1568202713 18<nickb18>	plpgsql function definitions are tied to the snapshot for volatile functions in read committed mode, right?
T 1568202725 18<doev18>	moldy, i.e. the label should be Child1.a, Child2.name or Child3.whatever ....
T 1568202781 18<moldy18>	doev: hmm, i don't see why the label field is not on the child tables then
T 1568202817 18<doev18>	moldy, cause I want to query the master table.
T 1568203160 18<[patrik]18>	doev: how many child tables do you have? is it a fixed number?
T 1568203232 18<[patrik]18>	doev: im thinking you can slap the label field in the child tables and create a view and query that when you need "label" rather than query your master table. So create a view with all the label fields joined together from the child tables.
T 1568203327 18<Bish18>	if i want to order a dataset by data i only have in memory how would i go with it?
T 1568203335 18<Bish18>	put it into an array, and check the index of the thingie?
T 1568203352 18<Bish18>	say i have [1,2,3] in my dateset and the order i have is [2,3,1]
T 1568203530 18<[patrik]18>	bish maybe you can select unnest(your_array) order by 1;
T 1568203554 18<[patrik]18>	and then put it back into an array data type again.
T 1568203569 18<Bish18>	nono, maybe i expressed myself to stupid
T 1568203586 18<Bish18>	i want to order an existing table, by an order i have in ram
T 1568203606 18<Bish18>	say i have "user" [1,2,4,6,9] and the order i have in memory is [4,6,9]
T 1568203616 18<Bish18>	then i want order "user" by their index of that memory array
T 1568203625 18<Bish18>	if that index cannot be found then place it at the end
T 1568203670 18<jtech18>	Hi Had executed the command "DROP ROLE IF EXISTS my_user;" but the below message shows up:ERROR: role "my_user" cannot be dropped because some objects depend on it DETAIL: owner of default privileges on new relations belonging to role my_user in schema public SQL state: 2BP01
T 1568203682 18<jtech18>	Then I have executed the below command:REASSIGN OWNED BY my_user TO postgres;
T 1568203695 18<jtech18>	So, the below commands it works but I am concerned about if there is any chance to drop any object that wasn't reassigned to postgres user before. Can it happen or I am getting crazy here?DROP OWNED BY my_user;DROP ROLE IF EXISTS my_user;
T 1568204086 18<ilmari18>	jtech: see the second-last note in https://www.postgresql.org/docs/current/sql-reassign-owned.html
T 1568204329 18<jtech18>	ilmari the issue is cannot be dropped using only REASSIGN OWNED, it is allowed to drop only if applied DROP OWNED BY - third-last note
T 1568204373 18<ilmari18>	The REASSIGN OWNED command does not affect any privileges granted to the old_roles on objects that are not owned by them. Likewise, it does not affect default privileges created with ALTER DEFAULT PRIVILEGES. Use DROP OWNED to revoke such privileges.
T 1568204410 18<ilmari18>	so yes, you ned to do REASSIGN OWNED, then DROP OWNED (in all datbases the user owns objects or has privileges) before dropping the role
T 1568204539 18<jtech18>	oh ok.. so, is there any change to miss an object for the new reassigned role after executed DROP OWNED BY previous role?
T 1568204608 18<jtech18>	because I just want drop the roles not any objects that should be reassigned to a new role at his point
T 1568204673 18<ariejan18>	Hi. I have a simple table that periodically receives a reported status. Basically it's a list of what the status was a different points in time. I want to write a query to find when the last status change happened. Here's a small example: https://gist.github.com/ariejan/70e7a5ef426ad5844ff6da3d4ae39f94
T 1568204730 18<ilmari18>	"reassign owned" will reassign ownership that can be reassigned. the subsequent drop owned will drop things that are associated with the role but aren't really ownership per se (granted and default privileges)
T 1568204744 18<doev18>	[patrik] the number of child tables is not fixed.
T 1568204757 18<jtech18>	ilmari just read again your comment and realized that "DROP OWNED" revoke privileges and don't drop objects. thanks
T 1568204822 18<jtech18>	now I am confused haha
T 1568204881 18<[patrik]18>	doev: that sucks more than a little :)
T 1568204943 18<doev18>	well, I think there is no perfect solution. I think a label filed in master, managed by insert/update triggers in the child table, can be what I need.
T 1568204957 18<ilmari18>	jtech: if someone creates an object owned by the role between REASSIGN OWNED and DROP OWNED, that will get dropped
T 1568205018 18<doev18>	Can I forbid direct insert/delete in a master table? But not with usage of privileges.
T 1568205038 18<durango18>	Im currently experiencing this issue: database is not accepting commands to avoid wraparound data loss in database <-- is there anyway to resolve this WITHOUT restarting the server and halting the entire database?
T 1568205083 18<[patrik]18>	bish: that doesnt make sense to me. the table (i think a db table) is not in any guaranteed order, and you build some wanted sort-order in memory.
T 1568205109 18<Bish18>	with "in memory" i mean the memory of the client
T 1568205184 18<[patrik]18>	ok so the client software builds a list of "users" and their wanted sort order, and then what is supposed to happen?
T 1568205208 18<jtech18>	ilmari um.. so if I run again REASSIGN OWNED and DROP OWNED the recent objects should not be dropped as they were just reassigned, is that correct?
T 1568205209 18<[patrik]18>	the database is going to reorder stuff based on this client wishlist of order?
T 1568205237 18<Bish18>	yis!
T 1568205252 18<Bish18>	so i want something like
T 1568205535 18<[patrik]18>	ok i would do this probably with a temporary table or CTE of some sort. Make a plpgsql function to take that array of "users" (user_ids).
T 1568205593 18<[patrik]18>	so we have 5 users in the array (5,10,15,20,25) and we make the index in the array the sort order, so if you want different sort order, you have the client reorganize the array-order.
T 1568205680 18<[patrik]18>	so we say the index of the array is the sort order. Create a temporary table and select user_id from users where user_id in (.....); and insert that into the temproary table along with the array-index of each id.
T 1568205720 18<[patrik]18>	so you will have a table like this temp_table(user_id,sort_order) and have the function populate it based on the client wishlist.
T 1568205740 18<lluad18>	... order by foo <> 5, foo <> 10, foo <> 15, foo <> 20, foo <> 25
T 1568205768 18<[patrik]18>	so it would be 5,1 10,2 15,3 20,4 and 25,5 in the example. and then return it as select * from temp_table order by sort_order.
T 1568205803 18<Myon18>	order by foo not in (5, 10, 15, 20, 25), foo
T 1568205852 18<lluad18>	That's not the same thing (and won't work, I don't think).
T 1568205932 18<lluad18>	(Though I'm vague on quite what not in () will turn in to when you pry the cover off)
T 1568206162 18<RhodiumToad18>	[patrik]: no point using a temp table
T 1568206168 18<Bish18>	that's what i thought
T 1568206195 18<RhodiumToad18>	select * from unnest(array[5,10,15,20]) with ordinality as a(id,ord) join ... order by a.ord;
T 1568206199 18<[patrik]18>	RhodiumToad: fair enough, there is always a more efficient way to do it in postgres :)
T 1568206237 18<RhodiumToad18>	pg will even avoid the sort when sorting by ordinality if the rest of the plan doesn't mess up the sort order
T 1568206288 18<Bish18>	RhodiumToad: but i want order an existing table according to that array i give it in
T 1568206356 18<RhodiumToad18>	yes, that's what the query does
T 1568206401 18<Bish18>	order by a.id you mean?
T 1568206419 18<Bish18>	after the join
T 1568206425 18<Bish18>	im confused
T 1568206468 18<Bish18>	i was expecting something like order by index of user_id in {1,2,3,4,5}::[Int]
T 1568206481 18<Bish18>	integer[] rather i guess
T 1568206525 18<RhodiumToad18>	that's generally less efficient
T 1568206533 18<Bish18>	 guess so
T 1568206558 18<RhodiumToad18>	select * from unnest(array[5,10,15,20]) with ordinality as a(id,ord) join users u on u.user_id=a.id  order by a.ord;
T 1568206601 18<ariejan18>	Is there a way to group sequential values, but across the entire table? E.g. a, a, b, b, a would result in a, b, a (not a, b)?
T 1568206646 18<RhodiumToad18>	you can do stuff with lag() to detect consecutive values according to some ordering of the rows
T 1568207029 18<aborigen102018>	d
T 1568207130 18<aborigen102018>	hi all. How to assemble odbc-driver from source code? command ./configure is says "error: C preprocessor "/lib/cpp" fails sanity check"
T 1568207202 18<lseactuary18>	i have say 10 tables in postgresql. i want to do something so user A has permissions to read 2 of them and not the others, B has other permissions etc. permissions can change. I am wondering what the setup should be to enable this? creating a role per user?
T 1568207228 18<Myon18>	aborigen1020: is /lib/cpp a working C preprocessor?
T 1568207234 18<ariejan18>	RhodiumToad I can use lag() to add the previous state to the current one. Maybe I can use that as a sub-query to select rows where the current state is not equal to the previous state.
T 1568207313 18<[patrik]18>	lseactuary: create a role per "application use case" instead. Like APP_USER, APP_SUPERUSER etc.
T 1568207340 18<[patrik]18>	and grant your select, update, insert privs to the different tables to these application usecase roles.
T 1568207370 18<lseactuary18>	that is what i was thinking, but with 10 tables, we are looking at many combinations. that is why i was thinking a role per user would be better?
T 1568207387 18<aborigen102018>	Myon, not certain
T 1568207420 18<lseactuary18>	[patrik] e.g. 1 user could have table1, table2; user2 could have table1, table3, user3 could have nothing, user4 could have table1,table2,table3 etc
T 1568207424 18<Myon18>	more details in config.log
T 1568207427 18<lseactuary18>	that is already 4 combinations in just 3 tables
T 1568207450 18<lseactuary18>	also permissions are read only
T 1568207572 18<[patrik]18>	lseactuary: 10 tables is a rather small data model. Also it is my experience that when you allow "individual" grants rather than application-use-case grants, you'll always have the self proclaimed Excel-master dude of the place login with his ODBC-driver and Excel and mess shit up.
T 1568207588 18<lluad18>	aborigen1020, configure generally leaves a configure.log file lying around that shows exactly what failed and where
T 1568207618 18<aborigen102018>	Myon, various version psql-odbc give various error. e.g.  "psqlodbc-09.06.0500" says "error: unixODBC library "odbcinst" not found". But  in config.log: "ODBC_CONFIG='/usr/local/bin/odbc_config'"
T 1568207631 18<[patrik]18>	I've seen this more times than i'd like to remember. Im still subscribing to the application-use-case role based stuff, so you allow access only through your app. Not directly to tables for individual users.
T 1568207635 18<lseactuary18>	[patrik] in reality there are hundreds, i am just testing on a smaller subset
T 1568207646 18<[patrik]18>	oh ok.
T 1568207657 18<lseactuary18>	[patrik] do you mean some API gateway which manages permission?
T 1568207671 18<lseactuary18>	the plan was for only admin to grant permissions, i will write this as code once we get the request
T 1568207678 18<lseactuary18>	im just trying to figure out the right mechanism
T 1568207682 18<lseactuary18>	so i can code it
T 1568207696 18<Myon18>	aborigen1020: why compile such an old version?
T 1568207750 18<aborigen102018>	Myon, i use postgresql "old' version - 9.6.5
T 1568207758 18<Myon18>	doesn't matter
T 1568207763 18<Myon18>	the driver is compatible
T 1568207776 18<Myon18>	also, why use 9.6.5 and not 9.6.15?
T 1568207800 18<Myon18>	your OS should have packages for psqlodbc, use these
T 1568207871 18<aborigen102018>	<Myon>, yes, have, but with version from repository i got segfault...
T 1568207898 18<[patrik]18>	lseactuary: no. just identify which functions (forms or webpage or whatever) use which tables and in your application grant the correct permissions to the role(s). When the next application user comes along just grant him "APP_USER" or whatever.
T 1568207902 18<dotjosh_18>	I'm running pg_dump for the first time on windows and I can see it writing the file to about 94MB and stopping, but the pg_dump process is never ending.  Using c:\Program Files\PostgreSQL\10\bin>pg_dump -U myuser -W -F t mydbname > d:\backup.tar
T 1568207914 18<aborigen102018>	so i tryed install driver from source
T 1568207931 18<Myon18>	even more reason to use a newer version
T 1568207944 18<Myon18>	which OS is that, btw?
T 1568207989 18<xocolatl18>	in a C trigger, what's the best way to find out if a column has been updated or not?
T 1568208020 18<aborigen102018>	Myon, Debian 9.2
T 1568208052 18<Myon18>	you should deploy upgrades, which have a current PostgreSQL version
T 1568208055 18<lavalike18>	how many triggers is a foreign key worth?
T 1568208060 18<Myon18>	for a newer psqlodbc, use apt.postgresql.org
T 1568208087 18<xocolatl18>	lavalike: two
T 1568208155 18<lavalike18>	xocolatl: really! I was starting to think it was a lot more
T 1568208162 18<xocolatl18>	why?
T 1568208196 18<lavalike18>	I started out with implementing INSERT so that it does check the referenced id in the other table, but then there's UPDATE, and DELETE
T 1568208222 18<xocolatl18>	that's the second trigger, on the other side
T 1568208222 18<Myon18>	aborigen1020: also, there's proper odbc libs in Debian, your /usr/local/bin suggests that you messed around a lot
T 1568208232 18<Myon18>	and /lib/cpp... good luck with that system
T 1568208257 18<xocolatl18>	lavalike: looks like it's 4 triggers
T 1568208292 18<aborigen102018>	and with psqlodbc-11.00.0000 i got : "error: unixODBC library "odbcinst" not found".
T 1568208305 18<lavalike18>	xocolatl: some of those VERBs can be squashed together?
T 1568208307 18*	xocolatl should know this, since he implemented his own fks with 4 triggers
T 1568208328 18<xocolatl18>	insert/update on fk side, update/delete on pk side
T 1568208365 18<ilmari18>	Myon: /lib/cpp is an alterntive managed by the 'cpp' package
T 1568208402 18<Myon18>	ilmari: *this* /lib/cpp is something that makes ./configure choke
T 1568208408 18<ilmari18>	ah
T 1568208429 18<Myon18>	aborigen1020: are odbcinst and odbcinst1debian2 properly installed?
T 1568208440 18<ilmari18>	aborigen1020: what does 'update-alternatives --display cpp' output?
T 1568208442 18<lavalike18>	xocolatl: good point, DELETE is not needed on the first side
T 1568208475 18<aborigen102018>	<Myon>, i will be happy to use version from  Debian repository, but with her i do not use odbc, because got segfault..
T 1568208517 18<aborigen102018>	Myon, yes
T 1568208632 18<aborigen102018>	<ilmari> - cpp - a  link best version is /usr/bin/cpp
T 1568208632 18<aborigen102018>	  links currentrly points to /usr/bin/cpp
T 1568208632 18<aborigen102018>	  link cpp is /lib/cpp
T 1568208632 18<aborigen102018>	/usr/bin/cpp  priority 10
T 1568208639 18<aborigen102018>	automatic mode
T 1568208739 18<xocolatl18>	lavalike: what are you working on?
T 1568208786 18<lavalike18>	xocolatl: I was helping a person yesterday and they wanted to make foreign key constraints transitive, the only solution I could come up with was using triggers, and then I realized I needed more than 1
T 1568208797 18<aborigen102018>	<Myon> and i assemble unixODBC from sources, and use option --with-unixodbc=/path/to/unixODBC/ and gets the same error - "odbcinst not found"
T 1568208803 18<xocolatl18>	lavalike: transitive?
T 1568208818 18<lavalike18>	xocolatl: I agree it warrants some explanation hang on
T 1568209148 18<lavalike18>	xocolatl: https://pastebin.com/raw/AeYnWCQ0 they were trying to make the last insert fail, because the referenced rows do not reference the same row in foo
T 1568209187 18*	xocolatl studies
T 1568209227 18<xocolatl18>	I see
T 1568209330 18<xocolatl18>	lavalike: create table quux (id, barid, bazid, fooid, foreign key (barid, fooid) references bar, foreign key (bazid, fooid) references baz);
T 1568209422 18<xocolatl18>	not sure how this should be modeled "correctly"
T 1568209442 18<xocolatl18>	what's the real-world use case for such a model?
T 1568209455 18<lavalike18>	that I dont' know
T 1568209492 18<xocolatl18>	you don't know the real-world use case?
T 1568209496 18<lavalike18>	nope
T 1568209501 18<Myon18>	aborigen1020: I'd think whatever makes your current install segfault will also make the build segfault, so better try to fix the real problem
T 1568209515 18<Myon18>	the Debian packages should really Just Work
T 1568209518 18<xocolatl18>	oh, was it something in this channel?  I was thinking you were at a client
T 1568209533 18<Myon18>	if something segfaults, install the dbgsym package, and run gdb
T 1568209622 19*	Now talking on 22#postgresql
T 1568209622 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568209622 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Tue Sep 10 01:19:16 2019)
T 1568209622 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568209624 18<lavalike18>	(:
T 1568209804 18<lavalike18>	how does  FOREIGN KEY (barId, fooId) REFERENCES bar  work?
T 1568209838 18<aborigen102018>	Myon, i use gdb from this command "isql -v dsn-name", and got some small lines: "Program received signal SIGSEGV, Segmentation fault.
T 1568209839 18<aborigen102018>	strlen () at ../sysdeps/x86_64/strlen.S:106
T 1568209839 18<aborigen102018>	106	../sysdeps/x86_64/strlen.S: No such file.
T 1568209856 18<lavalike18>	oh, yeah, fooId is in the table now, nevermind
T 1568209857 18<xocolatl18>	lavalike: what do you mean?  (it was shorthand, I can write out the whole thing if you want)
T 1568209877 18<lavalike18>	no I get it, I was thinking in the given example it seemed to pick barId from the quux table but fooId from the bar table and I was puzzled
T 1568209892 18<xocolatl18>	no
T 1568209924 18<lavalike18>	my brain refused to pick up the added column to quux even tho it had processed that it was there
T 1568209926 18<xocolatl18>	also unfortunate is postgres doesn't realize that bar(id, fooid) is unique even though id is the pk, so a second index is required
T 1568209939 18<aborigen102018>	Myon, send this to bugs.debian.org? :)
T 1568210295 18<Myon18>	aborigen1020: yes, but please install debug symbols first
T 1568212408 19*	Now talking on 22#postgresql
T 1568212408 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568212408 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Tue Sep 10 01:19:16 2019)
T 1568212408 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568212683 18<dognosewhiskers18>	I've spent the last "numerous hours" swearing and fiddling with my PostgreSQL installation (Windows 10). The EnterpriseDB installer for Windows has broken somehow. Likely Microsoft's rather than EnterpriseDB, Inc.'s fault, but still, it gives that nonsensical error about not being able to finish the installation and you end up with a semi-broken installation (no matter what "tricks" you try to solve it -- they don't work) and have to figure
T 1568212683 18<dognosewhiskers18>	out how to initdb and start PG on your own. No "service" is installed, so I now need to have an ugly extra cmd.exe window running at all times (as if I didn't already have enough of those...) just to keep PG up.
T 1568212691 18<dognosewhiskers18>	So to clarify: I have my PG running now, but it's in a very... "flimsy" state. I don't feel comfortable doing it like this, and it took me a very long time and lots of effort to figure out. If somebody from EnterpriseDB is reading this, you should fire up a disgusting Windows 10 VM and try to figure this out ASAP. (I don't blame you for not keeping up with the constant random breaking changes by MS.)
T 1568212741 18<rivyn18>	Why not contact EnterpriseDB support about it?
T 1568212827 18<dognosewhiskers18>	rivyn: Because they probably are in here and also, e-mail has been made frustrating these days.
T 1568213239 18<rivyn18>	Probably not, actually.
T 1568213373 18<lol-md518>	xocolatl: <xocolatl> also unfortunate is postgres doesn't realize that bar(id, fooid) is unique even though id is the pk, so a second index is required
T 1568213377 18<lol-md518>	could you elaborate on this?
T 1568213452 18<xocolatl18>	lol-md5: a foreign key requires a unique index on the pk side.  if you have a compound foreign key (such as (id, fooid)) that contains something that is already known unique/notnull, you still need to have a separate index over all the keys.  that's just dumb
T 1568213477 18<xocolatl18>	postgres should be able to use the pk index on (id) for a foreign key containing it
T 1568213488 18<lol-md518>	xocolatl: you mean you have to index (id, fooid)?
T 1568213491 18<xocolatl18>	yes
T 1568213501 18<lol-md518>	hum
T 1568213703 18<Zr4018>	did nobody bother to write a patch, or is there an actual reason why that's the case?
T 1568213715 18<xocolatl18>	I don't think anyone has bothered
T 1568214295 18<davidfetter18>	Myon, I was wondering whether pg_dirtyread, or at least some of its infrastructure, should go into core
T 1568214325 18<davidfetter18>	Myon, this is because it appears to depend pretty tightly on tupconvert from core
T 1568214691 18<localhorse18>	does the order of conditions chained with AND in a WHERE clause matter? are they shortcircuit-evaluated?
T 1568214736 18<peerce18>	the expression is optimized, and yes if any expression is false, it doesn't have to eval the rest
T 1568214753 18<peerce18>	AFAIK, there's no guaranteed execution order of a AND b AND c
T 1568214937 18<xocolatl18>	localhorse: there is absolutely no guarantee of execution order.  you can't "protect" one condition with another like you can in imperative languages
T 1568214954 18<localhorse18>	ok good
T 1568215111 18<esran18>	xocolatl, if id is the pk then surely your foreign key is just id and it doesn't matter what fooid is so drop it from the fk?
T 1568215127 18<xocolatl18>	surely not!
T 1568215310 18<xocolatl18>	there are many reasons to want more columns in the fk
T 1568216189 18<davidfetter18>	right now, surrogate keys are often a way to save space. I wonder if there isn't a way to implement FKs so that they're pointers to the PKs instead of separate storage
T 1568216324 18<xocolatl18>	wouldn't solve this problem
T 1568216560 18<zakharyas18>	Noob question, but how I can both return and use the result of a function in a SELECT?  I want to do something like SELECT length(foo) AS bar, bar - 3, but the second use of "bar" is treated as a column, and it doesn't exist.
T 1568216626 18<peerce18>	length(foo) as bar, lenth(foo)-3 as bar3
T 1568216632 18<peerce18>	length, even.
T 1568216646 18<peerce18>	you cna't reference aliases in other expressions at the same level
T 1568216663 18*	davidfetter DCCs peerce some coffee
T 1568216668 18<zakharyas18>	peerce: that would work but I'd prefer not to repeat the function as it's rather expensive.
T 1568216688 18<peerce18>	then use a CTE
T 1568216694 18<xocolatl18>	zakharyas: you'll have to use a superquery then
T 1568216704 18<davidfetter18>	zakharyas, pg is smart enough to execute functions appropriately...if they're not marked as VOLATILE
T 1568216727 18<davidfetter18>	(in which case it's still smart enough, and it executes them each time they're called)
T 1568275714 20*	Disconnected (20)
T 1568275738 19*	Now talking on 22#postgresql
T 1568275738 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568275738 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Tue Sep 10 01:19:16 2019)
T 1568275739 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568275777 18<enoq18>	what I've done in the past is to create some text field that contains contents of all fields
T 1568275786 18<enoq18>	the ones that I want to query
T 1568275874 18<aborigen102018>	peerce, no. All configs for access via odbc are identical
T 1568275923 18<peerce18>	you could create a full text index that was based on the concatenation of the fields.
T 1568275937 18<peerce18>	and use the full text search primitives
T 1568275944 18<peerce18>	??FTS
T 1568275944 18<pg_docbot18>	http://www.sai.msu.su/~megera/postgres/gist/tsearch/V2/ :: http://rachbelaid.com/postgres-full-text-search-is-good-enough/
T 1568275944 18<pg_docbot18>	http://www.sai.msu.su/~megera/postgres/fts/doc/index.html :: https://www.postgresql.org/docs/current/static/textsearch.html
T 1568275950 18<enoq18>	thank you
T 1568276028 18<Myon18>	aborigen1020: there's something fishy with the filesystem I think, config.log is full of errors that it can't find conftest.c
T 1568276163 18<aborigen102018>	and this file is missing on the 2 systems
T 1568277106 18<enoq18>	in mysql there's a size limit for indices, is there such a thing in postgres?
T 1568277118 18<enoq18>	mainly interested in indices over text fields
T 1568277493 18<Myon18>	enoq: yes, somewhere around 1-2kB
T 1568277521 18<enoq18>	is that a reason to go for varchar(255)?
T 1568277529 18<enoq18>	instead of text
T 1568277545 18<Myon18>	varchar(1000) is a thing as well
T 1568277563 18<Myon18>	(won't help with long utf8 chars, though)
T 1568277565 18<enoq18>	I suppose if you have an index over a text field it will throw an exception if it surpasses the index limit
T 1568277578 18<Myon18>	yes
T 1568277579 18<peerce18>	a varchar(255) would error if you tried to store a longer string in it, is that what you want?
T 1568277608 18<pstef18>	possibly
T 1568277627 18<pstef18>	if you want to avoid getting another error (that the value can't fit into an index)
T 1568277629 18<peerce18>	any sort of error exception requires the whole transaction be rolled back
T 1568277655 18<pstef18>	my workaround has been index on left(field, limit)
T 1568277688 18<Myon18>	pstef: but I guess the planner then needs that in the query as well?
T 1568277698 18<Myon18>	hash indexes would work I guess
T 1568277739 18<pstef18>	yes, sorry for being vague. you'd need left(field, limit) in the query too, and then the desired "x = field" predicate
T 1568277749 18<enoq18>	is varchar implemented on bytes? so a full unicode string would take 4 times it's size?
T 1568277768 18<Myon18>	enoq: varchar(n) is n characters
T 1568277827 18<pstef18>	hash indexes seems a good idea now; I didn't have those the last time I had this problem :)
T 1568277848 18<Myon18>	same here
T 1568277893 18<enoq18>	ah I see
T 1568277900 18<enoq18>	but that of course affects indices
T 1568277940 18<enoq18>	so varchar 512 would fit into the 2kb maximum for an index (worst case)
T 1568278177 18<enoq18>	correct?
T 1568279415 18<kevinsjoberg18>	I'm trying to write a query for this problem http://sqlfiddle.com/#!17/a6646/2. I've played around a bit but does not manage to get anything giving me the desired result. Any tips?
T 1568281601 18<Renter18>	kevinsjoberg: so that's a puzzle where you're given the schema and the problem and need to produce the result?
T 1568281610 18<Renter18>	Or do you need to alter the schema as well?
T 1568281680 18<kevinsjoberg18>	Renter: The schema I've written myself to demonstrate the problem I'm having. The schema is a simplified version of my application's schema.
T 1568281720 18<kevinsjoberg18>	The problem could be explained as "Given n amount of people belonging to the same fact, the same people need to be part of the same family to be considered valid".
T 1568281762 18<kevinsjoberg18>	I get false positives when I dabble with the queries myself (I'm no SQL-expert by any means).
T 1568281771 18<Renter18>	Is there a reason why you only have the maps?
T 1568281787 18<Renter18>	To me relational databases start off by having single lines for single entities in tables
T 1568281799 18<Renter18>	So for instances, to me this has three types of entities: facts, persons and families
T 1568281853 18<Renter18>	The need becomes more obvious if you add some more metadata to facts
T 1568281856 18<kevinsjoberg18>	Renter: There is a reason to it. This data does not map directly to tables in my database but are constructed from multiple data sources.
T 1568281875 18<kevinsjoberg18>	Renter: Some of the data, I don't even control.
T 1568281892 18<Renter18>	yeah, so your _maps tables are nxm relations that bind different tables together right?
T 1568281974 18<Renter18>	the problem becomes obvious when you simply want a list of unique facts, you have to write something like 'select distinct fact_id from fact_maps order by fact_id'
T 1568282013 18<Myon18>	kevinsjoberg: select fact_id, family_id, count(*) from fact_maps fc join family_maps fm on fc.person_id = fm.person_id group by 1, 2 having count(*) > 1;
T 1568282374 20*	Disconnected (20)
T 1568282397 19*	Now talking on 22#postgresql
T 1568282397 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568282397 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Tue Sep 10 01:19:16 2019)
T 1568282398 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568282407 18<kevinsjoberg18>	Myon: Thanks, it seems to be working alright. I think the having part was what I was missing.
T 1568282800 18<gajus18>	given that LATERAL JOINs and DISTINCT ON can often solve the same problem, is there a clear reason to always prefer one over the other?
T 1568282864 18<gajus18>	My rough way of picking one or the other depends on how large the dataset set it
T 1568282880 18<gajus18>	distinct on generally will have faster query execution plan but will be more memory greedy
T 1568282891 18<gajus18>	lateral will be more memory efficient but slower
T 1568282898 18<gajus18>	is there anything else to consider?
T 1568283397 18<Janni18>	Hello.
T 1568283440 18<Janni18>	Does anybody know how to log the parameters of failed queries?
T 1568283483 18<Janni18>	I DO get parameters in my logs for queries that are slower than the log_min_duration threshold. That allows me to reproduce and debug.
T 1568283512 18<Janni18>	However when a query times out or there is some other ERROR, I do not get any parameters in the logs, which makes things much harder.
T 1568284888 18<enoq18>	does unique check null?
T 1568284903 18<enoq18>	I have an optional field that should be either null or unique
T 1568284932 18<Myon18>	what happened when you tried?
T 1568284950 18<enoq18>	I'm currently defining the schema upfront
T 1568284994 18<enoq18>	ah nvm "However, two null values are never considered equal in this comparison. That means even in the presence of a unique constraint it is possible to store duplicate rows that contain a"
T 1568285013 18<enoq18>	https://www.postgresql.org/docs/11/ddl-constraints.html#DDL-CONSTRAINTS-UNIQUE-CONSTRAINTS
T 1568285169 18<enoq18>	I suppose that's why you use IS NULL rather than = NULL
T 1568288313 18<ne2k18>	I'm writing a trigger to modify a value on insert based on a modifier stored in another table. it feels as though it would be efficient to do this as a per statement trigger, but this appears to only be allowed AFTER insert; should I suck it up and do it per row, or is there a way to do what I want per statement?
T 1568288509 18<Myon18>	you'll need a transition table for that, and PG11+ (or was it 10?)
T 1568288531 18<Myon18>	otherwise, statement level triggers don't see individual rows
T 1568288556 18<ne2k18>	Myon, I have 11, but the docs say that they can only be used on AFTER. and I presume it would be bad form to allow it to insert the (unmodified) data, and the do an update to modify it?
T 1568288576 18<ne2k18>	if that would even work anyway -- it would end up being an upsert, which has to be written with a per-row thing anyway
T 1568288689 18<Myon18>	yeah that sounds horrible
T 1568288743 18<ne2k18>	I'll just do it per row
T 1568288795 18<aborigen102018>	Myon, i quit trying to build psql-odbc drivers from src. I created new cluster, is created dump from working cluster and restored him in new cluster. After this, i changed settings for DSN (on /etc/odbc.ini - write on port to connection), and ALL is working! Congratulations! This is large success.
T 1568288843 18<Myon18>	odbc is quite happy to segfault if something is wrong in the config, afaict
T 1568288889 18<aborigen102018>	I must say one thing: i dont install package odbc-postgresql, but copy two lib for him from working servers
T 1568289015 18<aborigen102018>	config odbc is standarted on all my servers, and not writted by "nahds" - only copy.
T 1568290781 18<Mikjaer18>	I am going to create a table which needs to hold historical data for each row, what is the best way to do this? An exampe, date, name and phonenumber and we want to be able to know what phonenumber a given person had at a given time. The best / most efficient way to do this?
T 1568290974 18<ne2k18>	Mikjaer, you basically just need to add a timestamp column and add it to the primary key; then you can select distinct on that order by ts desc to get the most recent one, or select where ts between a and b, or whatever.
T 1568291002 18<Mikjaer18>	ne2k: that was our idea as well, but we're afraid that the performance is going to be horible?
T 1568291033 18<ne2k18>	Mikjaer, you can either do that and handle it in the queries, or have two tables, one with the current data and one with historic data, which you append to with a trigger when you update the main one
T 1568291153 18<ne2k18>	Mikjaer, I would do it with a single table, and make a view for the "current one"; then you can access the current data as if it's a table, and you don't have to write any different queries, and then later, if you decide to optimize it by moving it to two tables and a trigger, you don't need to change your queries
T 1568291197 18<ne2k18>	Mikjaer, how much data and how many reads/writes per time are we talking about, here anyway?
T 1568291261 18<Mikjaer18>	around 100 millions rows, and it's only being used for report generating
T 1568291340 18<Mikjaer18>	We store the entire dataset on a different server, and record changes, because we need to be able to show the entire history of the data, for legal purposes.
T 1568291454 18<ne2k18>	Mikjaer, so you're likely to access the "current" one far, far more often than any of the old ones?
T 1568291482 18<Mikjaer18>	yes
T 1568291509 18<ne2k18>	and is the historic data never required to be accessible on the "live" server, only through some other access method?
T 1568291530 18<Mikjaer18>	but the system is to be used by accountants that might want to know things like "How many values have been altered by more then x during period z" and not have to wait several minutes to get data back
T 1568291568 18<Mikjaer18>	The historic data is only used by accountants, lawyers and stuff like that if we need to proove something in a court or towards a goverment agency
T 1568291635 18<ne2k18>	select * from phone_archive where ts between za and zb group by userid having count(*) > x;
T 1568291648 18<ne2k18>	I can't imagine that is going to take minutes on 100 million rows
T 1568291685 18<Mikjaer18>	That was my opinon as well
T 1568291709 18<ne2k18>	Mikjaer, but given the size, and the fact that current is used a lot, would make me optimize it from the start
T 1568291758 18<ne2k18>	i.e. store current in one table, and then update archive (possibly on a foreign table) with a trigger
T 1568291860 18<Mikjaer18>	Can
T 1568291869 18<Mikjaer18>	Can't i do that with something like cached-view?
T 1568292400 18<theseb18>	Why do subqueries need parens like this... "select <column list> from (<select statement>) <name>;
T 1568292489 18<Moonsilence18>	because the from list are individual table expressions... syntactically needed in the case of subqueries
T 1568292529 18<Moonsilence18>	think of each subquery as a tabular result, from which you select just like directly from real tables.
T 1568292550 18<Moonsilence18>	...FROM table_a, (subquery), table_b, view_c, ...
T 1568292765 18<theseb18>	"select 1, 2, 3" returns a single record of raw data...how would you return MULTIPLE records?  I tried "select ( (1, 2, 3), (4, 5, 6) );" but that barfed
T 1568292856 18<Moonsilence18>	have a look at the examples in the docs for the sql-command VALUES
T 1568292860 18<Moonsilence18>	??values
T 1568292860 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/sql-values.html
T 1568293219 18<ne2k18>	theseb, values ((select(1,2,3))), ((select(4,5,6))); does what you want. not sure why you'd want to, mind you
T 1568293306 18<ilmari18>	that gives you two rows of one column each containing a row value
T 1568293322 18<ilmari18>	values (1,2,3),(3,4,5); gives you two rows of three columns
T 1568293486 18<theseb18>	ilmari, ne2k:  values (1, 2, 3), (4, 5, 6); indeed gives 2 rows of 3 cols.....in mysql that syntax does not work....why isn't this more universal?
T 1568293540 18<Myon18>	the likely answer is "mysql isn't following the SQL standard there"
T 1568293873 18<ne2k18>	theseb, perhaps a more important question is: what are you actually trying to do?
T 1568293914 18<theseb18>	ne2k: thanks...i sometimes want/need to create table like objects from raw data because i don't have permission to add to the database
T 1568293924 18<theseb18>	ne2k: it seems select 1, 2, 3;
T 1568293932 18<Myon18>	there's also UNION
T 1568293934 18<theseb18>	ne2k; gives me a single record of raw data
T 1568293944 18<theseb18>	ne2k: i just got stuck at doing multiple records
T 1568293965 18<Myon18>	select 1,2,3 from generate_series(1,3);
T 1568294003 18<ilmari18>	theseb: the VALUES page shows the equivalent SELECT ... UNION ALL sequence
T 1568294022 18<theseb18>	Myon: wait...suppose you wanted 1st record to have 3 columns w/ values "a", "b", "c"....and second record to contain "d", "e", "f"
T 1568294032 18<Myon18>	values
T 1568294034 18<ilmari18>	if you need it to work on datbases that don't support the standard VALUES syntax
T 1568294046 18<theseb18>	Myon: values ("a", "b", "c"), ("d", "e", "f")
T 1568294068 18<theseb18>	Myon: sadly PG seems to be the only sane SQL today
T 1568294094 18<Myon18>	not our problem :)
T 1568294107 18<theseb18>	:)
T 1568294117 18<Myon18>	also, it's 'a', not "a"
T 1568294121 18<theseb18>	right
T 1568294131 18<ne2k18>	theseb, depending on what you're actually trying to do, you might want to consider jsonb columns too.
T 1568294152 18<theseb18>	Myon: does SELECT have a way to do the equivalent of values ("a", "b", "c"), ("d", "e", "f") ?
T 1568294156 18<theseb18>	(w/ single quotes) ?
T 1568294186 18<theseb18>	Myon: maybe it will be portable
T 1568294197 18<ilmari18>	14:13 < ilmari> theseb: the VALUES page shows the equivalent SELECT ... UNION ALL sequence
T 1568294200 18<ilmari18>	theseb: ^^
T 1568294216 18<ne2k18>	theseb, that bit about "what are you /actually/ trying to do" still seems to be rather vague
T 1568294224 18<ilmari18>	theseb: https://www.postgresql.org/docs/current/sql-values.html#id-1.9.3.184.8
T 1568294241 18<theseb18>	ne2k: ok...i'll give more details....sec
T 1568294360 18<theseb18>	ne2k: https://pastebin.com/wC3vf6ii
T 1568294373 18<theseb18>	ne2k: see those long hex strings?
T 1568294396 18<doev18>	Can a identity/always column used in combination with inheritance ?
T 1568294417 18<theseb18>	ne2k: the script pulls some data from a database on a specific company identified by its unique hex strings......i want to rerun that script for several companies all with different hex strings
T 1568294423 18<doev18>	It seems, if all child tables use an own counter.
T 1568294449 18<theseb18>	ne2k: so basically i want to do something like a for loop where i alter those hex strings for each loop (company)
T 1568294484 18<theseb18>	ne2k: because i can't add to the database....i need to make a table like structure of the hex strings on the fly to somehow use in a "for loop" type construct
T 1568294537 18<ne2k18>	theseb, and you have some bizarre restriction that you can't create an actual table of the values?
T 1568294601 18<wds18>	Hey, anyone that can help with a design question relating to narrow, large tables with timeseries-like data?
T 1568294611 18<theseb18>	ne2k: yes
T 1568294612 18<ne2k18>	wds, ??ask
T 1568294619 18<ne2k18>	??ask
T 1568294619 18<pg_docbot18>	http://www.catb.org/~esr/faqs/smart-questions.html :: https://workaround.org/getting-help-on-irc
T 1568294627 18<ne2k18>	don't ask to ask, just ask
T 1568294628 18<theseb18>	ne2k: it will say.."you don't have permission to create tables or some such"
T 1568294677 18<wds18>	Got a table with just 4 columns; id, value, source and timestamp. Most queries involve "select value, source, time where source = x and time between a and b"
T 1568294712 18<wds18>	table has like 40mil rows, and once a query is going to return a few tens of thousand, the planner uses a sequential scan
T 1568294731 18<ne2k18>	wds, do you have an index on (source, time)?
T 1568294745 18<wds18>	Yup, it still prefers sequential scan
T 1568294757 18<wds18>	we got it to use index scanning when using "include value" on that same index
T 1568294768 18<wds18>	I suppose because it then no longer has to access the table at all
T 1568294771 18<ne2k18>	wds, I was having much the same sort of problem the other day.
T 1568294779 18<wds18>	but that means our entire table is basically stored in the index, which seems a little... silly?
T 1568294804 18<ne2k18>	wds, what is id for?
T 1568294823 18<wds18>	it's really just the primary key and hasn't served a purpose in our application (so far).
T 1568294843 18<ne2k18>	wds, isn't (source, time) more likely to be the primary key?
T 1568294903 18<wds18>	It's a possibility, we haven't made the decision to limit the data that way
T 1568294947 18<ne2k18>	theseb, so you basically want table company(name, hex1, hex2, hex3) and to lateral join that to you query you pasted, substituting the hex values for the ones from the table?
T 1568294963 18<ne2k18>	theseb, except the table company has to be a literal
T 1568295033 18<ne2k18>	wds, that include thing is new to me, I'd need to read up on it
T 1568295099 18<wds18>	Clustering has helped the performance as well, but I'm a bit skeptical that for 40mil rows, our queries take over 2 minutes on an unclustered table?
T 1568295120 18<ne2k18>	"However, an index-only scan can return the contents of non-key columns without having to visit the index's table, since they are available directly from the index entry." presumably, the cost of doing an index scan and visiting the table for each row is deemed to be greater than a seq scan in your case.
T 1568295163 18<ne2k18>	wds, wait, what? selecting the rows between two times takes TWO MINUTES with a seq scan on 40m rows? that seems crazy
T 1568295296 18<wds18>	ne2k Yup! That's what is baffling us
T 1568295325 18<wds18>	Dataset doesn't seem too big to have reasonably performant queries even on a badly optimized database
T 1568295411 18<wds18>	although we just finished a manual vacuum analyze and the query we've been testing with went from 152s to 54s
T 1568295438 18<theseb18>	ne2k: i think so
T 1568295450 18<theseb18>	ne2k: wait...lateral join?
T 1568295450 18<wds18>	but that's also strange to us because we exclusively do insert and select statements, no update/delete. It was just after (re-) adding the composite index though
T 1568295465 18<theseb18>	ne2k: i learned about inner, left, right, full, cross joins
T 1568295480 18<theseb18>	ne2k: are you saying i missed one? there is also a LATERAL join?
T 1568295499 18<ne2k18>	wds, lateral join is like a foreach; it allows you to use values on the left of the join in where conditions on the right
T 1568295515 18<theseb18>	ne2k: was that for me?
T 1568295530 18<ne2k18>	theseb, yes, apologies
T 1568295540 18<theseb18>	ne2k: i was wondering how to do a for loop in SQL...if lateral joins is the way then they are definitely important
T 1568295600 18<ne2k18>	theseb, you can always write a procedural function in plpgsql (or other), but lateral join allows you to do a lot of "foreach"-y things without needing to resort to it
T 1568295649 18<ne2k18>	wds insert into test2 (select * from generate_series(1, 7000) a, generate_series('2019-01-01T00:00:00', now(), '1 hour') b, random()); # just making me 40m rows of test data.
T 1568295686 18<ne2k18>	it's taking... quite a while
T 1568295709 18<theseb18>	ne2k: lateral seems new... https://heap.io/blog/engineering/postgresqls-powerful-new-join-type-lateral
T 1568295712 18<wds18>	okay so after VACUUM ANALYZE, our sequentially scanning for 1.9mil result rows in our 40mil table takes about 42-44 seconds. Still long. Forcing it to use (source, time) index takes 18 seconds
T 1568295717 18<theseb18>	ne2k: it is a a "power new join type"
T 1568295722 18<ne2k18>	theseb, it's relatively new
T 1568295722 18<theseb18>	powerful*
T 1568295744 18<theseb18>	ne2k: i can't believe SQL is 50 years old and there is no universal way to do a for loop
T 1568295753 18<theseb18>	or that it was only added few years ago
T 1568295758 18<theseb18>	ne2k: why is that?
T 1568295897 18<wds18>	ne2k And if we only select source,time columns, it's like a 0.5s index scan and done. That's more like what we're expecting :D
T 1568295902 18<dognosewhiskers18>	It's scary to me that BigSQL was removed from the Windows download page for PG. Now there's just EnterpriseDB's installer left, which, as I reported the other day, is broken now (likely due to random Microsoft changes to Windows 10 since it worked recently). I sure hope they don't remove Windows support entirely.
T 1568295911 18<ne2k18>	wds, yes, that's because they're actually in the index
T 1568295929 18<wds18>	Mh. So is it recommended/normal to index all columns in a narrow, large table?
T 1568295944 18<wds18>	That's it's going to take twice the storage space
T 1568296152 18<ne2k18>	wds, not something I can answer, I'm afraid. i've just tried select source, ts from test2 where source=99 and ts between '1 Mar 2019' and '31 May 2019'; and that took 1.7s
T 1568296196 18<ne2k18>	oh, and now the same thing takes 6ms. as does select source, ts, value
T 1568296208 18<ne2k18>	guess the index wasn't cached
T 1568296230 18<wds18>	Yea so clearly we have some performance problems in general
T 1568296249 18<ne2k18>	wds, what is the storage of your value column?
T 1568296296 18<wds18>	\d+ gives storage: main for that column
T 1568296384 18<wds18>	also, source is a foreign key to another table, in case that would matter?
T 1568296440 18<ne2k18>	not for selects, it shouldn't
T 1568296462 18<ne2k18>	wds, as I said, this is kinda beyond me, but I'd be very interested in the answers you get back
T 1568296496 18<ne2k18>	wds, but when I do select source, ts, value from test2 where source=x and ts between a and b; it does it with an index scan and it's hella fast
T 1568296511 18<wds18>	and value is not in your index?
T 1568296516 18<ne2k18>	wds nope
T 1568296525 18<ne2k18>	storage is plain on all columns
T 1568296589 18<wds18>	Okay, we've tested some IO speeds, and while we run the super slow selects, the postgres processes add up to maybe 5MB/s (tops) while the statement is running (using oistat on ubuntu)
T 1568296595 18<ne2k18>	just realized I foolishly inserted the same single random value for all the data, rather than a different one for each
T 1568296626 18<watmm18>	Hi all. Can someone tell me why a standby with standby_mode on would be stuck recovering the last remaining megabyte or so of a db indefinitely? i.e. replication never completes :/
T 1568297263 18<ilmari18>	watmm: standby_mode=on means that it will continuosly keep recovering WAL as it gets it, insted of stopping at the end
T 1568297286 18<ilmari18>	watmm: do you have restore_command or primary_conninfo configured?
T 1568297409 18<ilmari18>	watmm: if you want to be able to run read-only queries against the standby, set hot_standby=on
T 1568297552 18<watmm18>	ilmari: I have hot_standy on, and primary_conninfo is defined, but no restore_command.
T 1568297600 18<watmm18>	Oh i do have a pg_restore tool, it's just not in a conf anywhere
T 1568297641 18<Bish18>	does a cascade delete not activate triggers?
T 1568297667 18<watmm18>	The problem sounds like the one described in the pg_standby section of https://pgdash.io/blog/postgres-physical-replication.html
T 1568297879 18<Bish18>	a after trigger will not fire for a cascade delete?
T 1568298224 18<wds18>	Is there a way to monitor/test performance of IO operations on a query?
T 1568298246 18<wds18>	*shifty eyes at our hard drives*
T 1568299569 22*	26ChanServ gives channel operator status to 18xocolatl
T 1568299588 22*	26xocolatl has changed the topic to: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568299591 22*	26ChanServ removes channel operator status from 18xocolatl
T 1568299791 18<watmm18>	Hrm. An update on the above. The standbys have the same wal files as the master so perhaps they are up to date, but the size of their DBs is smaller. Is this normal?
T 1568299895 18<doev18>	How can I insert into a table with not use any values ... just to get an new row with id.
T 1568299941 18<Moonsilence18>	Hi! How can orphan temp tables be left in the database? Will these eventually get dropped?
T 1568299965 18<xocolatl18>	Moonsilence: yes
T 1568299985 18<xocolatl18>	doev: insert into tablename default values;
T 1568300102 18<Moonsilence18>	we had a power outage this morning and since then autovacuum is logging thousands of messages "found orphan temp table". I dont understand how they could have been left behing, since afaik, the application that created them, also died due to power loss, hence sessions gone, hence temp tables should dissappear.
T 1568300130 18<doev18>	xocolatl, yes works
T 1568300158 18<Moonsilence18>	even after rebooting my instance, they still remain and autovacuum is noticing them
T 1568300174 18<Moonsilence18>	how can I fix this?
T 1568300194 18<xocolatl18>	depending on your version autovacuum will just get rid of them, or when a new connection uses the same temp schema, or when wraparound comes
T 1568300204 18<xocolatl18>	you can fix it yourself by dropping the temp schemas
T 1568300266 18<xocolatl18>	drop schema pg_temp_4 cascade;  and similar
T 1568300688 18<Moonsilence18>	thanks
T 1568300692 18<harks18>	I have a query with 1.3 s explain analyze time, yet the query is already running for 1 minute without returning stuff.
T 1568300729 18<xocolatl18>	harks: it's not something silly like forgetting the ; at the end, right?
T 1568300737 18<harks18>	The query now finished after 4 min. Any ideas?
T 1568300759 18<xocolatl18>	it might have been stuck behind a lock
T 1568300768 18<xocolatl18>	if you run it again right now, does it take a long time?
T 1568300880 18<harks18>	It's 1.4 sec now. But I'm the only one working on this local db.
T 1568300911 18<harks18>	That's annoying.
T 1568301019 18<harks18>	I hate bugs that vanish once I start to analyze them.
T 1568301521 18<Moonsilence18>	xocolatl: Seems that 9.4 autovacuum doesnt drop orphaned temp tables, however in my 11 instances i found these log messages: autovacuum: dropping orphan temp table
T 1568301543 18<Moonsilence18>	so somewhere between 9.4 and 11 this automatic cleanup was implemented
T 1568301560 18<G3nka118>	Hi can anyone help me with this error "Table has type tid at ordinal position 1, but query expects character varying."
T 1568301755 18<Moonsilence18>	there are hidden columns xmin that might have been explicitly selected... these have type tid
T 1568302023 18<depesz18>	G3nka1: what's the query?
T 1568302034 18<depesz18>	and also, can you show us \d of the table?
T 1568302039 18<xocolatl18>	Moonsilence: yes, I said "depends on the version"
T 1568302040 18<G3nka118>	depesz, Its an update query on a table with 11 collumns and first column is char var and last is timestamp and the rest being text
T 1568302077 18<G3nka118>	The update query is by a benchmark and I am not sure which collumns it is trying to update
T 1568302079 18<depesz18>	G3nka1:please note I asked about query (and \d) and not "a description of a query"
T 1568302194 18<G3nka118>	depesz, https://paste.ubuntu.com/p/tT7ZPsCnBx/
T 1568302265 18<G3nka118>	>> Error in processing update to table: usertableorg.postgresql.util.PSQLException: ERROR: table row type and query-specified row type do not match Detail: Table has type tid at ordinal position 1, but query expects character varying.
T 1568302266 18<depesz18>	G3nka1: i still don't know what the query is. also - did you try dropping the policy and retrying?
T 1568302276 18<G3nka118>	I am still digging the exact update query
T 1568302614 18<G3nka118>	Oh maybe I know whats wrong, will get back to you depesz thank you for your interest
T 1568303520 18<spread1218>	does postgres support distributed transactions on linux?
T 1568303628 18<pstef18>	what are distributed transactions?
T 1568303667 18<spread1218>	nvm, the limitation is in my library
T 1568303755 18<G3nka118>	Okay I figured out that only after adding the policy on my table, update queries are failing. https://paste.ubuntu.com/p/mHg5XSPWTg/ Here is the function. I am sorry I still don't know the exact update query yet
T 1568303767 18<incognito18>	spread12: if you mean distributed over network, i once saw an extension that gives to postgresql the capability of sharding data
T 1568303852 18<spread1218>	this is the limitation: https://github.com/dotnet/corefx/issues/13532
T 1568303931 18<incognito18>	spread12: yes, postgresql libs has this feature : they call that => connection pooling
T 1568304110 18<incognito18>	spread12: http://www.npgsql.org/doc/connection-string-parameters.html => see the Pooling paragraph
T 1568304330 18<G3nka118>	i wonder twhat the type mismatch really is
T 1568304548 18<incognito18>	G3nka1: what is your app ? it's weird. it's trying to update the ctid ?
T 1568304588 18<G3nka118>	incognito, I wrote that policy as an requireement to log every row that has an operation made on
T 1568304627 18<incognito18>	G3nka1: before the policies, it was working ?
T 1568304631 18<RhodiumToad18>	if you want to log modifications to the table you should use a trigger, not RLS
T 1568304639 18<G3nka118>	Yes incognito it was working
T 1568304660 18<RhodiumToad18>	if you want to log reads, consider whether your requirements are reasonable
T 1568304697 18<G3nka118>	you are right RhodiumToad , this was mostly an hack and an experiment to benchmark if read logs enabled
T 1568304723 18<RhodiumToad18>	G3nka1: nevertheless, I think you have found a bug - what pg version are you using?
T 1568304746 18<G3nka118>	9.5
T 1568304760 18<RhodiumToad18>	9.5.what?
T 1568304805 18<G3nka118>	9.5.19
T 1568304811 18<spread1218>	incognito: i was talking about this feature: https://www.npgsql.org/doc/transactions.html?q=distributed
T 1568304835 18<G3nka118>	Infact I didn
T 1568304838 18<G3nka118>	t
T 1568304841 18<RhodiumToad18>	G3nka1: what was the exact error message, including any CONTEXT lines, and the explain of the failing query?
T 1568304844 18<G3nka118>	see this in earlier version of pg
T 1568304897 18<G3nka118>	It worked on 9.5.17 when I tried a few months ago, the same function
T 1568305098 18<RhodiumToad18>	hm, tried to reproduce on 11.5 and failed
T 1568305249 18<RhodiumToad18>	whut, did someone break readline detection in autoconf in the back branches?
T 1568305297 18<RhodiumToad18>	oh, my mistake
T 1568305334 18<RhodiumToad18>	typoed a directory name
T 1568305385 18<incognito18>	G3nka1: in the example i just saw, they never use the table_name in the USING clause
T 1568305407 18<G3nka118>	SO, I am running YCSB workloada on table usertable with above policy. Typically this is how the update query will look 'usertable SET field4 = 'allow' WHERE field0 = 'ads'",,,,,,"UPDATE usertable SET field4 = 'allow' WHERE field0 = 'ads'",,,""' Pretty simple
T 1568305422 18<G3nka118>	incognito, I don
T 1568305447 18<G3nka118>	*I don't think its YCSB issue, rather something to do with postgres recent patchs
T 1568305540 18<RhodiumToad18>	hm.
T 1568305586 18<RhodiumToad18>	ok, I can reproduce on latest 9.5 stable
T 1568305608 18<G3nka118>	Should I file a bug RhodiumToad ?
T 1568305623 18<RhodiumToad18>	yes
T 1568305672 18<RhodiumToad18>	it doesn't seem to fail on 11.
T 1568305749 18<G3nka118>	RhodiumToad,  plpgsql immutable as $$ begin raise info 'log: %', $1; return true , likely this is the root cause combined with RLS?
T 1568305779 18<G3nka118>	or is it any policy?
T 1568307216 20*	Disconnected (20)
T 1568307242 19*	Now talking on 22#postgresql
T 1568307242 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568307242 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Thu Sep 12 16:46:28 2019)
T 1568307242 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568307400 18<G3nka118>	filed thanks RhodiumToad
T 1568308069 18<admin12318>	anyone know how to speed this up? https://explain.depesz.com/s/fDSM
T 1568308090 18<admin12318>	it's an event table with a start column of type date. I already have an index on date
T 1568308097 18<xocolatl18>	it's 2ms ...
T 1568308168 18<depesz18>	admin123: why do you want to speed it up?
T 1568308208 18<admin12318>	xocolatl: you are pointing out something I did not consider haha
T 1568308255 18<admin12318>	nvm sql is taking 41ms but the request as a whole takes 330ms. The speed up should be higher up the stack, probably in the serialization layer.
T 1568308273 18<admin12318>	I want my whole response to be under 100ms
T 1568308336 18<ilmari18>	make sure you're not doing multiple SQL queries that could be a single one with some joines
T 1568308339 18<ilmari18>	*joins
T 1568308406 18<admin12318>	this django debug toolbar says 29 queries in 22ms, request time 800ms. The slowness must be after the sql.
T 1568308532 18<peerce18>	how much data are you reading ?
T 1568308645 18<admin12318>	peerce: in the db or the response size?
T 1568309211 18<peerce18>	the response size
T 1568309223 18<davidfetter_work18>	hi
T 1568309306 18<davidfetter_work18>	I have a completely static DB consisting of one table of sha1 hashes of leaked passwords.  Would it make sense to make its PK index "hot" with pg_prewarm or similar tricks?
T 1568309318 18<davidfetter_work18>	(index is ~21GB, if that matters)
T 1568309449 18<RhodiumToad18>	depends what kinds of lookup rates you want to handle
T 1568309467 18<RhodiumToad18>	I would generally say not
T 1568309484 18<davidfetter_work18>	pretty high
T 1568309511 18<RhodiumToad18>	tens/sec? hundreds/sec? thousands/sec?
T 1568309529 18<davidfetter_work18>	probably hundreds per second at peak
T 1568309542 18<RhodiumToad18>	probably not worth bothering with then.
T 1568309549 18<davidfetter_work18>	OK
T 1568309590 18<davidfetter_work18>	I'd noticed that lookups can take from sub-ms (repeat of a very recent lookup) up through mid-hundreds of ms. I'd like to get my variance down, even if the minimum time goes up
T 1568309611 18<RhodiumToad18>	mid-hundreds of ms is surprising unless you have slow i/o.
T 1568309614 18<admin12318>	peerce: 46KB I think
T 1568309636 18<RhodiumToad18>	try explain (analyze,buffers) to see how many buffers are being read
T 1568309644 18<RhodiumToad18>	also track_io_timing
T 1568309728 18<davidfetter_work18>	It's on EC2 with gp2 storage. I suppose I could buy more IOPS if that was actually going to give a good chance of helping. This is more about latency than throughput, obvs
T 1568309744 18<RhodiumToad18>	ah.
T 1568309750 18<RhodiumToad18>	how much ram on the instance?
T 1568309763 18*	davidfetter_work checks
T 1568309808 18<davidfetter_work18>	dfetter@shadow01-db-new-admin-01:~$ cat /proc/meminfo
T 1568309808 18<davidfetter_work18>	MemTotal:       32939544 kB
T 1568309808 18<davidfetter_work18>	MemFree:          261456 kB
T 1568309808 18<davidfetter_work18>	MemAvailable:   28506968 kB
T 1568309834 18<davidfetter_work18>	I could put it on its own instance in cases the other DBs are being noisy neighbors
T 1568309840 18<davidfetter_work18>	case*
T 1568309874 18<RhodiumToad18>	what you really want I guess is to prewarm only the interior of the index and not the leaves
T 1568309890 18<RhodiumToad18>	not sure how you could do that
T 1568309896 18<davidfetter_work18>	I was about to ask
T 1568309931 18<davidfetter_work18>	is there some way to find out where the interior is and span out, say, to the first set of branches from there?
T 1568309940 18<davidfetter_work18>	er, where the root is
T 1568310050 18<RhodiumToad18>	yes
T 1568310064 18<RhodiumToad18>	a bit of work with pageinspect
T 1568310166 18<davidfetter_work18>	oh, and I noticed when I made a hash index on the same column, it was getting chosen preferentially, even though I was getting numbers more like 5ms than the usual 3ms I was getting with the b-tree
T 1568310182 18<RhodiumToad18>	the hash index was likely smaller?
T 1568310190 18<davidfetter_work18>	yeah, about 15GB
T 1568310213 18<RhodiumToad18>	that might give you more consistent lookup times
T 1568310389 18*	davidfetter_work rebuilds the hash index, having dropped it earlier
T 1568310444 18<brainicism18>	i'm trying to perform a PITR recovery, with a base backup and some archived WAL files. I'm getting a 'invalid checkpoint record' when trying to start the server when relying on the `recovery.conf`, but don't receive that error when copying my WAL files directly to pg_wal/. anyone have any clues on how to debug this?
T 1568310633 18<RhodiumToad18>	brainicism: does the base backup contain a backup_label file?
T 1568310943 18<brainicism18>	RhodiumToadyes
T 1568311016 18<peerce18>	is there a valid restore_command in the recovery.conf ?
T 1568311040 18<brainicism18>	yes
T 1568311047 18<brainicism18>	and its under the directory $PGDATA
T 1568311904 18<RhodiumToad18>	brainicism: what's the content of the backup_label, and what were the actual log messages from starting the server with an empty pg_wal dir?
T 1568312054 18<brainicism18>	```
T 1568312059 18<brainicism18>	Uploaded file: https://uploads.kiwiirc.com/files/f53dc51bc4be37f94bd166ad9cbf6624/pasted.txt
T 1568312082 18<brainicism18>	Uploaded file: https://uploads.kiwiirc.com/files/ff71275b6ef59929a17c13f22de5bd1f/pasted.txt
T 1568312187 18<RhodiumToad18>	you're sure you put the recovery.conf in the right place?
T 1568312204 18<RhodiumToad18>	it has to go in the data dir, NOT in the dir with the other conf files if those are elsewhere
T 1568312224 18<brainicism18>	in $PGDATA right?
T 1568312253 18<brainicism18>	its located at $PGDATA/recovery.conf
T 1568312280 18<RhodiumToad18>	yeah
T 1568312287 18<brainicism18>	postgres@MININT-KKCUCBK:~/10/main$ cat $PGDATA/recovery.conf
T 1568312303 18<RhodiumToad18>	wait, what OS?
T 1568312308 18<brainicism18>	ubuntu
T 1568312315 18<RhodiumToad18>	and what is the actual value of $PGDATA?
T 1568312342 18<brainicism18>	 /home/pg_data
T 1568312351 18<RhodiumToad18>	the recovery.conf should be in /var/lib/postgresql/10/main  according to those logs
T 1568312386 18<brainicism18>	which log are you looking at?
T 1568312394 18<RhodiumToad18>	the one you just pasted above
T 1568312397 18<peerce18>	are you running a hand compiled version of postgres or something?  /home/pg_data isn't any sort of normal debian/ubuntu data directory
T 1568312406 18<RhodiumToad18>	Error: /usr/lib/postgresql/10/bin/pg_ctl /usr/lib/postgresql/10/bin/pg_ctl start -D /var/lib/postgresql/10/main -l /var/log/postgresql/postgresql-10-main.log -s -o  -c config_file="/etc/postgresql/10/main/postgresql.conf"  exited with status 1:
T 1568312408 18<brainicism18>	i made the directory
T 1568312411 18<RhodiumToad18>	see ^^ that?
T 1568312429 18<xocolatl18>	postgres's $HOME is /usr/lib/postgresql
T 1568312437 18<brainicism18>	oh oops
T 1568312443 18<RhodiumToad18>	irrelevant
T 1568312453 18<xocolatl18>	I mean /var/lib/postgresql
T 1568312454 18<peerce18>	what counts is the -D /var/lib/postgresql/10/main
T 1568312461 18<RhodiumToad18>	brainicism: you created the directory, but where did you actually restore the base backup to?
T 1568312473 18<brainicism18>	 /var/lib/postgresql/10/main
T 1568312493 18<RhodiumToad18>	ok. so does /var/lib/postgresql/10/main/recovery.conf  exist
T 1568312501 18<brainicism18>	no, should it be there?
T 1568312521 18<brainicism18>	i was under the impression it should be wherever $PGDATA pointed to
T 1568312534 18<peerce18>	its wherever the postgres SERVER's $PGDATA is pointed to.
T 1568312537 18<peerce18>	not your user account
T 1568312551 18<brainicism18>	and thats at  /var/lib/postgresql/10/main/ ?
T 1568312555 18<RhodiumToad18>	$PGDATA is only the default value for the -D option is. Explicitly specifying -D when starting the server overrides that.
T 1568312559 18<brainicism18>	ah
T 1568312563 18<brainicism18>	got it
T 1568312600 18<RhodiumToad18>	also, if data_directory is specified in postgresql.conf, then that overrides even -D
T 1568312633 18<RhodiumToad18>	when we talk about PGDATA, we mean whichever the final value of data_directory is
T 1568312651 18<brainicism18>	it worked!
T 1568312657 18<brainicism18>	thanks guys :D
T 1568312680 18<brainicism18>	i didnt know about the -D
T 1568313436 18<regedit18>	hello 
T 1568313506 18<regedit18>	how do people usually do human name searching in with postgresql? closest match, a bit of fuzzy matching, ranked results based on how close the results match etc... or is this rarely done with postgresql?
T 1568313616 18<RhodiumToad18>	probably not done any more or any less often than with other databases
T 1568313648 18<regedit18>	okk
T 1568313650 18<RhodiumToad18>	there are modules for soundex, metaphone, and trigram similarity
T 1568313679 18<xocolatl18>	I wish soundex actually worked
T 1568313684 18<regedit18>	ok lemme look those up
T 1568313687 18<RhodiumToad18>	also levenshtein distance
T 1568351749 20*	Disconnected (20)
T 1568351773 19*	Now talking on 22#postgresql
T 1568351773 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568351773 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Thu Sep 12 16:46:28 2019)
T 1568351773 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568351987 18<RhodiumToad18>	p321: what's the output of  select txid_current_snapshot();
T 1568352054 18<p32118>	RhodiumToad, my Windows got rebooted to install Windows update. In the worst time possible... I am back now. Probably miss some of your posts. Can you please repost?
T 1568352079 18<RhodiumToad18>	I just did
T 1568352086 18<RhodiumToad18>	that's the only thing you missed
T 1568352135 18<p32118>	RhodiumToad: Thanks. It returns: 1042361:1042361:
T 1568352181 18<RhodiumToad18>	ok. and what about  show vacuum_defer_cleanup_age;
T 1568352200 18<p32118>	RhodiumToad: 0
T 1568352230 18<RhodiumToad18>	and if you do  vacuum verbose me.test8;  what is the output?
T 1568352285 18<p32118>	RhodiumToad: I executed I am waiting. Probably gonna take some time. It looks to me 188 million rows are still there. I will post when it is finished.
T 1568352299 18<RhodiumToad18>	using a paste site please
T 1568352592 18<p32118>	RhodiumToad: https://pastebin.com/MBzk7R8L
T 1568352637 18<RhodiumToad18>	ok, so it worked that time
T 1568352652 18<RhodiumToad18>	most likely there was still some other transaction open when you tried it before
T 1568352653 18<p32118>	RhodiumToad: Now repeating. SELECT n_dead_tup from pg_stat_user_tables where relname = 'test8'; and I get 0 which if fine.
T 1568352714 18<p32118>	RhodiumToad: Thanks for help. Really appreciate it.
T 1568355363 18<maxter18>	can postgres use more than 1 CPU for a query?
T 1568355417 18<incognito18>	maxter: yes, indeed it depends on the query
T 1568355442 18<incognito18>	see max_parallel_workers parameter
T 1568579907 20*	Disconnected (20)
T 1568579933 19*	Now talking on 22#postgresql
T 1568579933 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568579933 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Thu Sep 12 16:46:28 2019)
T 1568579933 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568579969 18<bencc18>	xocolatl: I'm missing from clause but it seems to work
T 1568579986 18<bencc18>	I usually try to understand the query before trying because if it works it might be misleading
T 1568585958 20*	Disconnected (20)
T 1568585985 19*	Now talking on 22#postgresql
T 1568585985 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568585985 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Thu Sep 12 16:46:28 2019)
T 1568585985 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568588237 18<s3a18>	Hello, everyone. :) I'm using Debian GNU/Linux, and when I try to run this PHP file ( http://dpaste.com/2TD8SQM#wrap ) (which tries to connect to a Postgresql database), I get the error "Fatal error: Uncaught Error: Call to undefined function pg_connect() in /home/user/NetBeansProjects/MyPhpProject/src/index.php:13 Stack trace: #0 {main} thrown in /home/user/NetBeansProjects/MyPhpProject/src/index.php on line 13". I have installed the php-pgsql,
T 1568588237 18<s3a18>	php7.3-pgsql and php-mdb2-driver-pgsql packages. I have restarted the apache server (and I even rebooted, just in case). I also uncommented the extension=php_pdo_pgsql.dll and extension=php_pgsql.dll lines in the /etc/php/7.0/apache2/php.ini file (but it feels odd to me that they're dlls in a GNU/Linux environment). I also added the line "host all myusername 0.0.0.0/0 md5 #I added this line." in the /etc/postgresql/11/main/pg_hba.conf file (and the
T 1568588238 18<s3a18>	total file is http://dpaste.com/0WJFJ1K#wrap ). So, could someone please help me get my PHP file to successfully connect to my Postgresql database?
T 1568617508 20*	Disconnected (20)
T 1568617535 19*	Now talking on 22#postgresql
T 1568617535 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568617535 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Thu Sep 12 16:46:28 2019)
T 1568617535 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568617537 18<depesz18>	based on the transaction status?
T 1568617542 18<RhodiumToad18>	or anything
T 1568617546 18<depesz18>	yeah. but I'm not sure you can.
T 1568617548 18<p32118>	depesz: Excellent. Thanks a lot.
T 1568617559 18<RhodiumToad18>	you can't as it stands
T 1568617621 18<RhodiumToad18>	maybe we could add a %?x{*=somestring:!=otherstring:...}   construct
T 1568617728 18<depesz18>	it would be enough if you could %`whatever-command %x`
T 1568617733 18<depesz18>	but apparently you can't.
T 1568617895 18<dars18>	`/usr/bin/pg_dumpall --file "/home/darshan/xyz" --host "<hostname here>" --port "5432" --username "paxcom" --password "passwordhere" "postgres" --verbose --role "paxcom" `
T 1568617905 18<dars18>	Is this command right?
T 1568617922 18<dars18>	I need to backup my database
T 1568618034 18<p32118>	depesz: and RhodiumToad: I have not incorporated my existing prompt with red "*" it is: \set PROMPT1 '%n@%M:%>_%/=#%[%033[1;31m%]%x%[%033[0m%]% '      Thanks for help. Really appreciate it.
T 1568618079 18<dars18>	Please help me wth above cmd
T 1568618082 18<RhodiumToad18>	dars: no, you can't put the password on the command line
T 1568618101 18<dars18>	RhodiumToad Then how to send password
T 1568618112 18<RhodiumToad18>	you'll be prompted for it
T 1568618118 18<RhodiumToad18>	possibly several times
T 1568618137 18<RhodiumToad18>	you can put it in a PGPASSWORD environment variable or in a .pgpass file
T 1568618163 18<RhodiumToad18>	you probably shouldn't use the --role parameter
T 1568618195 18<RhodiumToad18>	the rest is ok, but it's probably better to use pg_dumpall -g and a separate pg_dump -Fc or -Fd for each database
T 1568618840 18<tangara18>	 ID int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
T 1568618851 18<tangara18>	i got error.  Please help
T 1568618856 18<RhodiumToad18>	what error
T 1568618862 18<tangara18>	even I use Integer also wrong
T 1568618916 18<tangara18>	yntax error at or near "int"LINE 2:     ID int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,               ^SQL state: 42601
T 1568619033 18<RhodiumToad18>	that sounds like the error is on the previous line?
T 1568619068 18<RhodiumToad18>	what pg version?
T 1568619519 18<tangara18>	11
T 1568619560 18<tangara18>	it cannot be previous line cos this is the first definition of the field ...the first line is basically create table...
T 1568619575 18<tangara18>	the rest of the fields are all ok so I do not know why it is giving me problem
T 1568619597 18<RhodiumToad18>	what is the first line _exactly_
T 1568619612 18<tangara18>	or should I try using psql ..so it is the same way to do the create table right ..just plonk the whole sql table onto psql
T 1568619641 18<tangara18>	INSERT INTO public.oldmembers (    ID int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
T 1568619655 18<RhodiumToad18>	that's not a CREATE TABLE
T 1568619669 18<tangara18>	I am using INSERT because I have to create the table manually via PGAdmin 4
T 1568619678 18<RhodiumToad18>	you can't do that
T 1568619686 18<tangara18>	cos I can't use create table for strange reason
T 1568619692 18<RhodiumToad18>	why not?
T 1568619706 18<RhodiumToad18>	you certainly can't create tables with INSERT
T 1568619710 18<tangara18>	cos the other day when I have an important demo I desparately rebuild my database table on the 11th hour
T 1568619715 18<tangara18>	and it won't work
T 1568619754 18<tangara18>	so I did it by using pgAdmin4 to manually create a table name first before inserting all the fields using a SQL sCRIPT
T 1568619778 18<RhodiumToad18>	you're very confused
T 1568619790 18<RhodiumToad18>	you cannot create tables or columns with INSERT
T 1568619791 18<tangara18>	I still have that email with me
T 1568619828 18<tangara18>	i am not sure what is wrong with the system cos I am a copy of the table in postgres and then in postgres I have a membership database
T 1568619841 18<RhodiumToad18>	you cannot create tables or columns with INSERT
T 1568619842 18<tangara18>	so somehow it won't allow me to enter the same table
T 1568619853 18<tangara18>	i forgot how the same table was created
T 1568619879 18<tangara18>	@RhodiumToad I merely enter the fields name
T 1568619884 18<tangara18>	not create the Table
T 1568619895 18<tangara18>	cos it won't allow me to create table direct
T 1568619900 18<RhodiumToad18>	of course it will
T 1568619950 18<tangara18>	that's the strange reason....and nobody is able to resolve the problem for me and I can't keep on spending all the hours whole day a few days try to crack this
T 1568620002 18<tangara18>	anyway, now I just want to quickly insert all the table columns and be dond with it.
T 1568620006 18<RhodiumToad18>	you cannot create tables or columns with INSERT
T 1568620011 18<tangara18>	can I use that synaxt
T 1568620014 18<RhodiumToad18>	no!
T 1568620018 18<RhodiumToad18>	ffs
T 1568620020 18<tangara18>	the table is created already RhodiumToad
T 1568620027 18<RhodiumToad18>	with what columns?
T 1568620040 18<tangara18>	now only left with inserting the columns name
T 1568620052 18<RhodiumToad18>	you do not "insert" columns
T 1568620060 18<RhodiumToad18>	what columns does the table have?
T 1568620077 18<tangara18>	ok. so i have to use edit table and follows by add column name is that right ?
T 1568620084 18<RhodiumToad18>	no
T 1568620092 18<tangara18>	alter table
T 1568620097 18<RhodiumToad18>	yes
T 1568620132 18<RhodiumToad18>	alter table public.oldmembers add column id integer generated by default as identity primary key;   will work as long as the table doesn't already have a column of that name
T 1568620274 18<tangara18>	tks. done.
T 1568620307 18<RhodiumToad18>	now, why do you think that you can't use create table?
T 1568620350 18<tangara18>	now  I have another problem - I have altered an old table from generated Id always to generated by Default but then it won't allow me to insert the id via web but another table havning the same setting has no such problem
T 1568620411 18<tangara18>	to your question, I am not sure...cos I have the same table name in postgres which i have created via psql ...yeah...now I can really access psql
T 1568620416 18<RhodiumToad18>	"won't allow" tells us nothing, what actually happened when you tried?
T 1568620449 18<tangara18>	but maybe I didn't add in membership which is the database name...so it create another one in postgres which is the username
T 1568620476 18<tangara18>	and somehow I cam not able to create another same name table in memberhsip
T 1568620515 18<tangara18>	anyway, can you please advise me about the insertion thing cos no matter what I did, it just won't allow me to insert...
T 1568620519 18<RhodiumToad18>	you're still telling us nothing
T 1568620531 18<nbjoerg18>	tangara: precise error messages please
T 1568620539 18<RhodiumToad18>	never say "won't allow" or "not able" - tell us WHAT HAPPENED WHEN YOU TRIED
T 1568620557 18<RhodiumToad18>	give us the actual commands you used and the error message or other response
T 1568620560 18<tangara18>	give me 15min cos I havn't start my IDE
T 1568620745 18<tangara18>	sorry i need to ask this question first
T 1568620748 18<tangara18>	>psql -h localhost -p 5433 -U postgres copy oldmembers FROM 'd:\memberparticulars.csv' (FORMASV, HEADER)
T 1568620764 18<tangara18>	so now it gives me this error COPY doesnt exist
T 1568620786 18<tangara18>	psql: FATAL:  database "copy" does not exist
T 1568620796 18<tangara18>	i think i need to remove copy right ?
T 1568620804 18<incognito18>	tangara: typo error, see psql --help
T 1568620816 18<incognito18>	particularly -c option
T 1568620836 18<tangara18>	incognito should I remove copy in order to copy my d:\.. to oldmembers ?
T 1568620844 18<RhodiumToad18>	don't do it like that
T 1568620855 18<RhodiumToad18>	you'll have endless pain from the shell quoting
T 1568620913 18<RhodiumToad18>	if you're doing this manually and not from a script, then just do  psql -h localhost -p 5433 -U postgres -d yourdbname
T 1568620937 18<RhodiumToad18>	then AT THE PSQL PROMPT, enter the \copy oldmembers FROM 'd:\memberparticulars.csv' (FORMAT CSV, HEADER)    command
T 1568621136 18<tangara18>	ERROR:  null value in column "strremark" violates not-null constraint
T 1568621150 18<tangara18>	how do I allow copying of null value ?
T 1568621215 18<RhodiumToad18>	why did you create the column with NOT NULL if you want nulls in it?
T 1568621239 18<RhodiumToad18>	alter table oldmembers alter column strremark drop not null;
T 1568621263 18<tangara18>	erm... it is NOT Null
T 1568621279 18<tangara18>	it has always been Not null...even the old database
T 1568621285 18<tangara18>	using MYSQL
T 1568621288 18<tangara18>	I think
T 1568621310 18<tangara18>	erm...so i have to drop not null..I see
T 1568621317 18<tangara18>	stupid me..ha ha
T 1568621438 18<tangara18>	ERROR:  character with byte sequence 0x81 in encoding "WIN1252" has no equivalent in encoding "UTF8"
T 1568621451 18<tangara18>	i have non-English characters inside...
T 1568621464 18<tangara18>	how do I make it copy the characters as well ?
T 1568621494 18<tangara18>	i think let me alter everything to drop not null first
T 1568621536 18<RhodiumToad18>	if it was not null in the old database, why are there nulls in the file?
T 1568621554 18<RhodiumToad18>	is the file in UTF8?
T 1568621575 18<RhodiumToad18>	if so, make sure you do  set client_encoding = 'UTF8';  before the copy
T 1568621785 18<tangara18>	yap, i just did.
T 1568621800 18<tangara18>	but now i have another problem - missing data in a column
T 1568621821 18<tangara18>	so is it possible to copy with missing data ?
T 1568621832 18<RhodiumToad18>	where did this csv file come from? how was it created?
T 1568621840 18<tangara18>	from my webhost
T 1568621844 18<RhodiumToad18>	HOW
T 1568621854 18<tangara18>	i just downloaded it into csv file
T 1568621860 18<tangara18>	i also download one with sql format
T 1568621879 18<tangara18>	it is in maria db
T 1568621930 18<RhodiumToad18>	I have no idea if mariadb is capable of generating valid CSV files
T 1568621939 18<RhodiumToad18>	what is the exact error you got?
T 1568621955 18<tangara18>	it just mentioned a column name with no data
T 1568621962 18<RhodiumToad18>	what is the exact error you got?
T 1568621977 18<tangara18>	ERROR:  missing data for column "strnricno"CONTEXT:  COPY oldmembers, line 3073: ""
T 1568622007 18<RhodiumToad18>	how many lines are in the file?
T 1568622016 18<tangara18>	3000 plus
T 1568622027 18<RhodiumToad18>	exactly how many lines are in the file
T 1568622034 18<RhodiumToad18>	specifically, is line 3073 the last line
T 1568622074 18<tangara18>	yap
T 1568622086 18<RhodiumToad18>	and what column number is column "strnricno" ?
T 1568622151 18<tangara18>	it is after id so 2nd column
T 1568622210 18<RhodiumToad18>	is line 3073 empty or does it contain anything?
T 1568622232 18<tangara18>	i think it is empty
T 1568622244 18<tangara18>	let me double check
T 1568622331 18<tangara18>	3072 is the last line
T 1568622380 18<RhodiumToad18>	how does the last line end?
T 1568622476 18<tangara18>	prettey much the same as other lines
T 1568622493 18<RhodiumToad18>	what is the last character on the last line
T 1568622505 18<RhodiumToad18>	learn to be PRECISE in answering questions
T 1568622512 18<tangara18>	"
T 1568622530 18<tangara18>	for every field with data it will be enclosed with " "
T 1568622539 18<tangara18>	so the last column it is the same "xxxx"
T 1568622550 18<RhodiumToad18>	does the sequence "" ever appear in the middle of a field in your data?
T 1568622556 18<RhodiumToad18>	or just " on its own?
T 1568622560 18<tangara18>	so do i write a script to remove all the empty "" ?
T 1568622563 18<RhodiumToad18>	NO
T 1568622566 18<RhodiumToad18>	ffs
T 1568622569 18<tangara18>	it is always " xxx "
T 1568622591 18<tangara18>	for every column including the last column at last row
T 1568622612 18<RhodiumToad18>	are there any " characters that are NOT the start or end of a field
T 1568622664 18<tangara18>	i am not sure...i need to use find ..hang on
T 1568622787 18<tangara18>	there are a toal of 228 ""
T 1568622796 18<tangara18>	as in no data inside
T 1568622807 18<RhodiumToad18>	but I didn't ask that question, did I
T 1568622819 18<RhodiumToad18>	are there any " characters that are NOT the start or end of a field
T 1568622823 18<tangara18>	if you ask just how many alone
T 1568622829 18<tangara18>	i am not sure how to find it
T 1568622851 18<tangara18>	not at all
T 1568622862 18<tangara18>	all have " at the start and end of a field
T 1568622876 18<RhodiumToad18>	are there any " characters that are NOT the start or end of a field
T 1568622985 18<tangara18>	not at all
T 1568622994 18<tangara18>	i think
T 1568623004 18<tangara18>	how do I find out ?
T 1568623014 18<tangara18>	i have to import into Excel ?
T 1568623018 18<tangara18>	to see ?
T 1568623019 18<RhodiumToad18>	ghod, no
T 1568623020 18<tangara18>	or ?
T 1568623029 18<RhodiumToad18>	excel's csv handling is hopeless
T 1568623054 18<tangara18>	anyway my ms office is gone after my computer crash
T 1568623060 18<tangara18>	so i only have open source one
T 1568623107 18<RhodiumToad18>	make sure that there isn't a stray blank line at the end of the file.
T 1568623148 18<tangara18>	nope
T 1568623149 18<RhodiumToad18>	i.e. the newline (CR or CRLF) following the " on line 3072 should be the last one or two characters in the file
T 1568623188 18<RhodiumToad18>	if there's an extra newline, then this is exactly the error that you would get.
T 1568623278 18<tangara18>	ok. now what can i do to copy the empty data field ?
T 1568623454 18<tangara18>	or do I need to use PYTHON to do it ?
T 1568623472 18<dars18>	How to get count of all  the tables of my database?
T 1568623609 18<depesz18>	dars: select count(*) from pg_class where relkind = 'r'; ?
T 1568623621 18<tangara18>	i think i know the reason already
T 1568623650 18<tangara18>	cos there is indeed missing data before that missing data in strnricno
T 1568623667 18<tangara18>	i think i need to put in all the null value again before copying
T 1568623685 18<dars18>	Thank you depesz
T 1568623754 18<dars18>	depesz It is returning count = 0
T 1568623998 18<tangara18>	ERROR:  extra data after last expected column
T 1568624018 18<tangara18>	it seems it is impossible for postgresql to copy data from csv file ...
T 1568624032 18<tangara18>	i thought I will manually add in the missing column
T 1568624044 18<tangara18>	but when i added it then give me the above error
T 1568624055 18<tangara18>	so it is no ending of errors :(
T 1568624216 18<Myon18>	dars: that can't be true, it should return at least 70-ish catalog tables
T 1568624296 18<localhorse18>	when writing it like this, is NULL used as default value? `ALTER TABLE foobar ADD COLUMN baz TEXT NULL;` why doesn't it require the DEFAULT keyword?
T 1568624424 18<depesz18>	dars: i find it unlikely
T 1568624435 18<depesz18>	dars: show me screenshot of what you did.
T 1568624546 18<Myon18>	localhorse: NULL is just the opposite of a NOT NULL constraint
T 1568624546 18<depesz18>	localhorse: null/not null specifies whether null value is allowed.
T 1568624550 18<depesz18>	localhorse: also, it'
T 1568624562 18<depesz18>	s the default. so you don'
T 1568624567 18<depesz18>	t have to specify null
T 1568624573 18<depesz18>	how come i press enter too soon?
T 1568624594 18<dars18>	depesz: https://pasteboard.co/IxEopNn.png
T 1568624595 18<Myon18>	Gremlins
T 1568624599 18<localhorse18>	ah right
T 1568624608 18<Myon18>	dars: 'r'
T 1568624612 18<localhorse18>	i never write NULL, that's why it looked weird to me
T 1568624617 18<depesz18>	dars: why the hell did you put kinator whatever in the relkind ?
T 1568624626 18<depesz18>	dars: i typed: relkind = 'r'
T 1568624669 18<depesz18>	relkind is a kind (Type of) relation. 'r' means 'regular table'
T 1568624697 18<dars18>	I need to find the number of tables in my db
T 1568624704 18<depesz18>	then run the query i gave you
T 1568624759 18<tangara18>	anybody here can tell me what should i do to import my database :(
T 1568624788 18<RhodiumToad18>	we tried but you're not cooperating
T 1568624850 18<dars18>	Yeah it is working
T 1568624855 18<dars18>	Thank you
T 1568624890 18<dars18>	Now I want to know is their any way t pause pg_dump and continue later?
T 1568624907 18<depesz18>	darsi dont think so.
T 1568624914 18<depesz18>	dars: what is the real problem you're trying to solve?
T 1568624941 18<dars18>	I am creating bkup of a 840GB size database right now
T 1568624957 18<depesz18>	ok, and ?
T 1568625004 18<dars18>	How much time pg_dump_all is going to take for that?
T 1568625009 18<depesz18>	long
T 1568625028 18<dars18>	how long? approx?
T 1568625032 18<depesz18>	couple of notes: 1. do not use pg_dumpall. 2. when using pg_dump, use -Fd and -j
T 1568625040 18<depesz18>	can't approximate, i have no idea on speed of your system.
T 1568625067 18<dars18>	i3, with 8GB RAM
T 1568625107 18<depesz18>	the most imporatant factor is speed of hard disk. also. 850GB database on 8gb server? damn. even my laptop has more memory
T 1568625137 18<dars18>	that is config of my system not server
T 1568625143 18<dars18>	server is amazon rds
T 1568625217 18<depesz18>	still. there is no real way to estimate it, for me. you can dump a table that is ~ 2G, and multiply the time by 420.
T 1568625381 18<eofs18>	dars: Use RDS's Snapshots to backup things. If you need to access the data as well then you might consider using https://aws.amazon.com/dms/
T 1568625481 18<tangara18>	meaning RhodiumToad
T 1568625537 18<tangara18>	i already answered your last question that there isn't a line or black line at the end
T 1568625565 18<tangara18>	and after that there is no reply from you
T 1568625577 18<tangara18>	and so I thought you don't have the answer or solution
T 1568625582 18<Myon18>	tangara: the problem is that you aren't answering questions in a useful way
T 1568625626 18<tangara18>	but there isn't a black line and how should I answer if you can't accept this answer ?
T 1568625627 18<Myon18>	and your problem seems to be shifting around a lot
T 1568625653 18<tangara18>	yes. at first. but after that it is on that question only
T 1568625664 18<tangara18>	and all the way on that question only
T 1568625668 18<Myon18>	so what is the precise problem now?
T 1568625680 18<tangara18>	so, based on my last reply, what should i do next ?
T 1568625691 18<Myon18>	ERROR:  extra data after last expected column
T 1568625702 18<RhodiumToad18>	bah. power issue here. I'll be back in a bit.
T 1568625708 18<Myon18>	how many columns does the table have?
T 1568625718 18<tangara18>	i waited for so long and there is no continuation so naturally i thought RhodiumToad is also trying to think what could be the way to solve it
T 1568625737 18<depesz18>	tangara: please answer Myons question
T 1568625748 18<Myon18>	RhodiumToad is really the most patient and helpful person in here
T 1568625758 18<tangara18>	oh no. we should continue at the point which Rhodium Toad asked me tht question and then I answered it from there
T 1568625771 18<Myon18>	if you manage to upset him, that means you are doing *much* worse than anyone else in here seeking advise
T 1568625773 18<tangara18>	yes. I agreed. So are many of othere people hre and out there
T 1568625798 18<depesz18>	tangara: do you want help, or do you want to complain about not getting help and/or RhodiumToad ?
T 1568625802 18<Myon18>	tangara: so. I asked you something, and all I get in return is unhelpful babbling
T 1568625804 18<tangara18>	that's why i will / must keep him and others who have helped me in my buddhism prayers
T 1568625821 18<depesz18>	if you want help, please answer question from myon. if you want to complain - well, continue. but please be advised that it's not going to help you in the long run.
T 1568625858 18<tangara18>	@depesz you are not being helpful.  We must continue where Rhodium last question and my reply from there.
T 1568625870 18<depesz18>	tangara: ok. have a nice life.
T 1568625874 18<tangara18>	not after what I have said because there is no link
T 1568625882 18<Myon18>	I'll stop here, sorry
T 1568625932 18<tangara18>	cos the whole thing or the discussion started with me answering all the questions posted by Rhodium Toad till there is no further reply and then of course i tried things out myself right
T 1568625944 18<tangara18>	so everything after that doesn't count
T 1568625955 18<tangara18>	it has to continue from the last line I replied him...
T 1568625977 18<tangara18>	because I do not know what is it he wanted me to do next to resolve this copying problem
T 1568626014 18<adsf18>	would a window function sound about right if im after an average count of rows by a group by?
T 1568626054 18<incognito18>	depesz: me, i want to complain : we don't get the right question in order to give the right answer ^^
T 1568626069 18<Myon18>	adsf: you can combine group by and window functions, but likely you just want select avg(foo) from bar group by moo;
T 1568626079 18<depesz18>	adsf: do you just want the average count?
T 1568626096 18<depesz18>	adsf: if yes, then: select avg(count) from (Select count(*) from table group by whatever) x;
T 1568626102 18<tangara18>	not sure why am i getting attacks like that
T 1568626108 18<adsf18>	i have an aggregate mat view where i need the avg count of rows by a group
T 1568626123 18<depesz18>	adsf: select avg(count_column) from your_matview;
T 1568626133 18<tangara18>	maybe people don't want me to be here to ask question that is difficult to get an answer or No answer for my problem
T 1568626137 18<adsf18>	oh i mean this is the query making the mat view :)
T 1568626148 18<Myon18>	tangara: you are talking so much it's hard to see what your problem is, so keep it to the facts
T 1568626149 18<tangara18>	i think there is no solutions hence the ATTACKS !!!
T 1568626159 18<depesz18>	adsf: sorry, not sure what you mean.
T 1568626160 18<adsf18>	or are you saying just do count, then just use avg on it
T 1568626178 18<adsf18>	depesz: your first answer makes sense to me. with the sub query
T 1568626200 18<tangara18>	@Myon I have stated there was no reply since my last reply to Rhodium Toad hence I went on to try things out.
T 1568626221 18<tangara18>	and then I was told that I was not being Cooperative....
T 1568626228 18<tangara18>	why has it ended like that ?
T 1568626234 18<adsf18>	Myon: i think the issue with that is its not a numeric value, its an id.
T 1568626238 18<Myon18>	now cooperate in waiting for him please
T 1568626274 18<tangara18>	i think I have to be blunt : Even though Rhodiim Toad is VERY FRIENDLY and HELPFUL.  This problem of mine is nothing BUT DIFFICULT
T 1568626295 18<tangara18>	@Myon I can accept  a reply like yours now
T 1568626313 18<tangara18>	BUT I THINK I am not being Uncorporative !
T 1568626320 18<tangara18>	I am putting the FACTS right.
T 1568626326 18<incognito18>	<tangara> why has it ended like that ? <= because someone gave you the answer and you didn't try it, instead you went to another workaround not working
T 1568626342 18<tangara18>	that Rhodium Toad also don't have an answer which is fine cos nobody in this world knows everything
T 1568626348 18<tangara18>	and he is good in what he knows
T 1568626358 18<tangara18>	maybe my problem is too BEGINNER
T 1568626375 18<tangara18>	And he only deals with EXPERIENCED developer problem
T 1568626391 18<tangara18>	so Beginner one he just don't have an answer which is fine
T 1568626405 18<tangara18>	but don't say that I am being Uncoporative
T 1568626448 18<tangara18>	ok. Today is just too much. I am tired.  I have so many problems to deal with and struggling on my own because that's what most people want that happening to me.. So many tragic in my life already.
T 1568626460 18<incognito18>	tangara: to be cooperative : you only need to check if the answer resolve your problem; and give us log if not
T 1568626487 18<adsf18>	hrmm is it risky to make a mat view from a mat view?
T 1568626490 18<tangara18>	people just wanted me not to be successful in this developer things and jobs and so everywhere people are being so HOSTILE
T 1568626496 18<tangara18>	Too many tragic things already.
T 1568626499 18<tangara18>	bye.
T 1568626519 18<RhodiumToad18>	yeesh
T 1568626527 18<adsf18>	well that was melodramatic
T 1568626527 18<cartan18>	is it plugged in now?
T 1568626557 18<Myon18>	RhodiumToad: http://paste.debian.net/hidden/4d8c688e/ (but maybe better ignore it)
T 1568629287 18<Moonsilence18>	Hi! Is there a way to identify the associated session pid of a given temporary table?
T 1568629863 18<depesz18>	Moonsilence: afaik no.
T 1568629910 18<Moonsilence18>	Thanks depesz! Havent seen you around here lately :)
T 1568629927 18<depesz18>	i'm virtually always here, not always talking, though :)
T 1568629969 18<Moonsilence18>	then I meant 'talking'
T 1568629989 18<depesz18>	:). and i'm off again. bbl.
T 1568632043 18<bikeshedder18>	Does anyone know a good way to implement a sort function that sorts strings like that: ['1', '1.2.5', '1.3', '2.20', '20.1']. Maybe there is some builtin collation which supports that? I expect this to be a quite common requirement so I wonder if I just overlooked it or if it really requires a custom key function.
T 1568632158 18<bikeshedder18>	Aw. That example was terrible. I just realized that the alphanumeric sort already does that. I should have added a '3' and '20' in the list. e.g.: ['1', '1.2.5', '1.3', '2.20', '3', '20.1']. If I sort that alphanumerical I'll get '20.1' before '3'. I know how to solve that in code but I wonder how this would be applied to a postgresql index
T 1568632241 18<mbecroft18>	I have a PL/pgSQL function that returns type text. Internally, it has a dynamic "EXECUTE ... INTO" a record variable. I then return a field of the record, casted to the correct return type as follows: "RETURN rec.val::text;"
T 1568632360 18<mbecroft18>	Unfortunately, the type of the "rec" variable is determined and cached the first time the function is called, so on subsequent invocations when the EXECUTE results in a different type, it fails with "ERROR:  type of parameter ... does not match that when preparing the plan ... CONTEXT:  ... at RETURN"
T 1568632415 18<mbecroft18>	Per following reference, this can be worked around using EXECUTE (see para starting with "The mutable nature of record variables...":
T 1568632418 18<mbecroft18>	https://www.postgresql.org/docs/9.6/plpgsql-implementation.html
T 1568632471 18<mbecroft18>	But since I can't EXECUTE the actual RETURN statement itself, the advice given there (use EXECUTE) doesn't seem to help. Any ideas?
T 1568632516 18<mbecroft18>	The error message indicates it is in the RETURN statement itself that the type mismatch occurs
T 1568632544 18<mbecroft18>	As far as I know there is no way to dynamically execute the return statement itself!
T 1568632563 18<mbecroft18>	Or more to the point, its expression argument
T 1568632571 18<mbecroft18>	Help!?
T 1568632699 18<saper18>	how different are different types of "rec"?
T 1568632811 18<mbecroft18>	saper: They might be anything at all, but currently I am testing with text and numeric
T 1568632882 18<mbecroft18>	It appears that "EXECUTE ... INTO" a record variable works OK as no error is thrown at that point - the record variable's type is changing dynamically at each function invocation
T 1568632898 18<mbecroft18>	It complains when I try to RETURN a column of the record...
T 1568632911 18<harks18>	bikeshedder: I'm not entirely sure, if I understand your requirements correctly, but I think you just want to sort by regexp_split_to_array(version_number, '\.')::bigint[]
T 1568633248 18<mbecroft18>	saper: I am just writing a minimal test case to demonstrate the issue
T 1568633448 18<saper18>	mbecroft: would casting it to text in the dynamic query work?
T 1568633468 18<DuckyDev18>	Hi guys, I've this simple query ( https://pastebin.com/K23e18cW  ) where a.aid and ar.aid are returned, but is there any easy way to return everything from the two tables EXCEPT ar.aid?
T 1568633498 18<mbecroft18>	saper: good point. Looks like I am not - this may fix it!
T 1568633527 18<bikeshedder18>	harks, I think the sorting I'm looking for is called "numerical sorting". The most prominent example that comes to mind is windows filename sorting. "10.png" comes after "2.png" And "10-10.png" comes after "10-2.png".
T 1568633573 18<mbecroft18>	saper: Yay! Obvious in retrospect...
T 1568633580 18<mbecroft18>	All working now :D
T 1568633598 18<mbecroft18>	Thank you very much for pointing out the obvious
T 1568633631 18<saper18>	mbecroft: I think your function is too generic to work well, but here it is
T 1568633688 18<mbecroft18>	saper: It is a little ungainly, but necessary and the least bad way to achieve what's required...
T 1568633726 18<mbecroft18>	I had what seemed like a separate issue as well, but it may have just been a different manifestation of this. Will see in a moment...
T 1568633777 18<mbecroft18>	No...all fixed :D
T 1568633782 18<mbecroft18>	Whew!
T 1568633815 18<mbecroft18>	Thank you again saper! Sometimes the obvious needs to be pointed out.
T 1568633893 18<saper18>	Another happy customer^G
T 1568633930 18<harks18>	bikeshedder Never heard of it. Doesn't seem very useful to me. Maybe it's just regexp_split_to_array(version_number, '\D+')::bigint[] ? Or should strings be sorted as well? If you understand your semantics, just build an immutable function.
T 1568633938 18<bikeshedder18>	harks, https://imgur.com/501SymI ... that's how lots of users expect sorting to work.
T 1568641511 20*	Disconnected (20)
T 1568641539 19*	Now talking on 22#postgresql
T 1568641539 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568641539 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Thu Sep 12 16:46:28 2019)
T 1568641540 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568641593 18<Myon18>	it'll Just Work, yes
T 1568641601 18<mia18>	thank you!
T 1568643074 18<slax0r18>	hello, is it possible to copy database data from one database to the other? I'm trying with FDW, but it doesn't allow me to add a localhost host, so I need to add the external IP and it's slow as hell.. I thought of `with template` but I'm copying a production database to a test database, so I can't really disconnect clients, dump and restore from the server is not possible, since the DB is running as
T 1568643080 18<slax0r18>	an Azure service
T 1568643095 18<slax0r18>	is there any other way to copy and not having it go REALLY slowly?
T 1568643148 18<Myon18>	pg_dump -h db1 | psql -h db2
T 1568643188 18<Myon18>	you can parallelize both parts, but then you need to store the result on disk (pg_dump -h db1 -Fd -j; pg_restore -h db2 -j)
T 1568643201 18<krychu18>	How can I insert a new row only if the last recent one is older than X? The requirement is that this works concurrently. I think I cannot achieve it with SELECT FOR UPDATE, and INSERT ON CONFLICT. The only way I can think of is to use transaction with table lock, but perhaps there is a better way.
T 1568643256 18<slax0r18>	host is the same, so I'm guessing `pg_dump -h db -d dbname1 | psql -h db -d dbname2` along with auth stuff?
T 1568643270 18<Myon18>	yes
T 1568643294 18<mobidrop18>	you can only do it with locking because there's time between getting the "last recent one" and inserting a new one
T 1568643300 18<slax0r18>	I could try this, but doubt it will be any quicker since I'm connected over vpn
T 1568643301 18<Myon18>	krychu: SERIALIZABLE isolation should be able to do that
T 1568643305 18<chris6418>	hello
T 1568643363 18<krychu18>	Myon checking this out now
T 1568643383 18<slax0r18>	oh, pg_dump | psql will flip out on conflicts, right? :/
T 1568643413 18<Myon18>	it will ignore errors by default
T 1568643472 18<slax0r18>	ok, thanks for your help!
T 1568643598 18<krychu18>	Myon serializable 8-) is the thing, awesome stuff
T 1568643599 18<krychu18>	thnx
T 1568643605 18<Myon18>	aye
T 1568643616 18<Bish18>	if you were about to design a datastructure-model where every access, even read access should be monitored(and by whom it has been accessed), how would you do it?
T 1568643638 18<Bish18>	normal tables + audit logs (would you create a dbuser for every real user? to be able to use that in audit logs)
T 1568643647 18<Bish18>	event sourcing? it would be really happy about opinions
T 1568643727 18<doev18>	Can I reset a whole database server? I installed a fresh one, but reading in some dumbs going wrong. Can I start with a new Server, without reinstall?
T 1568643747 18<Myon18>	doev: you can just call initdb again (rm -rf first)
T 1568643794 18<doev18>	Myon thank you
T 1568644065 18<doev18>	with rm -rf ... you mean, the old datadir? /var/lib/postgresql/10/main in my case
T 1568644104 18<adsf18>	should i be concerned about locks on tables that can have fairly high volume of upserts concurrently? there are no for updates or any selects going on for that table.
T 1568644104 18<Myon18>	on Debian, do pg_dropcluster 10 main; pg_createcluster 10 main
T 1568644105 18<ilmari18>	doev: if you're using debian/ubuntu, use pg_dropcluster and pg_createcluster
T 1568644143 18<Myon18>	adsf: locks are normal, especially ACCESS SHARE locks
T 1568644165 18<Myon18>	that just means "please no one drop this table while I'm using it"
T 1568644168 18<adsf18>	Myon: okie dokie. It doesn't seem like it will be a huge issue.
T 1568644205 18<adsf18>	i have a few processes that write to the same table and usually they don't have to wait long if they overlap on a row
T 1568644244 18<Myon18>	that's why you should keep transactions short
T 1568644267 18<ilmari18>	https://www.postgresql.org/docs/current/explicit-locking.html shows the different lock levels, what takes them, and how they block eachother
T 1568644312 18<adsf18>	is there any locking for updaing a child table? (locking parent)
T 1568644334 18<adsf18>	ilmari: thanks, I had that up also :)
T 1568644410 18<krychu18>	Myon if a transaction is serializable is it guaranteed to behave as serial with other read committed transactions or only with other serializable transactions?
T 1568644540 18<Myon18>	the guarantee is for the transaction, if other transactions care less, they can see different results
T 1568644610 18<Myon18>	in your case, it won't prevent non-serializable transactions from violating this requirement
T 1568644645 18<Myon18>	you could probably install a trigger that enforces it by bailing out if not in serialzable mode
T 1568644664 18<Myon18>	but tbh I don't have much experience with that in practise
T 1568644676 18<krychu18>	ok, gonna give it a shot
T 1568644681 18<doev18>	ok, reset works fine. Next time I remember, that the config-files are reseted too :)
T 1568644685 18<krychu18>	need to run will chat later, thanks again
T 1568646291 18<enoq18>	when do you think about scaling postgres
T 1568646330 18<enoq18>	we've got a web shop that fetches some store information from a server that is hooked up to a postgres db
T 1568708580 20*	Disconnected (20)
T 1568708607 19*	Now talking on 22#postgresql
T 1568708607 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568708607 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Thu Sep 12 16:46:28 2019)
T 1568708607 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568708923 18<StucKman18>	Berge: well, I didn't really did any measures :(
T 1568709095 18<Berge18>	StucKman: That's the only way to make sure what you're doing is helping
T 1568709256 18<Myon18>	afaict SSL doesn't compress at all?
T 1568709302 18<StucKman18>	noy by default, but the only way to make pg compress by itself is enabling ssl
T 1568709367 18<StucKman18>	I just noticed that I can't control the compression level either
T 1568709387 18<StucKman18>	so I'll just stick to ssh tunnel, it's easier to setip
T 1568709411 18<Berge18>	Myon: Not since CRIME
T 1568709416 18<Berge18>	Or shouldn't, at least.
T 1568709436 18<Myon18>	"OpenSSL 1.1.0 disables compression by default"
T 1568709450 18<Myon18>	https://www.postgresql.org/docs/current/libpq-connect.html
T 1568709479 18<Berge18>	StucKman: Remember to measure it!
T 1568709487 18<Myon18>	does openvpn still do compression?
T 1568709525 18<Berge18>	It's off by default, afaik
T 1568709534 18<Berge18>	But I think maybe you can still turn it on
T 1568709551 18<Myon18>	ssh is likely the best/easiest option anyway
T 1568709562 18<Berge18>	But OpenVPN had a CRIME-related vulnerability a few years back.
T 1568709565 18<Berge18>	yeah
T 1568709582 18<Berge18>	It'd be nice with a tool that's as easy to use as ssh that'd do zstd or zstd-adapt over the network, though.
T 1568709599 18<Berge18>	Perhaps you can mangle socat into doing it.
T 1568709599 18<ioguix18>	but keeping a ssh tunnel up is not very conveniant
T 1568709614 18<Berge18>	ioguix: autossh
T 1568709659 18<Myon18>	fwiw what I recently started using is LocalForward 7632 /var/run/postgresql/.s.PGSQL.5432
T 1568709670 18<Myon18>	forwards a local TCP port to a remote PostgreSQL UNIX socket
T 1568709693 18<ioguix18>	Berge: thanks, I wasn't aware of this program
T 1568709706 18<Myon18>	(TCP because ssh is bad at removing the UNIX socket after closing, and fails on the next connect)
T 1568709708 18<Berge18>	Myon: As in ssh -L?
T 1568709712 18<Berge18>	I didn't know it could to unix sockets, that's neat
T 1568709729 18<Myon18>	-L would be the cli version, yes
T 1568709730 18<StucKman18>	awesome
T 1568709733 18<ioguix18>	I will compare it with our home grew perl daemon we use for supervision to multiplex the ssh connexions
T 1568709741 18<Berge18>	I use -L and -R all the time
T 1568709793 18<Berge18>	I've actually considered doing something akin to what StucKman wants to do for a postgres client program that grabs huge result sets, but when I measured (using ssh with compression), the difference wasn't all that great
T 1568709833 18<Berge18>	Parsing and processing time in the client is much greater than the network transfer speed anyway, in our case.
T 1568709838 18<Berge18>	The gains are very, very modest in practice.
T 1568709851 18<ioguix18>	compression without adding the crypto thing would be ideal
T 1568709865 18<Berge18>	Why?
T 1568709868 18<Berge18>	For CPU reasons?
T 1568709897 18<ioguix18>	crypto is already handled by the pgsql proto
T 1568709907 18<ioguix18>	if needed
T 1568709948 18<Myon18>	yeah that seems orthogonal and useful
T 1568709972 18<StucKman18>	Berge: good point, I simpy see the networkd capping while fetching w/o compression
T 1568709993 18<StucKman18>	capping=saturing
T 1568709994 18<Myon18>	.oO(some users might actually benefit from compressed queries... WHERE id IN (..., ...) )
T 1568710035 18<ioguix18>	Myon: really? on a LAN?
T 1568710049 18<ioguix18>	how huge would be your IN() list??
T 1568710066 18<Myon18>	I've seen megabytes (but that suggestion wasn't entirely serious)
T 1568710102 18<capitol18>	badly designed hibernate queries use huge in lists
T 1568710132 18<Berge18>	StucKman: What's the netowrk speed?
T 1568710153 18<DarkUranium18>	Berge, I think LZ4 might also be a good choice (apart from zstd).
T 1568710209 18<Berge18>	DarkUranium: yeah, likely. zstd has done better than lz4 in my tests, but it'll of course depend on datasets and CPU (features)
T 1568710226 18<Berge18>	And of course what you want to optimise for.
T 1568710254 18<DarkUranium18>	Yeah. I'm thinking LZ4 for compression speed/CPU use, zstd for compression ratio.
T 1568710266 18<DarkUranium18>	(as a rule of thumb, ofc)
T 1568710278 18<Berge18>	Well, to achieve the same compression level as zstd, lz4 needed quite a bit more CPU time in my tests
T 1568710292 18<Berge18>	As always, a speed-compression tradeoff
T 1568710313 18<DarkUranium18>	To be fair, LZ4 is very much decompression-speed optimized, not compression-speed.
T 1568710328 18<DarkUranium18>	But I didn't mean at the same compression level anyways.
T 1568710344 18<Berge18>	The zstd training mode is also cool
T 1568710383 18<DarkUranium18>	I have been toying with the idea of compressing some of my larger columns using either zstd or LZ4, with a predetermined table.
T 1568710399 18<DarkUranium18>	(they're 30kB on average ... not huge, but when you got thousands, it adds up)
T 1568710407 18<Berge18>	You can pre-train a dictionary for zstd on data sets similar to what you're going to compress.
T 1568710413 18<DarkUranium18>	Exactly.
T 1568710429 18<Berge18>	Which helps very much for small datasets.
T 1568710464 18<chris6418>	Berge: does it only speed up the compression or does it also improve the compression ratio?
T 1568710479 18<Berge18>	chris64: Both, to the same degree
T 1568710483 18<Berge18>	ish
T 1568710485 18<chris6418>	cool
T 1568710494 18<DarkUranium18>	Berge, I'd argue it helps even more for what I'm considering, which is small data *items* (per-column compression)
T 1568710501 18<Berge18>	DarkUranium: yep
T 1568710552 18<DarkUranium18>	Anyhow, the reason I was erring towards LZ4 here was to reduce CPU use, and because it's simpler (more portable).
T 1568710609 18<nickb18>	Hi. We had a "weird" crash yesterday: https://dpaste.de/4oRH/raw that resulted in replica losing a WAL segment (or part of it). This is the second time this has happened in the last few months, when primary removes a WAL-segment that wasn't actually shipped. Unfortunately I don't have WAL from primary, but I do have WAL from replica. Pg 10.9
T 1568710668 18<DarkUranium18>	Berge, I don't have zstd or LZ4 handy here, but interestingly, zip compression only gives me a 1.07 ratio.
T 1568710677 18<DarkUranium18>	rar gives me ~2.0
T 1568710686 18<Berge18>	apt install zstd
T 1568710686 18<DarkUranium18>	(DEFLATE vs whatever RAR uses ... LZMA?)
T 1568710697 18<DarkUranium18>	In Windows ATM.
T 1568710820 18<Myon18>	nickb: filesystem/kernel bug?
T 1568710845 18<Myon18>	nickb: or... what's "0_wal"?
T 1568710900 18<Myon18>	nickb: do you still have logs from last time?
T 1568711026 18<nickb18>	Myon: ugh. EXT4 journal is clean, kernel bug is possible; No idea what 0_wal is; Logs from previous time are gone as we migrated to new hardware since then, and logs are now gone unfortunately
T 1568711048 18<Myon18>	oh completely new hardware?
T 1568711055 18<Myon18>	could be RAM corruption
T 1568711075 18<Myon18>	is that segfault a bug?
T 1568711078 18<adsf18>	if i need to update 1 table from another table, and i don't care about update on conflict, what is the default handling of insert if it finds a dupe primary key?
T 1568711103 18<Myon18>	by default it raises an exception
T 1568711111 18<adsf18>	ahh, will it just stop the operation?
T 1568711115 18<Myon18>	yes
T 1568711118 18<f3f3lix18>	we have ecc
T 1568711122 18<adsf18>	guess im going down the on conflict update :p
T 1568711131 18<f3f3lix18>	if it was ram, it was a multibit ecc where the parity checks out
T 1568711145 18<nickb18>	Myon: we still have to investigate that segfault; Might be a bug, but it points to libc.so. How would RAM corruption lead to WAL file going missing? Also, what f3f3lix said (ECC)
T 1568711319 18<Myon18>	nickb: note that it tried to move the file to 0_wal/something, of course that fails
T 1568711365 18<f3f3lix18>	yes. we noted that. isn't that very odd
T 1568711369 18<Myon18>	it's suspicious that "pull" and postgres were corrupted at the same time
T 1568711389 18<Myon18>	or 2 min apart
T 1568711417 18<Myon18>	so it'd be interesting to check if pull has a real bug, or if that's some corruption somewhere
T 1568711428 18<f3f3lix18>	highly suspicious, but we have no evidence of actual hardware failure
T 1568716582 18<maxter18>	I have a table on which deletes are being fired. I want a trigger that ignores the delete .. anyway to achieve this?
T 1568716749 18<mobidrop18>	maxter, you could use a trigger or a rule
T 1568716777 18<maxter18>	I am trying trigger but instead of is not allowed on tables
T 1568716798 18<chris6418>	maxter: Did you find https://www.postgresql.org/docs/9.2/plpgsql-trigger.html (or newer)? There it says that "In the case of a before-trigger on DELETE, the returned value has no direct effect, but it has to be nonnull to allow the trigger action to proceed. Note that NEW is null in DELETE triggers, so returning that is usually not sensible. The usual idiom in DELETE triggers is to return OLD."
T 1568716848 18<chris6418>	I would interpret that as returning null ignores the delete.
T 1568716884 18<maxter18>	so your saying put a before delete with a return NULL will keep my row intact?
T 1568716907 18<ilmari18>	I would expect that to make the DELETE return an error, not silently not delete it
T 1568716948 18<maxter18>	I tired a 1/0 but got an error ..
T 1568716950 18<mobidrop18>	probably it will give an exception if you return null
T 1568716965 18<chris6418>	ilmari: mh, true when thinking one more about it there must be two possible reactions, pass, ignore, reject.
T 1568716968 18<chris6418>	*three
T 1568716973 18<mobidrop18>	postgres in general doesn't fail silently
T 1568717039 18<chris6418>	but this is not failing
T 1568717058 18<chris6418>	however I see your point mobidrop
T 1568717093 18<ilmari18>	actually, I just tested it, and it does seem to silently not delete the rows when I add a BEFORE DELETE  FOR EACH ROW EXECUTE PROCEDURE trigger_returning_null()
T 1568717149 18<chris6418>	interesting
T 1568717158 18<mobidrop18>	ok documentation seems to be right
T 1568717174 18<chris6418>	so would one then be supposed to throw some kind of error?
T 1568717221 18<chris6418>	to reject the delete and abort the transaction?
T 1568717806 18<maxter18>	worked. even though we assume some error should come.
T 1568717816 18<maxter18>	cleanly ignores
T 1568718278 18<Intelo18>	insert
T 1568718281 18<Intelo18>	sorry
T 1568718372 18<Intelo18>	I want to insert a table row but only if the id does not exists. So I need "select for insert"?
T 1568718420 18<Myon18>	that doesn't work because there's row to lock (what SELECT FOR UPDATE does)
T 1568718438 18<Myon18>	you want INSERT ON CONFLICT... DO NOTHING
T 1568718495 18<Intelo18>	hm
T 1568718530 18<Intelo18>	Myon,  is that upsert?
T 1568718534 18<Myon18>	yes
T 1568718546 18<Myon18>	with the "up" part muted
T 1568718584 18<Myon18>	you could also just INSERT and ignore/catch the error
T 1568718616 18<Intelo18>	hm
T 1568721522 18<svip18>	Does a large `col IN (...)` list make psql noticeably slower?
T 1568721589 18<Myon18>	the parse needs huge amounts of RAM if the list is really long
T 1568721619 18<svip18>	Suppose I have a large table, T, (say 1 million rows), and I have a list of 3000 IDs, that I want to return of.  But I wish to restrict it to 100 result (`LIMIT 100`), but I am including all 3000 IDs in the WHERE clause, to ensure they are sorted correctly, i.e. so it is the first 100 rows of those 3000 elements according to a specific sort.
T 1568721640 18<svip18>	Myon: So the limit is on the parser?
T 1568721656 18<RhodiumToad18>	use an array instead of IN
T 1568721678 18<RhodiumToad18>	WHERE id = ANY (?)   where the ? is an array parameter, as long as you're not using psycopg2
T 1568721745 18<svip18>	I am using Go's PostgreSQL driver.
T 1568721815 18<svip18>	And arrays are faster than IN?
T 1568722010 18<chris6418>	svip: I think he's proposing to use a prepared statement then, so that the array is only inserted as a placeholder and transferred separately not in text but in binary form
T 1568722040 18<svip18>	I was going to do it as a prepared statement in either case.
T 1568722043 18<chris6418>	this then more efficient than parsing 3000 values from the string. however the query execution probably won't benefit
T 1568722072 18<svip18>	Or does prepared statements not work properly with IN?
T 1568722089 18<chris6418>	RhodiumToad: does that make a difference then?
T 1568722117 18<chris6418>	because it's not parsed anyway then, no?
T 1568722168 18<RhodiumToad18>	the parsing speed is better with arrays even if you do  =ANY('{1,2,3,4,...}'::integer[])  with an interpolated literal, but of course you should use a real parameter
T 1568722194 18<svip18>	The IDs in this case are UUIDs.
T 1568722210 18<RhodiumToad18>	most drivers pass parameters in text mode, not binary, but the specialized array-value parser is still far more efficient than the SQL parser
T 1568722231 18<chris6418>	RhodiumToad: interesting, thanks!
T 1568722260 18<RhodiumToad18>	the IN (blah,blah,blah) stuff goes through the general bison grammar, which is no speed demon and does a lot of memory allocations for each token
T 1568722338 18<RhodiumToad18>	(the problem with psycopg2 is that it interpolates array parameters as ARRAY[blah,blah,blah] which means you're right back to the bison parser again)
T 1568722893 18<AWizzArd18>	Is there some way to test if pg is listening when connecting via telnet? Some simple text protocol that would allow me to simulate being a client?
T 1568722922 18<RhodiumToad18>	no, the protocol isn't really amenable to text
T 1568723315 18<AWizzArd18>	RhodiumToad: oki
T 1568723320 18<AWizzArd18>	thx
T 1568724250 18<pstef18>	CodeIgniter uses pg_advisory_lock() to serialize the reads and writes of PHP session data and it feels wrong, but at the same time I don't know how to do that better
T 1568724299 18<pstef18>	in all cases, the choice is between waiting (which risks a DOS) and erroring (but how to handle that?)
T 1568724478 18<Myon18>	select for update?
T 1568724503 18<Myon18>	or just plain UPDATE
T 1568724578 18<pstef18>	ok, but then another session wants to do the same. and it either waits (and subsequently saves its data when the first session is done) or it doesn't wait (and errors out)
T 1568724599 18<pstef18>	well, sorry, they all are the same session really, just different threads
T 1568724664 18<Myon18>	the usual trick is to use INSERT instead of UPDATE, and periodically consolidate entries back to the original row
T 1568724729 18<pstef18>	I thought of that as well. but the PHP session data is a blob that PHP encodes and decodes for itself
T 1568725050 18<Myon18>	the real fix for session data is to write stuff less often
T 1568725067 18<Myon18>	"user is still alive" doesn't need to be recorded for each single mouse click
T 1568725143 18<velix18>	I'm trying to do 140 left joins with USING :D
T 1568725427 18<velix18>	silence :D
T 1568725449 18<Myon18>	"don't"
T 1568725505 18<pstef18>	Myon: sure I'd like less contention, but the fundamental problem remains
T 1568725688 18<Myon18>	pstef: use SELECT FOR UPDATE instead?
T 1568725824 18<velix18>	nooooooooo ERROR:  54000: target lists can have at most 1664 entries
T 1568725857 18<pstef18>	Myon: doesn't avoid the problem of two DB sessions wanting to do the SELECT FOR UPDATE
T 1568725898 18<Myon18>	this is a php problem, not a PostgreSQL one, isn't it?
T 1568725902 18<pstef18>	I think the obvious conclusion is that since we don't want to wait on the other DB session, one of them will have to handle the conflict
T 1568725921 18<velix18>	Is there no way to raise the amount of maximum columns?
T 1568725941 18<Myon18>	velix: only by recompiling
T 1568725948 18<velix18>	;...-(
T 1568725950 18<Berge18>	But do you _really_ need it?
T 1568725950 18<Myon18>	the limesurvey people recommend doing that
T 1568725954 18<velix18>	Berge: Yes!
T 1568725959 18<Berge18>	Myon: wow
T 1568725969 18<velix18>	Myon: Guess what I'm doing ;)
T 1568725978 18<velix18>	Survey stuff.
T 1568725996 18<Myon18>	https://manual.limesurvey.org/Instructions_for_increasing_the_maximum_number_of_columns_in_PostgreSQL_on_Linux
T 1568726005 18<pstef18>	debatable. I think "it's about how PG is used in a particular case" is not incorrect
T 1568726129 18*	velix switching to R
T 1568726132 18<velix18>	;)
T 1568726149 18<Myon18>	velix: jsonb should work well there
T 1568726182 18<velix18>	Myon: I'm merging 140 tables on a single key to get > 2000 columns. How can jsonb help here?
T 1568726251 18<Myon18>	for storing the columns in a single table
T 1568726285 18<Myon18>	you could also jsonb_build_object in the query, but of course that's a kludge
T 1568726473 18<velix18>	Myon: but quering this would be the next problem. I need to export as normal columns.
T 1568726478 18<velix18>	I'm working on a work.around in R
T 1568726677 18<theseb18>	Can someone explain issue with second small snippet here?... https://pastebin.com/SD6mB2Ph
T 1568726694 18<theseb18>	I just added a FROM clause to end of a working select
T 1568726723 18<RhodiumToad18>	what on earth made you think that was allowed?
T 1568726843 18<theseb18>	RhodiumToad: i thought you can repeat any select by just appending a from
T 1568726859 18<theseb18>	RhodiumToad: or in this case a "WITH ..... SELECT ..."
T 1568726880 18<theseb18>	with a as (select k as u),
T 1568726887 18<theseb18>	There is a smaller version w/ same issue
T 1568726889 18<theseb18>	^^
T 1568726905 18<RhodiumToad18>	you can't randomly append clauses
T 1568726957 18<theseb18>	RhodiumToad: What i just pasted has 5 lines.....The first 4 depend on k......The 5th link sets k....what is the problem? how fix? is it a trivial parentheses or something?
T 1568726962 18<RhodiumToad18>	look at the syntax in psql using  \h select
T 1568727036 18<theseb18>	RhodiumToad: this is like last nite's discussion...if you think of 1st 4 lines as defining f(k)....the 5th line sets k = 2.....I thought the whole mess would return f(2) !
T 1568727079 18<theseb18>	RhodiumToad: i'm in psql...\h just lists the SQL keywords
T 1568727102 18<RhodiumToad18>	\h select
T 1568727114 18<incognito18>	FROM <= only once in a query
T 1568727133 18<RhodiumToad18>	all of the clauses appear only once, in fact
T 1568727223 18<theseb18>	select
T 1568727237 18<theseb18>	RhodiumToad: that bombed also...
T 1568727242 18<RhodiumToad18>	also, you can't think of the WITH clauses as defining a function, because they don't do that; the WITH subqueries cannot reference anything defined outside the WITH clause
T 1568727252 18<RhodiumToad18>	what bombed?
T 1568727260 18<theseb18>	select
T 1568727269 18<RhodiumToad18>	what did you do exactly
T 1568727270 18<theseb18>	RhodiumToad: that did
T 1568727283 18<theseb18>	RhodiumToad: i wrapped the original 4 lines in parens
T 1568727292 18<RhodiumToad18>	why did you do that?
T 1568727304 18<theseb18>	RhodiumToad: because i thought you can have multiple FROMs if you use parens?
T 1568727312 18<RhodiumToad18>	*headdesk*
T 1568727315 18<theseb18>	RhodiumToad: subqueries n' all that
T 1568727320 18<RhodiumToad18>	that doesn't mean you can randomly add parens!
T 1568727332 18<RhodiumToad18>	a subquery can appear only in very specific places
T 1568727386 18<theseb18>	RhodiumToad: you said something that broke my heart... "the WITH subqueries cannot reference anything defined outside the WITH clause"
T 1568727402 18<RhodiumToad18>	at the current query level that is
T 1568727417 18<RhodiumToad18>	they can contain outer references if the WITH clause is itself in a subquery
T 1568727441 18<RhodiumToad18>	also, in a non-recursive WITH, they can only reference CTEs defined earlier in the clause
T 1568727465 18<RhodiumToad18>	CTEs are _not functions_, they behave like tables
T 1568727482 18<theseb18>	RhodiumToad: I rewrote it with one from but still bombed...see this gem...
T 1568727486 18<theseb18>	with k as (select 2),
T 1568727507 18<RhodiumToad18>	I don't know what you think you just pasted in channel but it was just one line
T 1568727515 18<theseb18>	RhodiumToad: now i have the 2nd CTE referencing k from the 1st CTE
T 1568727523 18<theseb18>	RhodiumToad: lemmie try pastebin
T 1568727529 18<RhodiumToad18>	use the damn paste site like you're supposed to (and ideally dpaste.de rather than that pastebin shit)
T 1568727591 18<theseb18>	RhodiumToad: https://dpaste.de/b79R
T 1568727619 18<theseb18>	RhodiumToad: why is that wrong?
T 1568727632 18<theseb18>	RhodiumToad: it is so short it is running out of things to go wrong i hope
T 1568727633 18<RhodiumToad18>	because you're confusing tables and columns
T 1568727644 18<RhodiumToad18>	the name of a CTE is a _table_, not a column
T 1568727685 18<RhodiumToad18>	with k as (select 2 as v), a as (select v as u from k), ...
T 1568727749 18<theseb18>	RhodiumToad: i moved k to column..why this still bombed then?.. https://dpaste.de/CJB6
T 1568727833 18<RhodiumToad18>	because you ignored what I said
T 1568727876 18<theseb18>	RhodiumToad: ok...thinking...sec
T 1568727878 18<RhodiumToad18>	the columns in scope in a given subquery are (a) the ones from the tables in the FROM clause of that subquery, plus any outer references
T 1568727906 18<RhodiumToad18>	outer references come only from _enclosing_ queries, not other queries at the same level
T 1568727958 18<RhodiumToad18>	you can't ever reference anything from a CTE without using the CTE name _in a FROM clause_
T 1568727980 18<theseb18>	RhodiumToad: hmmm
T 1568728012 18<theseb18>	that last comment helped
T 1568728274 18<theseb18>	RhodiumToad: wow...thanks..i got something that worked but my brain hurts ;)
T 1568730633 18<eacameron18>	Can a SERIALIZABLE READ ONLY DEFERRABLE ever *fail* due to another write in the middle?
T 1568731405 18<Zr4018>	yes
T 1568731897 18<ilmari18>	I thought point of READ ONLY DEFERRABLE was that it waits until it can get a snapshot that can't possibly have serialisation conflicts
T 1568732075 18<ilmari18>	When all three of [SERIALIZABLE READ ONLY DEFERRABLE] are selected for a transaction [] it is able to run [] without any risk of contributing to or being canceled by a serialization failure - https://www.postgresql.org/docs/current/sql-set-transaction.html
T 1568733579 18<Sihar18>	hello all, I want to ask, anyone ever install stolon using docker swarm?
T 1568733599 18<Sihar18>	how we initial cluster when first time install?
T 1568733661 18<peerce18>	what is stolon ?
T 1568733842 18<Sihar18>	tools for make postgresql as cloud native HA
T 1568733855 18<Sihar18>	https://github.com/sorintlab/stolon
T 1568733889 18<peerce18>	best to ask them, thats not a postgres native project, and i've never heard of it before even after years of using postgres
T 1568733968 18<Sihar18>	alright peerce
T 1568734025 18<asakahs18>	zxcv
T 1568735188 18<velix18>	 min(numeric, numeric) doesn't exist? I'm confused.
T 1568735215 18<velix18>	ah no.
T 1568735218 18<velix18>	DONT ANSWER
T 1568737253 18<davidfetter_work18>	hi
T 1568737282 18<davidfetter_work18>	I noticed that we got a commit recently for date formats in fractional seconds
T 1568737351 18<davidfetter_work18>	it leaves out FF7-FF9 because we don't store time at ns precision. Is that feasible, or is it made in anticipation of feasibility?
T 1568737417 18<ysch18>	eacameron: No. At least, if it does, it would be a bug, I guess.
T 1568737538 18<eacameron18>	ysch: Ah good. So the once the deferrable read starts it effectively blocks other writes until it's done?
T 1568737567 18<eacameron18>	Or, it has a consistent snapshot of state that can't be changed in the middle
T 1568737582 18<ysch18>	eacameron: Nothing like it. ;) Reads _never_ block writes. It reads the consistent snapshot, like RR would.
T 1568737610 18<eacameron18>	Ah perfect.
T 1568737629 18<eacameron18>	Thank you. So there's no need to add retrying logic around a DEFERRABLE read-only serializable transaction
T 1568737771 18<ysch18>	eacameron: Well, not sure what happens if it bumps into a DDL lock (due ALTER TABLE in a concurrent session, for instance).
T 1568737851 18<jonez18>	depesz, it works fine in 10.x with a GIN
T 1568737863 18<jonez18>	note I'm using ltree[]
T 1568737908 18<davidfetter_work18>	I'm so sorry
T 1568738062 18<jonez18>	it is a proper usage afaik
T 1568738332 18<depesz18>	jonez: interesting. as far as I can tell, in my pg 13 - ltree doesn't have support for gin indexes.
T 1568738343 18<depesz18>	so, not sure what you have working, and would love to see it.
T 1568738441 18<depesz18>	oh, interesting. i was able to create the gin index too.
T 1568738476 18<jonez18>	could you try it on 9.6?
T 1568738482 18<depesz18>	i don't have 9.6
T 1568738486 18<jonez18>	hmm
T 1568738505 18<depesz18>	but if you have, just do: create table test (id serial primary key, ls ltree[]); CREATE INDEX path_gin_idx ON test USING GIN (ls);
T 1568738510 18<depesz18>	and you will immediately know.
T 1568738576 18<jonez18>	I get that same error about how ltree[] has no default operator class.
T 1568738605 18<depesz18>	jonez: based on my quick look, it seems that gin index can be created, but it's using array_ops, so i guess it will have limited functionality.
T 1568738619 18<jonez18>	but gist will work?
T 1568738627 18<depesz18>	sure. docs mention usage of gist.
T 1568738637 18<jonez18>	ok I will try that
T 1568739544 18<jonez18>	ok, gist is working. cut a couple of ms off the execution time for a common query. ty for your  help :
T 1568739545 18<jonez18>	:)
T 1568740201 18<davidfetter_work18>	so about ns timestamps...
T 1568740263 18<davidfetter_work18>	would that change fundamentally how we deal with them?
T 1568740286 18<esran18>	is there any way to identify currently running queries for a session that are nested deep within a function call?
T 1568740309 18<davidfetter_work18>	I'd love to know how if there is
T 1568740332 18<davidfetter_work18>	I don't suppose the functions are SQL all the way down...
T 1568740641 18<steve-chavez18>	Question regarding counting. I have a view like this: CREATE VIEW test AS SELECT id, expensive_op(id) FROM items;
T 1568740660 18<steve-chavez18>	And I do: SELECT count(id) FROM test;
T 1568740695 18<steve-chavez18>	I mistakenly thought that count would imply that expensive_op(id) is not executed but it does.
T 1568740716 18<steve-chavez18>	Is there a way to prevent that?
T 1568740792 18<steve-chavez18>	I've confirmed that it is executed by a RAISE NOTICE inside expensive_op.
T 1568741377 18<steve-chavez18>	Doing a SELECT 1 FROM test; also results in expensive_op(id) being executed.
T 1568741780 18<Zr4018>	steve-chavez: is expensive_op declared stable or volatile?
T 1568741812 18<steve-chavez18>	volatile
T 1568741827 18<davidfetter_work18>	that might account for it
T 1568741852 18<Zr4018>	s/might/will/
T 1568741853 18<steve-chavez18>	Oh.. stable would prevent it being called? Let me try..
T 1568741885 18<davidfetter_work18>	steve-chavez, if the outputs depend exactly and only on the inputs, you can mark it IMMUTABLE
T 1568741894 18<Zr4018>	yes, it will. But remember that STABLE is a promise that the function has no side effects, or in other words, you are okay with it being called zero times or ten times when you expect it to be called once
T 1568741933 18<Zr4018>	IMMUTABLE has a bunch of other restrictions too
T 1568741955 18<davidfetter_work18>	??immutable
T 1568741955 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/xfunc-volatility.html
T 1568742036 18<davidfetter_work18>	Quoth the manual: For best optimization results, you should label your functions with the strictest volatility category that is valid for them.
T 1568742076 18<davidfetter_work18>	If you lie to the DB, it will get its revenge ;)
T 1568742137 18<steve-chavez18>	I've just confirmed it doesn't execute the expensive_op with STABLE. Amazing.. I see why it works that way now.
T 1568742157 18<steve-chavez18>	Zr40, davidfetter_work: Thank you so much! :)
T 1568742174 18<davidfetter_work18>	always happy to help
T 1568742338 18<theseb18>	hi
T 1568742342 18<xocolatl18>	does anyone know off hand the history of why dropping a domain drops the columns associated with them?
T 1568742375 18<davidfetter_work18>	xocolatl, that sounds like a really unpleasant discovery. sorry to hear about it.
T 1568742411 18<xocolatl18>	I've known about it for a while, but was reminded of it today
T 1568742420 18<davidfetter_work18>	xocolatl, I presume you'd want the columns to demote to the next non-dropped level
T 1568742434 18<xocolatl18>	I'd want standard compliance
T 1568742454 18<davidfetter_work18>	what wisdom does the standard have to offer on this matter?
T 1568742469 18<xocolatl18>	assign the base type to the column and copy over the constraints
T 1568742504 18<davidfetter_work18>	in principle, that sounds doable.
T 1568742517 18<davidfetter_work18>	so the base type, not just the next-lower remaining domain
T 1568742530 18<Zr4018>	this happens without CASCADE?
T 1568742531 18<xocolatl18>	the type the domain is based on
T 1568742549 18<davidfetter_work18>	ah, so not necessarily the base type. cool :)
T 1568742584 18<theseb18>	davidfetter_work: say....i thought of something maybe smart about SQL I was wondered your thoughts about...in procedural languages you can debug functions 1 at a time ....call it "bottom up programming"...."divide and conquer" or "encapsulation".....If I gave you a 500 line SELECT to debug....I'm not sure if you can rewrite it as a bunch of
T 1568742584 18<theseb18>	separate views and CTEs that you can debug separately to do the same in SQL
T 1568742593 18*	davidfetter_work picturing layered domains like nonnegative_int and under_ten
T 1568742610 18<eacameron18>	Another question about SERIALIZABLE READ ONLY DEFERRABLE: Can this be starved? I.e. can the wait for a consistent state be infinite even without a deadlock?
T 1568742622 18<mst18>	davidfetter_work: I do those lots with Type::Tiny in perl
T 1568742629 18<mst18>	I mean, like, the moral equivalent thereof
T 1568742647 18<eacameron18>	cc ysch
T 1568742670 18<davidfetter_work18>	you can do just what you described, theseb. also, SQL is (generally) easier to reason about because there isn't much by way of implementation detail there to distract you
T 1568742681 18<davidfetter_work18>	mst, neat :)
T 1568742714 18<theseb18>	davidfetter_work: hmmm ok..good to know
T 1568742771 18<davidfetter_work18>	that said, debugging a 500-line program is still debugging a 500-line program no matter what. It's just that they're a *lot* less common in SQL than they are elsewhere.
T 1568742814 18*	davidfetter_work has heard of generated SQL from OLAP systems that runs to megabytes, but is fortunate not to have dealt with anything like that yet
T 1568742833 18<davidfetter_work18>	the code, not the result sets
T 1568742842 18<theseb18>	davidfetter_work: i thought SQL has no concept of scope and everything was global...i was glad to discover today that CTEs have some idea of scope....e.g. you can only refer to columns of another CTE in a CTE with a FROM
T 1568742858 18<theseb18>	thanks to RhodiumToad for pointing that out
T 1568742881 18<davidfetter_work18>	right, and you can't refer to other things in a target list (the part between SELECT and FROM) inside the target list
T 1568742887 18<davidfetter_work18>	so there's scope
T 1568743280 18<eacameron18>	To answer my own question, according to https://drkp.net/papers/ssi-vldb12.pdf, the answer is YES, SERIALIZABLE READ ONLY DEFERRED can be starved, but it is highly unlikely even with large work loads.
T 1568743667 18*	davidfetter_work wonders whether Dan will come back and work on PostgreSQL some more
T 1568743679 18<davidfetter_work18>	drkp == Dan R. K. Ports
T 1568744100 18<OliverMT18>	I am left joining a measurement table, constraining on data_type = "foo" or data_type = "bar"
T 1568744111 18<OliverMT18>	and grouping by a source_id
T 1568744138 18<OliverMT18>	no wait, sorry, as I typed it out I realized my logic was flawed :D
T 1568744189 18<andres18>	davidfetter_work: Seems pretty unlikely.
T 1568744207 18<OliverMT18>	I want to be able to select so I get a result row as source.name, source.id, measurements.bar_value, measurements.foo_value
T 1568744239 18<OliverMT18>	and exlude any rows where I don't have both a data_type = "bar" value and data_type = "foo" value
T 1568744268 18<OliverMT18>	is that possible without joining measurements as two different named joins and putting the where in the on: clause?
T 1568744280 18<OliverMT18>	as in the row not being joined if the row with the correct data type doesnt exist
T 1568744315 18<Myon18>	do we get to guess the schema?
T 1568744397 18<OliverMT18>	I am typing up sample query
T 1568744424 18<OliverMT18>	https://gist.github.com/olivermt/3f0811ea27905c2a3323afb0dfb8b185
T 1568744439 18<OliverMT18>	so requirements is to have only one row per source
T 1568744458 18<OliverMT18>	and to only return a result if BOTH measurement vertical and horizontal has a result for that source
T 1568744470 18<OliverMT18>	the schema makes sure there can only be one horizontal or vertical measurement per source
T 1568744475 18<Myon18>	yeah that would be two JOINs
T 1568744479 18<OliverMT18>	ok, thought so
T 1568744480 18<Myon18>	but don't do LEFT
T 1568744481 18<OliverMT18>	thanks
T 1568744486 18<OliverMT18>	INNER ?
T 1568744495 18<OliverMT18>	if I do two INNERS I would indeed end up with zero rows if not both are present
T 1568744503 18<OliverMT18>	is that reasoning correct?
T 1568744511 18<Myon18>	yes
T 1568744514 18<OliverMT18>	thanks
T 1568744529 18<Myon18>	actually your current version isn't an OUTER (LEFT) join either because of the WHERE clause
T 1568744559 18<Myon18>	outer joins are rare, you shouldn't be using them by default
T 1568744697 18<OliverMT18>	that WHERE was just to illustrate the requirements but yes
T 1568745148 18<agohoth18>	are there any big data firms simply reading everything into postgresql and then building all kinda data reductions?
T 1568745152 18<agohoth18>	and rules stuff?
T 1568745163 18<agohoth18>	can you build rules for incoming streams in psotgresql?
T 1568745166 18<agohoth18>	whats new in 12?
T 1568745174 18<agohoth18>	wow 11 was fast here n gone
T 1568745193 18<Myon18>	read the 12beta announcement mails?
T 1568745465 18<StuckMojo18>	or the full release notes (which are in beta, but there)
T 1568745469 18<ryantrinkle18>	I'm trying to understand how read-only repeatable read transactions can observe non-serializable states of the DB
T 1568745479 18<ryantrinkle18>	in particular, this sentence from the manual: For example, even a read only transaction at this level may see a  control record updated to show that a batch has been completed but not see one of the detail records which is logically part of the batch because it read an earlier revision of the control record.
T 1568745499 18<ryantrinkle18>	if the detail records and the control record were committed in the same transaction, how could this happen?
T 1568745513 18<ryantrinkle18>	https://www.postgresql.org/docs/current/transaction-iso.html#XACT-REPEATABLE-READ
T 1568745596 18<ryantrinkle18>	is this for a scenario where the detail records are commited in txn 1, then the control record is updated in txn 2, but somehow the repeatable read transaction can see 2 but not 1?
T 1568746086 18<StuckMojo18>	anyone doing encryption at rest in the datacenter (not aws) and if so how are you doing it? (i.e. what are you using for it)
T 1568746102 18<xocolatl18>	??standard
T 1568746103 18<pg_docbot18>	http://wiscorp.com/SQLStandards.html :: http://troels.arvin.dk/db/rdbms/
T 1568746103 18<pg_docbot18>	http://savage.net.au/SQL/ :: http://wiki.postgresql.org/wiki/Developer_FAQ
T 1568746103 18<pg_docbot18>	http://wiki.postgresql.org/wiki/Developer_FAQ#Where_can_I_get_a_copy_of_the_SQL_standards.3F :: https://wiki.postgresql.org/wiki/PostgreSQL_vs_SQL_Standard
T 1568746103 18<pg_docbot18>	https://www.postgresql.org/docs/current/static/features.html
T 1568746127 18<xocolatl18>	can't find 99 anymore
T 1568746189 18*	davidfetter_work doesn't quite get why people refer to previous versions of the standard. the latest supersedes all previous
T 1568746259 18*	xocolatl doesn't need for davidfetter_work to get it
T 1568746386 18<OliverMT18>	Myon: I am looking at some old code I have that does something similar, and for some reason I am doing GROUP BY on the source
T 1568746406 18<OliverMT18>	but I am not doing any counts or anything, do you have a clue as to any situation I would need to GROUP BY in what I descriebd above?
T 1568746678 18<OliverMT18>	agohoth: a lot of 'big data firms' do that yes, using both timescaledb and citusdb
T 1568746843 18<agohoth18>	oh?
T 1568746946 18<agohoth18>	wow like extended postgresql
T 1568746947 18<agohoth18>	woa
T 1568746974 18<agohoth18>	at work they want to do batch and streams and use barfy java frameworks like argo
T 1568746983 18<agohoth18>	wadda mess
T 1568746991 18<agohoth18>	no one thinks about the data
T 1568747012 18<agohoth18>	just an afterthought   I blame java oo programing culture and finance incentives
T 1568747265 18<Logicgate18>	hey guys
T 1568747279 18<Logicgate18>	I'm using go-pg and having problems with locking queries staying open
T 1568747286 18<Logicgate18>	or "active" I should say
T 1568747307 18<Logicgate18>	when I start my process, everything is fine and eventually the PG processes start to pile up and the CPU usage rockets to 100%
T 1568747328 18<Logicgate18>	I have workers on a tick (every second), will query the DB.
T 1568747365 18<Logicgate18>	it's the same query idling
T 1568747382 18<Logicgate18>	https://puu.sh/Eik8J/d90135dd66.png
T 1568747742 18<agohoth18>	Is there any extended postgresql that is for batch processing?
T 1568747791 18<elmcrest18>	hey everybody. can one say it's better to have more smaller tables or less bigger tables?
T 1568747807 18<xocolatl18>	it's better to have the tables you need
T 1568747833 18<elmcrest18>	I have the choice ... so I was wondering if there maybe is a general rule of thumb
T 1568747848 18<xocolatl18>	how can you have the choice?
T 1568747881 18<Logicgate18>	agohoth, what do you mean?
T 1568747893 18<elmcrest18>	give this Django ORM model definition ... I was going with this first because of the comfort of having nice ORM queries ... but I wonder if that makes sense for postgres https://dpaste.de/2GpB
T 1568747948 18<elmcrest18>	xocolatl because I can either use inheritance, so Django just create foreignKey relations for me for "inheritance" and joins them, or I create a huge table with optional fields
T 1568747971 18<elmcrest18>	the example document types aren't the end of the game, in the end I expect about 70-100 document types
T 1568748001 18<agohoth18>	well my company is 80% batch ingenstion for its needs in analystics
T 1568748007 18<xocolatl18>	I'd need to see actual tables and queries.  I don't know what django does
T 1568748017 18<agohoth18>	they are looking at various java batch processing frameworks on apache.org
T 1568748029 18<agohoth18>	django lol
T 1568748038 18<elmcrest18>	I like it :P
T 1568748046 18<agohoth18>	its nice?
T 1568748071 18<agohoth18>	seems cart b4 horse to have a framework in python generate db tables
T 1568748098 18<elmcrest18>	agohoth its nice to me at least :P
T 1568748192 18<agohoth18>	Well seems to be like wrong to have some code generate some table definitons
T 1568748196 18<elmcrest18>	xocolatl tables https://dpaste.de/4Aea
T 1568748198 18<agohoth18>	where does the db engineering come in?
T 1568748216 18<elmcrest18>	agohoth at the level of implementing the ORM in python
T 1568748229 18<elmcrest18>	I'm pretty sure it does the SQL job better than I would ...
T 1568748297 18<elmcrest18>	xocolatl so that's the "base table" which is currently rather small ... https://dpaste.de/Nsh0
T 1568748321 18<elmcrest18>	if I go with my current "design" the "Referenced by" section would grow a lot ...
T 1568748499 18<elmcrest18>	so probably it doesn't really matter at my scale I guess ... I'm neither having millions of entries nor millions of tables
T 1568753031 20*	Disconnected (20)
T 1568753056 19*	Now talking on 22#postgresql
T 1568753056 22*	Topic for 22#postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
T 1568753056 22*	Topic for 22#postgresql set by 26xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (24Thu Sep 12 16:46:28 2019)
T 1568753056 22*	Channel 22#postgresql url: 24https://www.postgresql.org
T 1568753571 18<ryantrinkle18>	if i run a bunch of transactions in serializable mode, is there any guarantee that their commit order in the WAL will correspond to a valid serialization of the transactions?
T 1568754197 18<jonez18>	greetings. I'm experimenting with 'create table as', and it appears it only looks at rows from __link once at the time I create the table. so if I add something to __link, link does not have it.
T 1568754247 18<jonez18>	is there a way to create a view (or an object that updates automagically) that I can also add a gist index to?
T 1568754372 18<Intelo18>	xocolatl,  just pseudo code to explain
