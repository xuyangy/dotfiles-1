**** BEGIN LOGGING AT Sat Aug 31 11:22:36 2019

Aug 31 11:22:36 *	Now talking on #postgresql
Aug 31 11:22:36 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Aug 31 11:22:36 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Aug 31 11:22:36 *	Channel #postgresql url: https://www.postgresql.org
Aug 31 11:23:30 <dob1>	hi, what is wrong about this query https://pastebin.com/xTxPhkZZ   it complains about s
Aug 31 11:25:14 <xocolatl>	dob1: you cannot use the alias in an expression
Aug 31 11:25:33 <xocolatl>	dob1: so you need  WHERE ricerca ->> 'nome' = 'test'
Aug 31 11:26:41 <dob1>	xocolatl, ok thanks
Aug 31 11:48:51 <BigFoot455>	How can you explain an increase of buffer hits for an index-scan after some columns are removed? I expected that after removing two columns (one var and one timestamp) the whole table will consist of less database pages and therefore lead to less buffer hits in the index scan.
Aug 31 11:49:26 <BigFoot455>	The query I'm executing is executed completely in memory.
Aug 31 11:50:02 <BigFoot455>	And after this "optimization" the query is minimal slower than before.
Aug 31 11:52:52 <BigFoot455>	The query itself didn't change. The columns were just useless for this query.
Aug 31 11:53:27 <incognito>	BigFoot455: what did you change in the index ?
Aug 31 11:54:17 <xocolatl>	BigFoot455: removing columns doesn't actually remove the columns from existing data
Aug 31 11:54:33 <BigFoot455>	Nothing on the index. I just dropped two columns from the table, that were not used in the query or any indices.
Aug 31 11:55:16 <incognito>	the index scan buffer hits is only for the index_tuple
Aug 31 11:55:23 <xocolatl>	cool, so that's kind of important information that you should provide
Aug 31 11:55:29 <xocolatl>	did you do a normal vacuum after that?
Aug 31 11:55:51 <BigFoot455>	Normal vacuum after the full vacuum?
Aug 31 11:55:59 <incognito>	if it increases after a query, maybe more pages of your index are cached in the shared buffer
Aug 31 11:55:59 <xocolatl>	yes
Aug 31 11:56:03 <BigFoot455>	No. Is this necessary?
Aug 31 11:56:16 <xocolatl>	it is necessary, yes
Aug 31 11:56:22 <BigFoot455>	Why?
Aug 31 11:56:30 <xocolatl>	several reasons
Aug 31 11:57:09 <xocolatl>	the main ones are to recreate the visibility map and free space map
Aug 31 11:58:24 <BigFoot455>	I thought a full vacuum, does everything a "normal" vacuum does, but first rewrites the whole table.
Aug 31 11:58:40 <xocolatl>	nope
Aug 31 11:58:42 <incognito>	BigFoot455: wanna show the explain and the query ?
Aug 31 12:00:10 <BigFoot455>	Alright let me try with an additional vacuum then.
Aug 31 12:00:48 <BigFoot455>	I can uploaded, but just the obfuscated version. One second.
Aug 31 12:01:10 <xocolatl>	BigFoot455: don't bother with an obfuscated version
Aug 31 12:01:41 <xocolatl>	if your table names are that important, you should hire a consultant agency and sign an nda with them
Aug 31 12:04:17 <BigFoot455>	Thing is that I didn't talk to my supervisor, whether company is okay with sharing the queries on the internet. So that's what I can do for now.
Aug 31 12:04:54 <BigFoot455>	https://explain.depesz.com/s/IXVb
Aug 31 12:05:54 <RhodiumToad>	and what was the plan before?
Aug 31 12:06:25 <BigFoot455>	As you can see the "optimized" query has about 20k more buffer hits than before.
Aug 31 12:07:03 <BigFoot455>	This is the before plan. There should be a link to the "optimized" query. At least for me there is. I used the "Add optimization" feature.
Aug 31 12:08:19 <BigFoot455>	One question in between: I see you guys somehow referencing my nickname all the time. How can I do this?
Aug 31 12:08:39 <xocolatl>	just type it
Aug 31 12:09:32 <xocolatl>	you didn't vacuum
Aug 31 12:11:23 <RhodiumToad>	BigFoot455: so your two queries are not exactly the same?
Aug 31 12:11:51 <Rembane>	BigFoot455: If you have a reasonable client you can type the two first letter of the nick you want to hilight and press TAB and then it will autocomplete.
Aug 31 12:12:00 <BigFoot455>	xocolatl: I didn't vacuum. I need to setup my database again, because I tried some other stuff meanwhile.
Aug 31 12:12:32 <BigFoot455>	Rembane: Yes it works. thanks ;).
Aug 31 12:13:02 <RhodiumToad>	this clause: ((quebec_seven five NOT NULL) OR (quebec_seven five NULL))  appears only in one of the queries
Aug 31 12:13:28 <Rembane>	BigFoot455: No worries, welcome to IRC. :)
Aug 31 12:13:50 <RhodiumToad>	(I grant you, that should not affect the result)
Aug 31 12:13:59 <BigFoot455>	RhodiumToad: I did some minor adjustments. Like removing this tautology.
Aug 31 12:15:13 <RhodiumToad>	what else did you change?
Aug 31 12:18:04 <BigFoot455>	RhodiumToad: In the query? Nothing.
Aug 31 12:18:29 <RhodiumToad>	see, I get suspicious when people say that
Aug 31 12:18:42 <RhodiumToad>	especially when they said it before and have just been proved wrong
Aug 31 12:19:46 <RhodiumToad>	is the order of items the same in that ANY clause?
Aug 31 12:23:52 <BigFoot455>	Except for some whitespace changes and the tautology, I can confirm that nothing changed. I just double checked.
Aug 31 12:24:00 <BigFoot455>	Same. Yes.
Aug 31 12:28:23 <BigFoot455>	Okay. So after removing the columns I'll run VACUUM FULL, then VACUUM and then ANALYZE.
Aug 31 12:52:33 <m1chael>	hello. I am trying to use pg_dump... when i'm logged in to the postgres user, doing pg_dump works, however pg_dump -h localhost or pg_dump -h 127.0.0.1 produces an error: FATAL:  Ident authentication failed for user "postgres"  # i know the short answer would be to remove the -h host switch, but can anyone explain why this doesn't work and how i can fix it so that it works with -h localhost ?
Aug 31 12:55:27 <dennis_>	m1chael: your pg_hba.conf file decide how auth is handled, in your case when you don't use -h it uses a "local" line in the pg_hba.conf and with -h it uses some other line
Aug 31 12:55:48 <m1chael>	ah
Aug 31 12:56:48 <m1chael>	i just got something unrelated working after 1 year of troubleshooting, so this one sounds easy
Aug 31 13:04:03 <dennisb>	You probably have a line stating that when you come from 127.0.0.1 it should use the "ident" method. If you change it to "md5" or "scram-sha-256" it will ask you for a password. If you set it to "trust" it will let you in, no questions asked, but that's probably not the security you want.
Aug 31 13:05:39 <m1chael>	i'm going to modify the script to leave out -h since i'll be running this from the pg user on a crontab... after i get this working i'd like to look at Barman
Aug 31 13:07:10 <RhodiumToad>	"ident" should generally not be used on "host" lines since it uses an external identd server, which (a) you probably don't have running, (b) you probably should not run, and (c) isn't all that secure anyway
Aug 31 13:07:35 <DrAgOn_>	CiauuuZ!!!!!!!
Aug 31 13:07:54 <RhodiumToad>	whut
Aug 31 14:17:58 <BigFoot455>	So I run the query now again, after executing VACUUM (after VACUUM FULL). The result is the same. With the smaller table about 20k more buffer hits are counted.
Aug 31 14:20:04 <RhodiumToad>	which table exactly did you vacuum?
Aug 31 14:20:35 <BigFoot455>	The table that I columns I dropped.
Aug 31 14:21:38 <RhodiumToad>	yes but which one is it in the query
Aug 31 14:24:40 <BigFoot455>	Is it possible, that due to the dropped columns, the rows are organized less efficient in the database pages for this query? Another query shows a performance boost (a query that has about 600k less buffer reads after dropping columns)
Aug 31 14:25:43 <RhodiumToad>	dropping the columns doesn't move any data. the vacuum full afterwards did, but any effect on the efficiency would be pure chance
Aug 31 14:26:16 <BigFoot455>	RhodiumToad: the "juliet yankee_golf" table.
Aug 31 14:44:49 <RhodiumToad>	BigFoot455: the number of buffer hits for an index scan may vary slightly depending on the order in which rows are visited. not sure if this would explain what you see
Aug 31 14:45:24 <RhodiumToad>	for example, if it visits a row on block 1, then one on block 2, then one on block 1, that would be three hits, but 1,1,2 would be two hits
Aug 31 14:46:54 <RhodiumToad>	("hit" is incremented by taking a pin on a buffer that didn't need to be read in, but there's an optimization whereby if the scan's last tuple was on the same block as the current one, it reuses the old pin rather than taking a new one)
Aug 31 14:50:46 <unclechu>	hello. what actually `select into` does? does it really creates a new table?
Aug 31 14:51:16 <unclechu>	can i somehow mark such a shortcut to be automatically removed what transaction is commited?
Aug 31 14:53:12 <RhodiumToad>	select into  as a plain sql statement, or in plpgsql? they are different
Aug 31 14:53:35 <RhodiumToad>	select into  in plain sql is just another way to write  create table ... as select ...
Aug 31 14:53:48 <RhodiumToad>	if that's what you want, you can use  create temp table ... as select ...
Aug 31 14:55:09 <BigFoot455>	RhodiumToad: could be an explanation. Even though on average I'd expect the order to become more efficient. Maybe for this query it's just unfortunate.
Aug 31 14:55:36 <unclechu>	RhodiumToad: in `psql` repl
Aug 31 14:56:23 <RhodiumToad>	unclechu: then use  CREATE TEMP TABLE x ON COMMIT DROP AS SELECT ...;   and "x" will be dropped on commit
Aug 31 14:56:25 <unclechu>	RhodiumToad: is there any difference between `create temp table ... as select ...` and `with` statement?
Aug 31 14:56:50 <RhodiumToad>	unclechu: yes, they're completely different things
Aug 31 14:57:06 <unclechu>	RhodiumToad: what would be more efficient in context of memory and performance?
Aug 31 14:57:08 <RhodiumToad>	create temp table as select  actually creates a new temp table, that you can't access in the same query
Aug 31 14:57:24 <RhodiumToad>	but which you can access in following queries, create indexes on, etc.
Aug 31 14:57:36 <unclechu>	i assumet `with` would work better? since it probably doesn't create such a big structure but doing it on the fly?
Aug 31 14:58:19 <RhodiumToad>	until pg12 (not out yet), WITH actually materializes a copy of the query result, in memory up to work_mem and then spilling to disk
Aug 31 14:58:50 <RhodiumToad>	it's more efficient in some ways, but it has the problem that you can't create indexes on the WITH result, which can be an issue
Aug 31 14:59:14 <unclechu>	RhodiumToad: thanks a lot, now it's a lot more clear for me
Aug 31 15:02:02 <BigFoot455>	RhodiumToad: Also thanks from me :)
Aug 31 15:31:17 <dennisb>	So, shouldn't --data-checksums be the default for initdb? Do you people use checksums in production?
Aug 31 15:40:11 <Zr40>	dennisb: I'd want to, but since this is production this will have to wait until we decide to upgrade from 9.4
Aug 31 15:40:56 <Zr40>	at previous $work I did enable it as part of upgrading to 10
Aug 31 16:09:14 <Foxfir3>	troule connection with php. installed the php postgres module. guessing that takes care of the postgres side, but php has no idea. build in php server says: Uncaught Error: Call to undefined function pg_connect()
Aug 31 16:09:58 <Foxfir3>	assuming its a very common beginners mistake
Aug 31 16:16:14 <wkalt>	anyone know why this declarative partitioning fails? https://gist.github.com/wkalt/27eddddd6a69cfb2b8f0f38cc43d2a13
Aug 31 16:16:47 <wkalt>	those tables don't seem like they should overlap to me
Aug 31 16:53:01 <xocolatl>	wkalt: the overlap condition is the high end of the first partition compared with the low end of the second partition.  the condition expands like this:  1 > 1 OR (1 = 1 AND '2019-01-01' > '2019-01-02') OR (1 = 1 AND '2019-01-01' = '2019-01-02' AND 2 > 1)
Aug 31 16:53:07 <xocolatl>	which is false, so they overlap
Aug 31 16:53:28 <RhodiumToad>	why is it false?
Aug 31 16:53:34 <RhodiumToad>	actually the problem is more subtle
Aug 31 16:53:42 <xocolatl>	which one of those conditions is true?
Aug 31 16:54:15 <RhodiumToad>	oh yeah
Aug 31 16:55:08 <RhodiumToad>	anyway, if retention_policy is to be used to decide which partition it goes in, that has to come before the date
Aug 31 16:56:06 <wkalt>	yeah, that's what I'm seeing experimentally. Thanks for confirming.
Aug 31 16:56:41 <RhodiumToad>	remember that (1,1,2) comes between (1,1,1) and (1,2,1)
Aug 31 16:57:40 <wkalt>	if that's the way to think about it, makes plenty sense
Aug 31 16:58:26 <xocolatl>	switching upload_time and retention_policy give: (1 > 1) OR (1 = 1 AND 2 > 1) OR (1 = 1 AND 2 = 1 AND '2019-01-01' > '2019-01-02')
Aug 31 16:58:32 <xocolatl>	and that one has a true condition
Aug 31 16:59:14 <xocolatl>	in other news, it is very likely that upload_time should actually be of type timestamptz
Aug 31 16:59:52 <pgwhatever>	Im trying to register my blog to postgresql.  Not really sure about all this RSS feed stuff.  Do i just provide my blog's URL for the Feedurl?
Aug 31 17:00:02 <RhodiumToad>	no
Aug 31 17:00:23 <wkalt>	yeah, the issue is I really want to be able to get appropriate pruning behavior for queries on upload+dataset alone
Aug 31 17:00:40 <RhodiumToad>	you need to provide an RSS url, ideally one that includes only the postgresq-related content
Aug 31 17:00:45 <xocolatl>	pgwhatever: what is your blog's url?
Aug 31 17:00:55 <pgwhatever>	blog.sqlexec.com
Aug 31 17:01:03 <wkalt>	seems like there could be a trick to get the execution time pruning to do it
Aug 31 17:01:30 <xocolatl>	pgwhatever: right at the bottom of your page you have http://blog.sqlexec.com/index.php?feed/atom
Aug 31 17:01:39 <xocolatl>	pgwhatever: that's what you give to planet
Aug 31 17:02:22 <RhodiumToad>	except that if you have non-postgresql content on the blog too, you want to give it a feed of just the relevant posts
Aug 31 17:02:35 <pgwhatever>	hmmm I don't see that link at the bottom of the page
Aug 31 17:02:51 <xocolatl>	it's called "Entries feed"
Aug 31 17:03:18 <pgwhatever>	ahhh
Aug 31 17:03:52 <xocolatl>	but as RhodiumToad says, if your feed publishes something that is not postgres related, you will be immediately de-listed
Aug 31 17:04:06 <pgwhatever>	gotcha, the name of my blog is PG Stuff, hehehe, guess its ONLY PG
Aug 31 17:04:16 <pgwhatever>	thanks big time!
Aug 31 17:04:37 <pgwhatever>	hot doggy, pending approval
Aug 31 17:04:50 <xocolatl>	moderation is manual and may take some time
Aug 31 17:04:59 <pgwhatever>	no problemo, im on the right path FINALLY!
Aug 31 17:05:03 <xocolatl>	(I moderate a lot of postgres stuff, but not planet)
Aug 31 17:05:24 <sobriquet>	RhodiumToad: copy and paste please.  I would like to see the answer.
Aug 31 17:05:43 <RhodiumToad>	what answer?
Aug 31 17:05:49 <xocolatl>	42
Aug 31 17:06:18 <dob1>	can I rename the key of a json object stored in jsonb column?
Aug 31 17:06:39 <RhodiumToad>	dob1: you can set a new key to the old value and remove the old key
Aug 31 17:07:33 <dob1>	RhodiumToad, I am looking at json functions
Aug 31 17:08:05 <RhodiumToad>	the - operator can delete a key, jsonb_set or || can be used to set a new key
Aug 31 17:08:41 <RhodiumToad>	sobriquet: what answer?
Aug 31 17:09:22 <dob1>	RhodiumToad, can I do this in an update statement?
Aug 31 17:09:26 <wkalt>	ok xocolatl RhodiumToad, I'm trying to get my pruning working and I found this: https://gist.github.com/wkalt/079356ac924cc8e41b7a57e641c46d25
Aug 31 17:09:38 <wkalt>	that last result seems incorrect to me. Thoughts?
Aug 31 17:10:14 <RhodiumToad>	dob1: yes of course
Aug 31 17:10:16 <dob1>	RhodiumToad, or I need to get the value in the old key, delete key, and then store a new key ?
Aug 31 17:10:40 <RhodiumToad>	dob1: update ... set col = (expression to calculate new value from col) where ...
Aug 31 17:11:06 <RhodiumToad>	if we're talking about top-level keys, the expression might look something like,
Aug 31 17:11:11 <dob1>	RhodiumToad, but then I need to delete the key in another statement
Aug 31 17:11:19 <dob1>	(old key)
Aug 31 17:12:33 <RhodiumToad>	why would you need another statement?
Aug 31 17:13:18 <RhodiumToad>	jsonb_set(col - text 'b', array['c'], col->'b', true)  removes the key "b" from "col" and stores its old value in key "c"
Aug 31 17:13:19 <dob1>	RhodiumToad, the update create the new key copying the vcalue from the old key, but I don't see how it delete the old key too
Aug 31 17:13:29 <xocolatl>	wkalt: I get a result when I try that here
Aug 31 17:13:29 <dob1>	ah
Aug 31 17:13:44 <wkalt>	xocolatl: I'm on 11.3. Downloading latest now.
Aug 31 17:14:10 <xocolatl>	I doubt anything has changed here between 11.3 and 11.5
Aug 31 17:14:39 <dob1>	RhodiumToad, I don't know this functions/operators, I have to read a bit :)
Aug 31 17:15:27 <wkalt>	xocolatl: curious to see. That script returns no result for me at the end.
Aug 31 17:16:00 <RhodiumToad>	wkalt: \d retention_policies
Aug 31 17:16:25 <RhodiumToad>	oh never mind
Aug 31 17:16:40 <wkalt>	https://gist.github.com/wkalt/245c616cb17ade8acbbf235095f81f49 fwiw
Aug 31 17:16:50 <xocolatl>	not sure what there is to "see", but https://dpaste.de/mjHG
Aug 31 17:17:26 <RhodiumToad>	wkalt: explain analyze that last query
Aug 31 17:17:54 <xocolatl>	missed a bit, https://dpaste.de/H1x9
Aug 31 17:18:41 <wkalt>	https://gist.github.com/wkalt/ebee4a1aafb897ed909b77a6a6d4cbb1 RhodiumToad
Aug 31 17:18:44 <wkalt>	it never hits the files table
Aug 31 17:19:17 <RhodiumToad>	One-Time Filter: false
Aug 31 17:19:45 <xocolatl>	I don't see where that filter is coming from
Aug 31 17:20:49 <RhodiumToad>	try it with 11.5
Aug 31 17:20:54 <wkalt>	alright give me a min
Aug 31 17:21:07 <xocolatl>	my reproduction doesn't have that.  https://dpaste.de/OgL6
Aug 31 17:21:15 <RhodiumToad>	what version are you on?
Aug 31 17:21:33 <xocolatl>	I guess the filter is coming from partition pruning
Aug 31 17:21:57 <RhodiumToad>	"Fix assorted errors in run-time partition pruning logic" -- 11.4 relnotes
Aug 31 17:22:08 <xocolatl>	ouch
Aug 31 17:22:22 <RhodiumToad>	no, that's possibly not it since this is plan-time pruning... still worth testing on 11.5
Aug 31 17:22:31 <xocolatl>	mine is 11.5
Aug 31 17:23:24 <wkalt>	works on 11.5
Aug 31 17:23:43 <RhodiumToad>	so it may have been that bug, or one of the others fixed between 11.3 and 11.5
Aug 31 17:23:54 <xocolatl>	that's a nasty bug
Aug 31 17:24:49 <RhodiumToad>	such false one-time filters are added in planning any time the planner thinks it has proved a contradiction, of which excluding all the partitions of a partitioned table is one example
Aug 31 17:25:43 <RhodiumToad>	if you have constraint exclusion enabled, you can get them from just normal query conditions like   WHERE x > 2 AND x < 1
Aug 31 17:26:14 <RhodiumToad>	but that level of checking isn't done by default, only partition and inheritance constraints are checked
Aug 31 17:26:32 <wkalt>	is what I really need subpartitioning here? that sounds a little scary
Aug 31 17:26:45 <incognito>	RhodiumToad: 12beta3 no pruning
Aug 31 17:26:46 <wkalt>	but I could partition on (dataset, upload_time) and subpartition on retention_policy
Aug 31 17:26:56 <wkalt>	that actually feels somewhat sensible
Aug 31 17:29:05 <wkalt>	RhodiumToad: xocolatl thanks for the help btw
Aug 31 17:30:56 <incognito>	xocolatl:  did you notice it works w/o the subquery ?
Aug 31 17:31:18 <xocolatl>	did you notice it's been fixed since 11.3?
Aug 31 17:31:31 <xocolatl>	I almost (but not quite) want to bisect it
Aug 31 17:32:11 <wkalt>	xocolatl: if you're talking to me, yeah it works fine in 11.5
Aug 31 17:33:31 <xocolatl>	wkalt: no, I was talking to incognito
Aug 31 18:10:18 <Alexthek1d>	hello
Aug 31 18:10:28 <Alexthek1d>	does postgresql come with a gui on board?
Aug 31 18:10:37 <pmjdebruijn>	not that I recall
Aug 31 18:10:42 <Alexthek1d>	or are GUIs just 3rd party?
Aug 31 18:10:46 <Alexthek1d>	okay
Aug 31 18:10:47 <pmjdebruijn>	afaik
Aug 31 18:11:01 <pmjdebruijn>	not sure what you want/need to gui for though
Aug 31 18:13:14 <Alexthek1d>	I don't know. i just want a database system that i can edit without queries.. Already looked up mongodb but it doesn't has this feature
Aug 31 18:13:35 <Alexthek1d>	in the end maybe what i'm looking for is just a excel table xD
Aug 31 18:16:56 <pmjdebruijn>	Alexthek1d: if you just need an excel table, why would even consider a real database?
Aug 31 18:17:30 <Alexthek1d>	pmjdebruijn, ye it just came to my mind
Aug 31 18:18:01 <Alexthek1d>	i make a stats-website and just have to save football players and their stats
Aug 31 18:18:10 <Alexthek1d>	editing would be easier with a gui then
Aug 31 18:18:30 <Alexthek1d>	so in the end i think that using a normal table is better
Aug 31 18:18:39 <pmjdebruijn>	Alexthek1d: wouldn't the admin backend of your website be the "gui" then
Aug 31 18:19:10 <pmjdebruijn>	anyhow, for a web application it will likely be a decent idea to have a db backend, unless it's only lookups
Aug 31 18:19:33 <pmjdebruijn>	if it's readonly, you might as well just place a static json file on your website, that gets read in my some javascript
Aug 31 18:20:14 <Alexthek1d>	pmjdebruijn, in the ends it would be read only and editing with an editor
Aug 31 18:20:19 <Alexthek1d>	pmjdebruijn, yes good idea
Aug 31 20:13:59 <daemon>	hey all I installed 9.4.24 and even though contrib appers to be installed ... hstore is still not installable
Aug 31 20:14:07 <daemon>	I 'make world' when I compiled it
Aug 31 20:14:14 <daemon>	is there some other requirements to include that extension?
Aug 31 20:14:26 <xocolatl>	don't install 9.4
Aug 31 20:14:29 <xocolatl>	install 11
Aug 31 20:14:45 <daemon>	xocolatl, I do not have a choice its an existing dataset and the requirements clearly state 9.4
Aug 31 20:14:50 <daemon>	something to do with something called postgis
Aug 31 20:15:09 <xocolatl>	then complain loudly.  9.4 is on the verge of being obsolete
Aug 31 20:15:19 <xocolatl>	postgis doesn't need such an old version
Aug 31 20:15:40 <daemon>	so any knowledge about hstore?
Aug 31 20:15:50 <xocolatl>	plenty
Aug 31 20:16:53 <daemon>	and how to make it available?
Aug 31 20:18:15 <xocolatl>	you're installing from self-compiled source?
Aug 31 20:18:19 <daemon>	yes
Aug 31 20:18:24 <RhodiumToad>	you said you did make world, did you also do make install-world when installing?
Aug 31 20:18:31 <xocolatl>	make install-world
Aug 31 20:18:41 <daemon>	RhodiumToad, I did not, just a plain install; that would be what I missed thank you :)
Aug 31 20:22:37 <xocolatl>	then once you get it running, upgrade to 11 and whatever the newest postgis is
Aug 31 20:25:43 <JamesB>	Hello. I am finding after pg_dumpall, that the restore misses whole tables, because I have triggers on some tables, and also it misses some functions, because they depend on other functions, etc.
Aug 31 20:25:47 <JamesB>	What can I do? This is a disaster..
Aug 31 20:26:39 <xocolatl>	JamesB: okay, this isn't consistent with what usually happens.  how are you restoring it?
Aug 31 20:26:50 <JamesB>	psql
Aug 31 20:26:55 <JamesB>	Is there any better option?
Aug 31 20:27:02 <JamesB>	after pg_dumpall
Aug 31 20:27:16 <xocolatl>	I'm asking for exact command lines
Aug 31 20:27:33 <JamesB>	psql -f backup.sql --username ... -d ...
Aug 31 20:27:39 <xocolatl>	psql just puts you in interactive mode, so that would certainly be a disaster if you're expecting a cluster to be restored
Aug 31 20:27:57 <JamesB>	It's just a single server. What's a better way then? All the documentation shows pg_dumpall followed by psql.
Aug 31 20:28:25 <xocolatl>	I don't understand what you're trying to do
Aug 31 20:28:32 <JamesB>	I'm trying to dump and restore to a new server.
Aug 31 20:28:47 <JamesB>	Upgrading from Postgres 9 to 11.
Aug 31 20:28:50 <xocolatl>	what are the complete commands you are running?
Aug 31 20:29:08 <JamesB>	pg_dumpall -f backup.sql -h (old server) -U (username)
Aug 31 20:30:11 <JamesB>	and then, psql -U (username) -d postgres -f backup.sql
Aug 31 20:35:21 <JamesB>	Would --disable-triggers help? It says "during data-only restore"
Aug 31 20:35:57 <xocolatl>	help what?
Aug 31 20:36:04 <JamesB>	It to actually restore properly?
Aug 31 20:36:14 <JamesB>	instead of copying half the tables and leaving the other half empty?
Aug 31 20:36:36 <xocolatl>	it restores everything properly
Aug 31 20:36:57 <RhodiumToad>	JamesB: what is the actual error output from psql
Aug 31 20:36:58 <JamesB>	No, it doesn't.
Aug 31 20:37:09 <xocolatl>	yes it does
Aug 31 20:37:18 <RhodiumToad>	also why -U username? dumpalls should be restored as the postgres user only
Aug 31 20:37:22 <JamesB>	Funny, I'm watching it not do it.
Aug 31 20:37:29 <JamesB>	Because I had renamed the postgres user on this server.
Aug 31 20:37:48 <RhodiumToad>	by "postgres user" I mean the db superuser, not necessarily the user named "postgres"
Aug 31 20:38:00 <JamesB>	Yes, I know.
Aug 31 20:38:20 <RhodiumToad>	so what is the error output?
Aug 31 20:38:53 <JamesB>	Thousands of them. I'm redumping with --disable-triggers to see if that helps, and then I'm going to initdb again, and then maybe I can tell you.
Aug 31 20:39:19 <xocolatl>	you don't want to restore your triggers?
Aug 31 20:39:26 <JamesB>	I do. I want to disable them while I am restoring.
Aug 31 20:39:30 <JamesB>	That's going to screw everything up.
Aug 31 20:39:33 <RhodiumToad>	JamesB: do not use --disable-triggers, it will do no good
Aug 31 20:39:39 <JamesB>	Great
Aug 31 20:39:41 <RhodiumToad>	just tell us what the errors are
Aug 31 20:39:57 <RhodiumToad>	if it helps, psql -v ON_ERROR_STOP=1  will stop at the first error
Aug 31 20:40:05 <JamesB>	Well that'd be a start
Aug 31 20:40:08 <JamesB>	But I need to redo the initdb first
Aug 31 20:40:09 <JamesB>	Just a sec
Aug 31 20:40:14 <JamesB>	because there's tons of crap there now.
Aug 31 20:40:53 <RhodiumToad>	the problem with text-mode dumps is that any error invariably causes a ton of fallout from following commands, usually whining about \N
Aug 31 20:41:01 <JamesB>	Yup
Aug 31 20:41:06 <RhodiumToad>	which is why you look at the first error and work forwards
Aug 31 20:41:25 <JamesB>	Is there any way to make the database not screwed up when it does it?
Aug 31 20:41:32 <JamesB>	I'd really rather it did the whole thing in a transaction if it can.
Aug 31 20:41:35 <RhodiumToad>	no
Aug 31 20:41:39 <JamesB>	Well that's terrible
Aug 31 20:41:40 <JamesB>	ok
Aug 31 20:41:43 <JamesB>	Going to initdb yet again
Aug 31 20:41:52 <RhodiumToad>	can't really do that because dumpall uses create database which isn't allowed in transactions
Aug 31 20:42:14 <RhodiumToad>	this is why people usually prefer pg_dumpall -r and individual pg_dump commands on each db
Aug 31 20:42:28 <xocolatl>	or pg_upgrade
Aug 31 20:42:43 <JamesB>	It's a separate server. I can't do pg_upgrade.
Aug 31 20:46:17 <JamesB>	Well, the first error is
Aug 31 20:46:20 <JamesB>	that the role already exists
Aug 31 20:46:26 <JamesB>	for the admin user
Aug 31 20:46:38 <JamesB>	ANy way to go to the second error?
Aug 31 20:46:50 <JamesB>	That's just a blatantly obvious error
Aug 31 20:46:58 <RhodiumToad>	unfortunately not. pipe the error output into a file or pager
Aug 31 20:47:10 <RhodiumToad>	or edit the dump to make the first error go away
Aug 31 20:47:16 <JamesB>	That'd be 2>test.txt or somesuch?
Aug 31 20:47:21 <RhodiumToad>	yes
Aug 31 20:47:28 <RhodiumToad>	or psql ... 2>&1 | more
Aug 31 20:47:54 *	RhodiumToad makes a bet with himself
Aug 31 20:48:16 <RhodiumToad>	my bet is fe60dbac172e35be65ac84eb0be424d1cabf32e7c8e3ad133595ac8cb303420c
Aug 31 20:48:49 <JamesB>	Functio does not exist
Aug 31 20:48:51 <JamesB>	is the first one
Aug 31 20:48:55 <RhodiumToad>	what function?
Aug 31 20:49:13 <JamesB>	psql:/root/backup2.sql:3256617: ERROR:  function check_order_code(text) does not exist
Aug 31 20:49:13 <JamesB>	LINE 1: SELECT check_order_code(x) or check_age_code_long(x)
Aug 31 20:49:13 <JamesB>	               ^
Aug 31 20:49:13 <JamesB>	HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
Aug 31 20:49:13 <JamesB>	QUERY:  SELECT check_order_code(x) or check_age_code_long(x)
Aug 31 20:49:14 <JamesB>	CONTEXT:  PL/pgSQL function public.check_age_code(text) line 1 at RETURN
Aug 31 20:49:21 <JamesB>	It's one of mine. It should have dumped it, I'd think...
Aug 31 20:49:41 <JamesB>	After that, psql:/root/backup2.sql:3561347: ERROR:  function base32_char_regex() does not exist
Aug 31 20:49:41 <JamesB>	LINE 2:   select exists(select regexp_matches($1, '^' || base32_char...
Aug 31 20:49:41 <JamesB>	                                                         ^
Aug 31 20:49:41 <JamesB>	HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
Aug 31 20:49:41 <JamesB>	QUERY:
Aug 31 20:49:42 <JamesB>	  select exists(select regexp_matches($1, '^' || base32_char_regex() || '{10}$'));
Aug 31 20:49:47 <RhodiumToad>	use a paste site!
Aug 31 20:50:17 <JamesB>	I would, but I am going to be back in a few hours. I thought this would be a quick affair, but it's my son's birthday today, so all of our servers are just going to have to be down. I dunno, very frustrating
Aug 31 20:50:25 <RhodiumToad>	this is probably the search_path issue.
Aug 31 20:50:33 <RhodiumToad>	are these functions used in CHECK constraints?
Aug 31 20:50:34 <JamesB>	Oh?
Aug 31 20:50:36 <JamesB>	Yes probably so.
Aug 31 20:50:54 <JamesB>	The server I am connecting to to pg_dumpall is running 9.4 if it helps
Aug 31 20:50:58 <JamesB>	Is this fixable by altering the old tables?
Aug 31 20:51:06 <RhodiumToad>	what pg_dumpall version did you use?
Aug 31 20:51:27 <RhodiumToad>	oh wait that wouldn't help
Aug 31 20:51:37 <JamesB>	Yes, check_age_code is called in a check constraint.
Aug 31 20:51:43 <JamesB>	I will be back later, have to go.
Aug 31 20:51:58 <RhodiumToad>	the problem is that your functions call other functions assuming that "public" is on the search path, and for dubious security reasons that's no longer the case during restore
Aug 31 20:52:07 <JamesB>	....um
Aug 31 20:52:19 <JamesB>	So, all functions have to be reworked to call public.function_this_that?
Aug 31 20:52:25 <RhodiumToad>	so in the old db, you could use ALTER FUNCTION ... SET search_path =   to fix it
Aug 31 20:52:58 <JamesB>	search_path to blank? I don't understand, why would that fix it?
Aug 31 20:53:00 <JamesB>	brb pizza to oven
Aug 31 20:53:12 <RhodiumToad>	if you use public for everything, then  alter function ... set search_path = public;   should do it if you do it on all plpgsql funcs that call other funcs
Aug 31 21:11:20 <syf_z_otchlani>	hello
Aug 31 21:11:36 <syf_z_otchlani>	i have a problem connecting from docker to postgres
Aug 31 21:11:45 <JamesB>	So is there no explicit search_path,
Aug 31 21:11:51 <JamesB>	it no longer includes public, is the basic issue?
Aug 31 21:12:16 <syf_z_otchlani>	No pg_hba.conf entry for host "192.168.1.59", user "jakubsadowski", database "adventureworks", ssl off.
Aug 31 21:12:44 <syf_z_otchlani>	i set host all all 0.0.0.0/0 trust
Aug 31 21:12:53 <syf_z_otchlani>	in ph_hba.conf
Aug 31 21:12:57 <RhodiumToad>	JamesB: while executing the restore, public is not in the search path
Aug 31 21:13:02 <JamesB>	All of the functions that call other f unctions, or all of the functions that call other functions, and are in CHECK statements?
Aug 31 21:13:04 <syf_z_otchlani>	so what is blocking me?
Aug 31 21:13:16 <syf_z_otchlani>	TIA for help..
Aug 31 21:13:16 <JamesB>	Is there any way I could put public in the search path during restore?
Aug 31 21:15:17 <RhodiumToad>	JamesB: not conveniently
Aug 31 21:15:42 <RhodiumToad>	syf_z_otchlani: did you reload after changing the file?
Aug 31 21:15:59 <JamesB>	Aha. I could change backup.sql
Aug 31 21:16:02 <JamesB>	It has a set_config search path line..
Aug 31 21:16:11 <RhodiumToad>	syf_z_otchlani: but seriously don't use that line, you don't want to open your db up to the whole world, people do scan for and exploit open dbs
Aug 31 21:16:24 <RhodiumToad>	JamesB: probably more than one?
Aug 31 21:16:59 <RhodiumToad>	JamesB: I guess you could edit that (do check whether there's just one or many of them)
Aug 31 21:18:29 <syf_z_otchlani>	RhodiumToad: yes, restarted the server
Aug 31 21:18:55 <JamesB>	So is this literally all of my functions that call any other functions basically?
Aug 31 21:18:56 <syf_z_otchlani>	this is a home computer behind NAT
Aug 31 21:19:19 <RhodiumToad>	syf_z_otchlani: are you editing the correct file? you can check with  show hba_file;  and  select * from pg_hba_file_rules;
Aug 31 21:22:27 <RhodiumToad>	JamesB: editing all set_config('search_path'...) instances in the dump should be enough.
Aug 31 21:22:30 <syf_z_otchlani>	          82 | host  | {all}         | {all}     | 127.0.0.1 | 255.255.255.255                         | md5         |         |
Aug 31 21:22:45 <RhodiumToad>	syf_z_otchlani: don't paste in channel, use a paste site
Aug 31 21:23:07 <syf_z_otchlani>	but interesting fact is that I just have 4 lines in my config file..
Aug 31 21:23:24 <syf_z_otchlani>	and this one shows this config comes from 82th line..??
Aug 31 21:23:43 <syf_z_otchlani>	anyway it should accept ANY user connecting ANY database
Aug 31 21:23:43 <RhodiumToad>	you're looking at the wrong file then, check the  show hba_file;  output
Aug 31 21:23:48 <myrkraverk>	syf_z_otchlani: this line has nothing to do with 192.168.1.59
Aug 31 21:23:51 <syf_z_otchlani>	shouldn't it?
Aug 31 21:24:05 <RhodiumToad>	that line you quoted only matches connections from 127.0.0.1
Aug 31 21:24:16 <RhodiumToad>	not from 192.168.1.59
Aug 31 21:24:31 <syf_z_otchlani>	uh, right that's 32 bit mask
Aug 31 21:25:16 <JamesB>	RhodiumToad: Is there any larger consequence of that though? Aren't those lines to improve the database security?
Aug 31 21:25:25 <JamesB>	I'm guessing it changes the default security, yes?
Aug 31 21:25:29 <JamesB>	default search path,r ather
Aug 31 21:26:36 <JamesB>	By the way, is there any reason to have pg_temp in the search path at all? I notice I did that in a few. Is it still pg_temp at the start by default if it is not listed?
Aug 31 21:27:04 <RhodiumToad>	you should not put it in the path explicitly.
Aug 31 21:27:16 <RhodiumToad>	it is added at the front if it exists
Aug 31 21:27:33 <RhodiumToad>	should not usually put it in the path, that is
Aug 31 21:28:24 <JamesB>	Hmm. I thought with earlier Postgres, if it is not listed (say, at the end), it went at the front.
Aug 31 21:32:00 <JamesB>	It would be nice if ALTER FUNCTION didn't require parameters if the function weren't overloaded
Aug 31 21:32:38 <myrkraverk>	JamesB: I guess that's something you get to add yourself to alter function.
Aug 31 21:48:00 <JamesB>	All right. I hope that is all of the functions.
Aug 31 21:48:11 <JamesB>	I guess it's good to know this before I try upgrading my really big database
Aug 31 21:48:18 <JamesB>	This one at least is only a few gigs
Aug 31 21:52:53 <JamesB>	So far no other errors.
Sep 02 08:51:42 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep  2 08:51:42 2019

**** BEGIN LOGGING AT Mon Sep  2 08:52:08 2019

Sep 02 08:52:08 *	Now talking on #postgresql
Sep 02 08:52:08 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 02 08:52:08 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 02 08:52:09 *	Channel #postgresql url: https://www.postgresql.org
Sep 02 09:05:49 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep  2 09:05:49 2019

**** BEGIN LOGGING AT Mon Sep  2 09:06:12 2019

Sep 02 09:06:12 *	Now talking on #postgresql
Sep 02 09:06:12 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 02 09:06:12 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 02 09:06:12 *	Channel #postgresql url: https://www.postgresql.org
Sep 02 09:09:25 <alireza>	How can I do a three way join update faster?
Sep 02 09:09:27 <alireza>	http://sqlfiddle.com/#!17/28d65
Sep 02 09:13:02 <alireza>	it's about 400k of records
Sep 02 09:13:19 <alireza>	i'm just curios if there's a faster way to update recorsd
Sep 02 09:13:34 <alireza>	err, records that need other tables informatino (join)
Sep 02 09:19:16 <Widdershins>	joins for updates aren't enormously different than joins for read as i understand it
Sep 02 09:22:01 <incognito>	alireza: try create unique index, you can also defer the constraint check
Sep 02 09:25:28 <photonios>	Hi everyone! I have a patch that adds a hook to copydir().. I looked a bit at how other hooks are implemented and I can't find any tests for other hooks.. Is that correct or I am I just blind?
Sep 02 09:26:03 <incognito>	alireza : if you make your column like that, it should do what i said : id serial primary key deferrable initially deferred, ....
Sep 02 09:37:54 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep  2 09:37:54 2019

**** BEGIN LOGGING AT Mon Sep  2 09:38:17 2019

Sep 02 09:38:17 *	Now talking on #postgresql
Sep 02 09:38:17 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 02 09:38:17 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:06 2019)
Sep 02 09:38:17 *	Channel #postgresql url: https://www.postgresql.org
Sep 02 11:11:27 <ysch>	??dont
Sep 02 11:11:27 <pg_docbot>	https://wiki.postgresql.org/wiki/Don%27t_Do_This
Sep 02 11:12:01 <Aram>	Hi, so I have this query: http://ix.io/1U2J which uses GROUP BY to count things.
Sep 02 11:12:04 <Aram>	analyze: http://ix.io/1U1F
Sep 02 11:12:06 <Aram>	as you can see, it's quite slow.
Sep 02 11:12:09 <Aram>	since this has to iterate through every row in that range, I assume there's not much I can do to make it faster?
Sep 02 11:12:19 <Aram>	sorry, analyze link: https://explain.depesz.com/s/ExRB
Sep 02 11:16:29 <Aram>	I also have an index on author, though I don't think that would help, as iterating by author would mean random i/o instead of linear i/o (and there are many authors)
Sep 02 11:18:59 <Myon>	Aram: you need an index on messages(date)
Sep 02 11:19:14 <Aram>	I have that
Sep 02 11:19:56 <Myon>	1.6M messages from that timestamp, wtf
Sep 02 11:20:05 <Aram>	yeah
Sep 02 11:20:36 <Myon>	well, you'll have to scan everything with that date, so there's little room for optimization
Sep 02 11:20:44 <Aram>	so I thought
Sep 02 11:23:26 <Aram>	I could get the TOTAL number of messages per author from the index, but not restricted to a particular date range, no?
Sep 02 11:24:03 <Aram>	also, I have both author, and author_id, which is a bigint instead of a string. I expected that to be faster, but no, it's eaxactly the same.
Sep 02 11:25:14 <Aram>	it's about ~5% faster
Sep 02 11:25:14 <incognito>	Aram can you show the index on messages(date) ? \d+ messages
Sep 02 11:25:43 <Aram>	yes: http://ix.io/1U2L
Sep 02 11:26:00 <Aram>	oops, let me run \d+
Sep 02 11:26:13 <Aram>	http://ix.io/1U2M
Sep 02 11:28:55 <incognito>	Aram how much rows do you have in this table (approximation) ?
Sep 02 11:29:44 <Aram>	7.80928e+06
Sep 02 11:31:41 <nickb>	I'd argue that if you need the number of messages per author you should keep track of it instead of running a query like that
Sep 02 11:32:11 <Aram>	I suppose so, yes.
Sep 02 11:32:36 <nickb>	also partitioning in on date in that kind of setup will make life a lot easier
Sep 02 11:32:41 <nickb>	s/in//
Sep 02 11:33:32 <vedu_>	Hello. Probably a silly question: I get `syntax error at or near "ORDER"` for `UPDATE core_job SET is_part_time=true ORDER BY random() LIMIT 1000;`
Sep 02 11:33:40 <nickb>	Aram: is that _the_ Discord, or do you just have some sort of discord bot that also has that data around? (you don't have to answer)
Sep 02 11:33:54 <nickb>	vedu_: you can't `order` in update
Sep 02 11:34:05 <Aram>	the latter. we have a discord bot on some large servers
Sep 02 11:34:05 <incognito>	can you try : WHERE  date BETWEEN '2018-09-01T23:52:55.355' AND '2019-09-01T23:52:55.325' ?
Sep 02 11:34:23 <incognito>	without the Z
Sep 02 11:34:31 <Aram>	let me see
Sep 02 11:34:36 <vedu_>	nickb: Ohh. Subquery then :)
Sep 02 11:34:37 <incognito>	explain only
Sep 02 11:34:57 <nickb>	vedu_: what are you trying to achieve?
Sep 02 11:35:13 <nickb>	vedu_: in that context your order by doesn't make any sense
Sep 02 11:35:57 <vedu_>	nickb: update 1000 rows but not first 100
Sep 02 11:36:01 <Aram>	incognito: without the Z: https://explain.depesz.com/s/6NPy
Sep 02 11:36:05 <Aram>	not much difference
Sep 02 11:36:08 <vedu_>	s/100/1000
Sep 02 11:37:01 <[patrik]>	why, oh why?
Sep 02 11:37:07 <nickb>	vedu_: ah.. Yeah you want `UPDATE core_job c SET is_part_time=true FROM (SELECT id FROM core_job ORDER BY random() LIMIT 1000) AS s WHERE c.id=s.id`
Sep 02 11:37:16 <nickb>	vedu_: I hope I didn't mess up the syntax
Sep 02 11:37:57 <vedu_>	UPDATE core_job SET is_part_time=true WHERE ID IN (SELECT id FROM core_job ORDER BY random() LIMIT 1000);
Sep 02 11:38:01 <vedu_>	nickb: I did this ^
Sep 02 11:38:20 <nickb>	compare the plans with EXPLAIN UPDATE ...
Sep 02 11:38:33 <nickb>	I'm pretty sure your version is going to be bad
Sep 02 11:38:51 <vedu_>	nickb: :')
Sep 02 11:38:52 <incognito>	Aram: when you : explain select author_id from messages where date BETWEEN  '2018-09-01 23:52:55.325' AND  '2019-09-01 23:52:55.325' ?
Sep 02 11:40:04 <nickb>	??loose indexscan
Sep 02 11:40:04 <pg_docbot>	http://wiki.postgresql.org/wiki/Loose_indexscan
Sep 02 11:40:51 <incognito>	Aram: does it seqscan or indexscan ?
Sep 02 11:41:07 <Aram>	"Seq Scan on messages  "
Sep 02 11:42:21 <nickb>	one thing I'd check is if its (a) even possible to use the index (which it should be) and (b) if index would make it faster; So I'd try `set enable_seqscan = off` and run the explain again to see what happens
Sep 02 11:42:54 <incognito>	you said 7M rows, but according to your 1st explain: the filter only removes 913k, and retrieve 1.6M
Sep 02 11:43:16 <Aram>	because of the date range, I think
Sep 02 11:43:19 <nickb>	Aram: also, do you run regular VACUUMs on the table?
Sep 02 11:44:30 <Aram>	I don't think vacuum ever ran, let me check.
Sep 02 11:44:53 <Aram>	but this db is written in date order and nothing is ever deleted or updated.
Sep 02 11:45:03 <Aram>	yeah, no vacuum
Sep 02 11:45:10 <xocolatl>	then you should certainly cron regular vacuums
Sep 02 11:45:23 <nickb>	ok so if you run VACUUM ANALYZE on the table, there is a good chance you can get an index-only scan
Sep 02 11:45:38 <nickb>	you might need to rewrite the query as `count(*)` instead of `count(id)`
Sep 02 11:45:43 <incognito>	+1 only to gather good stats
Sep 02 11:46:02 <nickb>	(not sure if postgres checks the column for NOT NULL and rewrites count(id) as count(*))
Sep 02 11:46:04 <Aram>	tried count(*) instead of count(*), but it's very slightly slower.
Sep 02 11:46:11 <xocolatl>	incognito: no, ANALYZE will still run.  VACUUM is needed for the VM
Sep 02 11:46:12 <nickb>	after VACUUM?
Sep 02 11:46:31 <nickb>	Aram: ^
Sep 02 11:46:34 <xocolatl>	what's the link to the query we're optimizing?
Sep 02 11:46:41 <Aram>	no. I just ran vacuum
Sep 02 11:46:46 <Aram>	let me run the query again
Sep 02 11:46:48 <incognito>	xocolatl: https://explain.depesz.com/s/6NPy
Sep 02 11:46:55 <nickb>	xocolatl: http://ix.io/1U2J http://ix.io/1U2M https://explain.depesz.com/s/ExRB
Sep 02 11:47:56 <Aram>	ok, so after vacuum it's doing "Index Only Scan "
Sep 02 11:47:57 <nickb>	Aram: one more thing I wanted to point out is you don't really need all those indexes. Postgres can use an index on (a,b) to filter on (a), so you can drop a few of those
Sep 02 11:48:01 <Aram>	it's not faster though
Sep 02 11:48:10 <Aram>	nickb: ah yeah, good point
Sep 02 11:48:12 <nickb>	Aram: its not faster because it has to scan the entire table
Sep 02 11:48:12 <incognito>	explain SELECT
Sep 02 11:48:12 <incognito>	  author,
Sep 02 11:48:12 <incognito>	  count(id)
Sep 02 11:48:12 <incognito>	FROM (select * from messages WHERE date BETWEEN '2018-09-01T23:52:55.325Z' AND '2019-09-01T23:52:55.325Z') m
Sep 02 11:48:12 <incognito>	GROUP BY author
Sep 02 11:48:13 <incognito>	ORDER BY count(id) DESC
Sep 02 11:48:13 <incognito>	LIMIT 5;
Sep 02 11:48:15 <incognito>	oups
Sep 02 11:48:19 <Aram>	yeah
Sep 02 11:48:34 <nickb>	incognito: please don't paste into the channel
Sep 02 11:49:01 <incognito>	nickb: sorry, i typed without checking in which window i am
Sep 02 11:49:09 <photonios>	Re-posting: Hi everyone! I have a patch that adds a hook to copydir().. I looked a bit at how other hooks are implemented and I can't find any tests for other hooks.. Is that correct or I am I just blind?
Sep 02 11:49:11 <xocolatl>	this table is... not normalized
Sep 02 11:49:13 <nickb>	Aram: but its as good as it gets. You won't get a faster plan for this query
Sep 02 11:49:19 <Aram>	yeah, I imagined
Sep 02 11:49:20 <julius>	ive tried pgmodeler as recommended here, if the database does exist and you choose "drop db" from the connection dialog it says: cannot drop the currently open db.   -  disconnection all users with; https://stackoverflow.com/questions/36502401/postgres-drop-database-error-pq-cannot-drop-the-currently-open-database    does nothing for pgmodeler.  if you delete the database and export again: new_database does not exist. please remove this tool from your
Sep 02 11:49:20 <julius>	recommendations!
Sep 02 11:49:45 <Myon>	photonios: C-level things are hard to test
Sep 02 11:50:06 <Aram>	nickb: after sorting, the hashing can be done in parallel, no? will more cores help?
Sep 02 11:50:15 <xocolatl>	Aram: what's the new plan?
Sep 02 11:50:30 <photonios>	Myon: Ye they are. I read the guide on submitting patches (its my first) and not adding test is a quick way to get your patch rejected. Figured I should ask first before submitting
Sep 02 11:50:33 <Myon>	photonios: there's a regress.so that contains some things, but probably not very extensive
Sep 02 11:50:44 <nickb>	Aram: sorting is done _after_ the heavy part. Yeah post explain analyze of current plan
Sep 02 11:50:57 <Aram>	current plan: https://explain.depesz.com/s/ef0w
Sep 02 11:51:24 <nickb>	Aram: huh? you said it does Index-only scan?
Sep 02 11:51:32 <Myon>	photonios: src/test/regress/regress.c
Sep 02 11:52:23 <photonios>	Myon: Thanks! I'll try to whip up a simple test there
Sep 02 11:52:48 <Aram>	nickb: sorry, I confused myself, I does index only scan for another query without the WHERE (whereas before it did seq scan for that). this query doesn't seem to have changed before and after.
Sep 02 11:53:19 <nickb>	yeah, that kinda makes sense
Sep 02 11:53:20 <Aram>	this is the important query, so we can forget about the other one
Sep 02 11:54:13 <nickb>	ok so my point stands, if this is an important query that you want to run regularly, you're better off maintaining a per-user count as part of inserting a new message into `messages`
Sep 02 11:54:27 <incognito>	Aram: can you also try : select author_id, count(1) filter(where date between ... and ...) from messages group by author_id order by 2 desc?
Sep 02 11:54:37 <Myon>	if you want a per-user count, why are you restricting by date?
Sep 02 11:54:49 <nickb>	Aram: also I agree with what xocolatl said, having `author` and `author_id` seems dubious
Sep 02 11:55:43 <Aram>	well author can change, although for unrelated reasons it doesn't change for our bot.
Sep 02 11:56:00 <xocolatl>	but author_id doesn't change with it?
Sep 02 11:56:05 <Aram>	no, that should stay fixed
Sep 02 11:56:10 <xocolatl>	eww
Sep 02 11:56:27 <Aram>	author is just a name that the user can change.
Sep 02 11:56:29 <nickb>	ah, discord allows that yeah
Sep 02 11:56:40 <xocolatl>	so you have messages from different authors with the same id
Sep 02 11:56:46 <nickb>	anyway, what resolution do you need on message counts?
Sep 02 11:57:13 <Aram>	xocolatl: no, that should never happen.
Sep 02 11:57:16 <nickb>	xocolatl: author is actually "name" which isn't unique. This is denormalized still, but not too bad
Sep 02 11:57:37 <xocolatl>	so when the name changes you update all the rows?
Sep 02 11:58:13 <Aram>	nickb: well for recent queries, like last hour or day, we need exact resolution. for larger time span queries, like last year, resolution is not so important. probably 3 digit of resolution is enough.
Sep 02 11:59:14 <nickb>	Aram: I'd propose you add daily totals per author and then do `SELECT author, sum(cnt) FROM author_digest WHERE date ... ORDER BY 2 DESC LIMIT 5`
Sep 02 12:00:10 <nickb>	or partition `messages` and run summarize on finalized partitions so that you don't have to count them and count only _live_ partitions
Sep 02 12:03:58 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep  2 12:03:58 2019

**** BEGIN LOGGING AT Mon Sep  2 12:04:24 2019

Sep 02 12:04:24 *	Now talking on #postgresql
Sep 02 12:04:24 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 02 12:04:24 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 02 12:04:25 *	Channel #postgresql url: https://www.postgresql.org
Sep 02 12:04:46 <incognito>	this one, a little
Sep 02 12:05:04 <incognito>	Aram: do you have more threads available for this server ?
Sep 02 12:05:23 <Aram>	if that helps, I can probably make another VM
Sep 02 12:05:32 <Aram>	at the moment no
Sep 02 12:05:42 <xocolatl>	a brin index on date *might* help
Sep 02 12:06:51 <nickb>	keep in mind that BRIN index requires manual maintenance in a heavily append-dominated _database_
Sep 02 12:07:53 <xocolatl>	I think that was fixed recently
Sep 02 12:08:53 <ccoffey>	@nickb oh, maybe I mean sequences. i.e.   ```pg_restore: [archiver (db)] Error from TOC entry 4904; 0 0 SEQUENCE OWNED BY <name>_seq some_user    pg_restore: [archiver (db)] could not execute query: ERROR:  must be owner of relation <name>_seq ```
Sep 02 12:09:45 <nickb>	xocolatl: last I checked `summarize` was run by autovacuum worker connected to the $database
Sep 02 12:10:18 <xocolatl>	nickb: correct, but I seem to remember something being done about that after I pointed it out
Sep 02 12:11:07 <xocolatl>	nickb: 7526e10224f0792201e99631567bbe44492bbde4
Sep 02 12:14:28 <nickb>	xocolatl: yeah thats cool, but I also read it as: you still need autovacuum worker to start somehow for the summarization to happen.
Sep 02 12:14:53 <xocolatl>	maybe
Sep 02 12:14:56 <xocolatl>	I'd have to study it closer
Sep 02 12:15:01 <nickb>	its just an update of the algorithm of listing the pages that need to be summarized. I'll check the code later, thanks for the pointer
Sep 02 12:15:40 <nickb>	coffee: can you show your restore command?
Sep 02 12:16:09 <nickb>	ooops
Sep 02 12:16:12 <nickb>	ccoffey: ^
Sep 02 12:23:27 <ccoffey>	@nickb I'll get a better error, I should have been more prepared before I came in here. I was trying to use the AWS DMS system to load the data, but that's a different error. Before starting I create a DB like so: ```pg_restore -U postgres -h myrds.rds.amazonaws.com --dbname=web_db_synced --schema-only -C ../synced/web_db_synced.schema.sql --verbose ``` I'm running the command as an rds_superuser but I'll get errors
Sep 02 12:23:27 <ccoffey>	like similar to ```pg_restore: [archiver (db)] could not execute query: ERROR:  must be owner of relation clientemailtemplates_id_seq    Command was: ALTER SEQUENCE clientemailtemplates_id_seq OWNED BY cientemailtemplates.id;```
Sep 02 12:33:48 <ccoffey>	Here's an easier example. I re-ran it DMS with drop tables and re-create, as an rds_superuser, but it can't drop the tables it doesn't own. I created these tables as the same user I am using now. I clearly don't understand how permissions work in RDS `Failed to drop table public.datajob_upload`  Do I need to run the RDS database with a different parameter group?  I'm am using the "default.postgres9.6" group
Sep 02 13:02:17 <Moonsilence>	Hi! Is it possible to dump and restore data within a single transaction? I want to dump data, drop and recreate some tables, then restore the data. I have an idea that within a psql session, I could use \copy..to to put the data into files, do my ddl stuff, then generate the corresponding \copy..from commands. This seems a bit finicky, hence my question if there is a simpler way?
Sep 02 13:03:36 <nickb>	Moonsilence: `pg_dump --clean .... | psql -1` should do the trick?
Sep 02 13:56:00 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep  2 13:56:00 2019

**** BEGIN LOGGING AT Mon Sep  2 13:56:26 2019

Sep 02 13:56:26 *	Now talking on #postgresql
Sep 02 13:56:26 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 02 13:56:26 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 02 13:56:26 *	Channel #postgresql url: https://www.postgresql.org
Sep 02 14:13:36 <ufk>	hi! :) i loaded auto_explain, set log_nested_statements to on, log_analyze to true and log_min_duration to zero and when I explain a select from a function that executes a dynamic query, i don't see any deep info in the postgresql logs. any ideas ?
Sep 02 14:15:51 <RhodiumToad>	loaded it how?
Sep 02 14:16:12 <RhodiumToad>	and ran the query how?
Sep 02 14:18:25 <ufk>	ok.... first.. the logs are not empty, i do see function scan in the logs, but it doesn't drill down to the actual query. I connect with psql and run: LOAD 'auto_explain'; SET auto_explain.log_nested_statements = ON;SET auto_explain.log_analyze = true;set auto_explain.log_min_duration=0; explain select * from my_function(my_params)"
Sep 02 14:19:43 <RhodiumToad>	you have to actually run the function not explain it
Sep 02 14:19:53 <RhodiumToad>	i.e.  select * from my_function(my_params);
Sep 02 14:20:32 <RhodiumToad>	btw, you can do  set client_min_messages='log';  and you'll get the log output in psql as well
Sep 02 14:25:19 <ufk>	oh nice
Sep 02 14:25:26 <ufk>	thanks
Sep 02 14:26:06 <iamyojimbo>	Hi all, question about how I could/should be using postgres. Say for example I have a stream of RMQ messages of JSON format coming in (around 20/second). These messages describe create and update events on certain entities from a data provider. Would it be insane to just dump all those messages as JSONB messages to a single table and then use only
Sep 02 14:26:06 <iamyojimbo>	views and JSON indexing to look at the data.
Sep 02 14:27:11 <iamyojimbo>	or should I be doing some ETL before on my data and storing it in a relational way. I do need see a live view of the latest data as up to date as possible.
Sep 02 14:29:16 <fabian__>	Hello, if I try to select data from a view, I get  "permission denied for relation" error. But if I run the query that was used to define the view, it works. What could be the reason for that?
Sep 02 14:31:36 <xocolatl>	fabian__: you don't have select privileges on the view
Sep 02 14:31:50 <xocolatl>	grant select on the_view to the_user;
Sep 02 14:33:33 <RhodiumToad>	either that or the view owner does not have select privileges on the table
Sep 02 14:35:43 <fabian__>	RhodiumToad: THX!!! the user had select privileges on the table, but not the view owner
Sep 02 14:42:23 *	xocolatl wonders why relation_needs_vacanalyze() doesn't check for pending work requests
Sep 02 14:53:22 <ufk>	can I somehow dump only index  creation for a specific table ?
Sep 02 14:54:08 <dim>	use pg_dump -Fc format, and then use pg_restore -l and -L to list the objects from the dump and filter out non indexes from the list, and then use the new list with pg_restore
Sep 02 14:54:40 <dim>	note that you can pg_dump -Fc --schema-only, so that you don't have to dump the whole data set in your case
Sep 02 14:54:43 <ufk>	sounds simple enough :)
Sep 02 14:56:04 <xocolatl>	or select pg_get_indexdef('indexname');
Sep 02 14:57:15 <ufk>	ohh that's nicer
Sep 02 15:37:49 <dim>	ah yeah good point xocolatl ; then you can to a catalog query and use \gexec IIRC
Sep 02 15:38:54 <dim>	(well I guess you're recreating the index on another system so maybe \gexec is not useful here)
Sep 02 15:57:48 <Moonsilence>	Hi! How do I obtain a single value as a block of text from a query yielding one column of text with multiple rows? I want to store that "block of text" into a psql variable.
Sep 02 16:00:36 <ilmari>	Moonsilence: you mean you want to combine a single column from multiple rows into a single string?
Sep 02 16:00:48 <azeem>	if my username has a @ in it, can I use a 'psql postgres://[...]' type connection string and how to escape the @? It seems the part afterwards always gets interpreted as the hostname
Sep 02 16:01:05 <ilmari>	Moonsilence: string_agg()
Sep 02 16:01:07 <Moonsilence>	ilmari, yes exactly. ah, think I got it with string_agg(mytextcol, E'\n')
Sep 02 16:01:10 <Moonsilence>	:)
Sep 02 16:01:13 <ilmari>	azeem: %40
Sep 02 16:01:22 <ilmari>	(standard URI escaping)
Sep 02 16:10:56 <azeem>	ilmari: thx!
Sep 03 10:14:12 *	Disconnected ()
**** ENDING LOGGING AT Tue Sep  3 10:14:12 2019

**** BEGIN LOGGING AT Tue Sep  3 10:14:37 2019

Sep 03 10:14:37 *	Now talking on #postgresql
Sep 03 10:14:37 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 03 10:14:37 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 03 10:14:38 *	Channel #postgresql url: https://www.postgresql.org
Sep 03 13:41:13 <pstef_>	"pg_dump -S x --disable-triggers --data-only" will generate SET SESSION AUTHORIZATION 'x' -- but I'm missing the point of it. The assumption is that x is a superuser, but in order to SET SESSION AUTHORIZATION you already have to be a superuser role
Sep 03 13:41:55 <pstef_>	with my limited understanding, it would have been much more useful to me if the line generated was SET ROLE ...
Sep 03 13:42:11 <RhodiumToad>	none of those options are really any use
Sep 03 13:42:43 <RhodiumToad>	SET SESSION AUTHORIZATION is historical, pg_dump used to use that for everything before it was changed to use ALTER ... OWNER TO ...
Sep 03 13:42:47 <pstef_>	a non-superuser would be able to SET ROLE into a superuser, so that would be a bit of help
Sep 03 13:42:55 <RhodiumToad>	uh, no?
Sep 03 13:43:45 <pstef_>	why not? I use a role like that all the time. It's not a superuser until I do SET ROLE superuser; (which has been granted to my personal database role)
Sep 03 13:48:13 <RhodiumToad>	that's not any different from a security perspective than making your personal role a superuser
Sep 03 13:51:15 <adsf>	RhodiumToad ilmari found the issue with copy i was having yesterday. I ran csvclean from csvkit and it found null byte chars. Replaced them all with tr and copy worked :)
Sep 03 13:51:50 <RhodiumToad>	hm
Sep 03 13:51:59 <RhodiumToad>	I'd have expected a different error for that
Sep 03 13:52:46 <adsf>	command with tr was tr < file -d '\000' > outfile
Sep 03 13:53:41 <adsf>	this morning i exported the single row and re-saved it in us-ascii, which also seemed to work for some reason
Sep 03 14:21:14 <pstef_>	RhodiumToad: sorry, I was distracted at work. So I think it is a bit different as being non-superuser by default at least prevents me from making mistakes. besides, this is an application role, which it doesn't need to be a superuser most of the time - it only needs the ability to disable system triggers (for foreign keys)
Sep 03 15:06:06 <Zaab1t>	hello hackers. What's the difference between having all privileges and being the owner of a database?
Sep 03 15:06:25 <tangara>	i need help to know why psql can't find my table
Sep 03 15:06:37 <tangara>	i am using postgres as user
Sep 03 15:07:01 <tangara>	postgres=# \copy oldmembers from 'd:\memberparticulars.csv' csv header;ERROR:  relation "oldmembers" does not exist
Sep 03 15:07:55 <mobidrop>	is the table there?
Sep 03 15:09:06 <hruske>	tangara: you seem to be connected to postgres database, I am guessing your data is not in that database
Sep 03 15:09:38 <hruske>	tangara: \l to see all databases and \c databasename to switch psql client to use that
Sep 03 15:09:40 <tangara>	i follow this tutorial to create a table first before I do the copying
Sep 03 15:09:49 <tangara>	https://popsql.com/learn-sql/postgresql/how-to-import-a-csv-in-postgresql/
Sep 03 15:10:19 <tangara>	i am using psql in windows run batch
Sep 03 15:10:22 <Myon>	Zaab1t: only the owner can drop the database
Sep 03 15:10:22 <tangara>	not linux
Sep 03 15:11:01 <Myon>	Zaab1t: and only the owner can grant privileges (see also: WITH GRANT OPTION)
Sep 03 15:11:10 <hruske>	tangara: sure, all this should still work
Sep 03 15:12:00 <tangara>	hey I used \l it shows me all the database
Sep 03 15:12:12 <tangara>	it shows me the database schema name without the table names
Sep 03 15:12:37 <Zaab1t>	Myon: I tried `alter database mydb set enable_sort to on;` and got "ERROR: must be owner of database mydb"
Sep 03 15:12:44 <tangara>	postgres=# \copy membership.oldmembers from 'd:\memberparticulars.csv' csv header;ERROR:  schema "membership" does not exist
Sep 03 15:12:58 <tangara>	it shows me membership so why it is giving me error still?
Sep 03 15:13:16 <hruske>	tangara: \l shows databases
Sep 03 15:13:23 <hruske>	you need to do \c membership
Sep 03 15:13:33 <hruske>	to connect to specified database
Sep 03 15:13:36 <tangara>	ok. let me try
Sep 03 15:13:56 <tangara>	postgres=# \copy c:\membership.oldmembers from 'd:\memberparticulars.csv' csv header;
Sep 03 15:13:57 <Myon>	Zaab1t: what's your actual question?
Sep 03 15:13:58 <tangara>	like this ?
Sep 03 15:14:28 <hruske>	tangara: no
Sep 03 15:14:45 <hruske>	tangara: in psql prompt, you write "\connect membership"
Sep 03 15:14:50 <hruske>	without the quotes
Sep 03 15:14:53 <tangara>	oh ok
Sep 03 15:15:39 <tangara>	ERROR:  extra data after last expected column
Sep 03 15:15:46 <tangara>	it gives me error :(
Sep 03 15:15:57 <hruske>	tangara: success, looks like it finds the table!
Sep 03 15:16:06 <Zaab1t>	Myon: What can the owner do, that a user with all privileges cannot?
Sep 03 15:16:14 <hruske>	now you only need to have a valid CSV for the given table
Sep 03 15:16:25 <Myon>	I told you some things, you found some more
Sep 03 15:16:31 <tangara>	what do you mean by valid CSV ?
Sep 03 15:16:38 <Zaab1t>	Or really, is there any security implications of upgrading a user with all privileges on a database to the owner of said database?\
Sep 03 15:16:51 <tangara>	It is a database that  I have downloaded from my host
Sep 03 15:16:56 <tangara>	i mean webhost
Sep 03 15:17:06 <tangara>	cos I need to migrate it to Postgresql
Sep 03 15:17:08 <hruske>	tangara: "ERROR:  extra data after last expected column" is a common error when you have for example a table with 5 fields and the CSV has 7 fields
Sep 03 15:17:10 <Myon>	they can drop all schemas
Sep 03 15:17:36 <Myon>	Zaab1t: no implications outside that database
Sep 03 15:17:37 <tangara>	@hruske how do i overcome the problem
Sep 03 15:17:45 <hruske>	tangara: or if the separator is not what postgreqsl expects
Sep 03 15:18:07 <Myon>	Zaab1t: except maybe if there's objects in there with bad permissions (like user-executable admin functions)
Sep 03 15:18:09 <tangara>	ok. how do I make it work ?
Sep 03 15:18:35 <tangara>	it's been 2 months I have tried to import and only now able to use the psql command line...
Sep 03 15:18:38 <Myon>	tangara: make sure each row the the correct number of fields
Sep 03 15:18:59 <tangara>	so, you mean I have to check row by row ?
Sep 03 15:19:11 <tangara>	it is supposed to have all fields
Sep 03 15:19:38 <Myon>	no context for that ERROR?
Sep 03 15:19:46 <Myon>	there's probably a line number
Sep 03 15:19:58 <Myon>	but yes, you have to check each row
Sep 03 15:20:06 <Myon>	but there's thing called computer that can help you
Sep 03 15:20:08 <tangara>	yes. it gives me line 2
Sep 03 15:20:18 <tangara>	so, i just go check that line ?
Sep 03 15:20:46 <tangara>	last time when i tried the other way to import, it gives me same error and it never stops
Sep 03 15:21:09 <tangara>	is there an easier way to tackle this problem?
Sep 03 15:21:17 <Myon>	checking line 2 doesn't seem too hard
Sep 03 15:23:00 <tangara>	i checked that line. There isn't any missing field
Sep 03 15:24:01 <Myon>	pastebin the table definition and the first few lines from that file
Sep 03 15:24:32 <tangara>	table definition?
Sep 03 15:24:34 <Myon>	plus the full error message
Sep 03 15:24:40 <tangara>	i just merely create a table that's all
Sep 03 15:24:46 <Myon>	\d oldmembers
Sep 03 15:25:56 <tangara>	how do i go back to postgres# ? cos it is now at membership#
Sep 03 15:26:07 <Myon>	why?
Sep 03 15:26:14 <Myon>	that would be \c postgres
Sep 03 15:26:22 <Myon>	but that's hardly useful now?
Sep 03 15:26:35 <hruske>	tangara: membership database is where your table apparently is
Sep 03 15:26:56 <tangara>	membership=# \d oldmmembersDid not find any relation named "oldmmembers".
Sep 03 15:27:14 <tangara>	yes because i can't find it so i thought i have to go back to postgres
Sep 03 15:27:29 <hruske>	what is your table named again?
Sep 03 15:28:06 <Myon>	no wonder it took you two months to get there
Sep 03 15:28:09 <tangara>	membership-# \d oldmembers           Table "public.oldmembers" Column | Type | Collation | Nullable | Default
Sep 03 15:28:32 <Myon>	tangara: put the full output on http://paste.debian.net/
Sep 03 15:28:41 <tangara>	oh..so i need to set the collation to utf-8 right?
Sep 03 15:30:08 <Myon>	wat
Sep 03 15:30:20 <Myon>	you need to stop doing 3 things at once
Sep 03 15:30:52 <Myon>	if you know where the problem is, fix it
Sep 03 15:30:56 <Myon>	if not, answer questions
Sep 03 15:31:26 <RhodiumToad>	note that  membership-#  as the prompt means that you're part-way through entering a command
Sep 03 15:31:40 <RhodiumToad>	the prompt is =# or => if it's waiting for the start of a command
Sep 03 15:31:52 <tangara>	hi RhodiumToad finally see you here
Sep 03 15:32:24 *	RhodiumToad may not have time to help
Sep 03 15:33:02 <tangara>	i need your help in a problem - I got this error from my java code :  ResultSet not positioned properly.
Sep 03 15:33:29 <dognosewhiskers>	I'm grabbing statistics from pg_catalog.pg_database and information_schema to display how big my databases and tables are. I ran it once before and once after I had DELETEd hundreds of thousands of records. Both outputs are identical. Either all those rows took literally 0 bytes of storage (seems logically impossible to me) OR they have not updated. How do I force them to update?
Sep 03 15:33:41 <tangara>	i searched all over the net but there is only one which suggested putting a else to see if there is any rows after going thru the resultset.next() loop
Sep 03 15:33:49 <RhodiumToad>	tangara: that means you did something wrong in your java code. I don't do java.
Sep 03 15:34:10 <tangara>	i doubt so ...
Sep 03 15:34:19 <RhodiumToad>	dognosewhiskers: deleted rows are just marked dead.
Sep 03 15:34:24 <tangara>	cos it is saying a posgresql error
Sep 03 15:34:55 <RhodiumToad>	dognosewhiskers: vacuum will then convert them back to free space, but unless that free space happens to fall at the end of a data file, the file doesn't shrink and so no space is freed back to the OS
Sep 03 15:35:10 <RhodiumToad>	dognosewhiskers: (but the space will be reused by future inserts)
Sep 03 15:35:27 <RhodiumToad>	dognosewhiskers: if you absolutely need to reduce the on-disk size, there is vacuum full.
Sep 03 15:35:34 <dognosewhiskers>	:/
Sep 03 15:36:00 <dognosewhiskers>	?? VACUUM FULL
Sep 03 15:36:00 <pg_docbot>	https://www.postgresql.org/docs/current/static/sql-vacuum.html#AEN88966 :: http://rhaas.blogspot.com/2014/03/vacuum-full-doesnt-mean-vacuum-but.html
Sep 03 15:36:34 <tangara>	no. i can't define the utf encoding...
Sep 03 15:37:53 <Myon>	tangara: if you don't answer any questions, this will take more months to resolve
Sep 03 15:38:12 <tangara>	@Myon what did you ask ?
Sep 03 15:39:06 <Myon>	15:24 <Myon> pastebin the table definition and the first few lines from that file
Sep 03 15:39:15 <tangara>	ERROR:  extra data after last expected column
Sep 03 15:39:52 <tangara>	sorry Myon.. can I come back again cos it's been 3 hours I have been working on this...
Sep 03 15:40:15 <tangara>	i seem to be working on this kind of things non-stop....
Sep 03 15:40:21 <Myon>	?
Sep 03 15:40:30 <Myon>	just show us the details of your problem
Sep 03 15:40:34 <tangara>	and can't find a way to resolve all the coding problem...hours after hours
Sep 03 15:40:39 <tangara>	days after days
Sep 03 15:40:44 <tangara>	months after months
Sep 03 15:40:47 <Myon>	15:39 <Myon> 15:24 <Myon> pastebin the table definition and the first few lines from that file
Sep 03 15:40:50 <tangara>	may be this is not suitable for me
Sep 03 15:41:17 <tangara>	i already showed the table definition just now
Sep 03 15:41:24 <Myon>	no you didn't
Sep 03 15:41:36 <Myon>	you showed the header of the definition without any content
Sep 03 15:42:49 <RhodiumToad>	remember not to paste in channel, instead paste to a paste site (e.g. dpaste.de) and give us the url
Sep 03 15:44:28 <dognosewhiskers>	Ah. That's better. 90 megs freed.
Sep 03 15:45:00 <tangara>	https://pastebin.com/tE1Zi7PA
Sep 03 15:45:14 <dognosewhiskers>	I have a feeling that there's probably a good reason this isn't done automatically, but I can't really think of why.
Sep 03 15:45:15 <tangara>	there is no content
Sep 03 15:45:25 <tangara>	that's all the things that was printed out
Sep 03 15:46:44 <hruske>	tangara: did you by any chance create a table with no columns?
Sep 03 15:46:48 <tangara>	https://pastebin.com/azUZjWjd
Sep 03 15:47:15 <tangara>	yes. according to that tutorial columns need not be created to do the csv import
Sep 03 15:47:30 <hruske>	that is not correct.
Sep 03 15:48:00 <tangara>	so, you are saying i should create the columns as well before anything can be imported?
Sep 03 15:48:07 <hruske>	correct
Sep 03 15:48:25 <tangara>	ok. let me do it and shall i come back later or ?
Sep 03 15:53:32 <hruske>	sure
Sep 03 17:24:19 <karlpinc>	I'm having a brain freeze.  Is there a way to take an integer number of seconds and convert to an interval without going through a string?  The undo of: extract(epoch from someinterval)::integer
Sep 03 17:27:41 <karlpinc>	I guess I can do: someseconds * '1 second'::interval
Sep 03 17:27:59 <karlpinc>	Which way is "better"?
Sep 03 17:28:35 <Zr40>	that form is correct
Sep 03 17:29:52 <karlpinc>	Well, "(someseconds || ' seconds')::interval" works too.
Sep 03 17:30:32 <Zr40>	it works, but it's terrible (-:
Sep 03 17:30:50 <karlpinc>	Feels terrible.
Sep 03 17:31:24 <ilmari>	yes, creating and re-parsing a string at runtime is just silly
Sep 03 17:32:07 <ilmari>	'1 second'::interval (or interval '1 second') gets turned into an interval at parse time
Sep 03 17:32:20 <karlpinc>	Ok.  *duh*
Sep 03 19:20:42 *	Disconnected ()
**** ENDING LOGGING AT Tue Sep  3 19:20:42 2019

**** BEGIN LOGGING AT Tue Sep  3 19:21:06 2019

Sep 03 19:21:06 *	Now talking on #postgresql
Sep 03 19:21:06 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 03 19:21:06 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:06 2019)
Sep 03 19:21:06 *	Channel #postgresql url: https://www.postgresql.org
Sep 03 19:27:35 <StuckMojo>	yeah i'm kind of wondering if the numbers dpm
Sep 03 19:27:41 *	StuckMojo sign
Sep 03 19:28:01 <StuckMojo>	don't match because they're two different queries run slightly out of sync
Sep 03 19:28:54 <davidfetter_work>	that's a possibility. SHOW doesn't appear to have any kind of transactionality
Sep 03 19:29:44 <davidfetter_work>	you're at least not worse off than you would be through the standard console, though
Sep 03 19:33:10 *	davidfetter_work wonders whether it'd even be possible to implement snapshots or other concurrency control in `pgbouncer`
Sep 03 19:38:46 <xocolatl>	you mean like a transaction or something?
Sep 03 19:40:39 <davidfetter_work>	yes
Sep 03 19:43:14 <davidfetter_work>	It's not clear even whether the data structures SHOW reads are self-consistent, let alone whether multiple such structures could be said to have a cohesive state, as you'd be doing for a JOIN.
Sep 03 20:25:44 *	Disconnected ()
**** ENDING LOGGING AT Tue Sep  3 20:25:44 2019

**** BEGIN LOGGING AT Tue Sep  3 20:26:12 2019

Sep 03 20:26:12 *	Now talking on #postgresql
Sep 03 20:26:12 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 03 20:26:12 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 03 20:26:12 *	Channel #postgresql url: https://www.postgresql.org
Sep 03 20:27:17 <davidfetter_work>	what are all those bouncers for?
Sep 03 20:27:39 <xocolatl>	funneling, most likely
Sep 03 20:27:54 <lordcirth_>	What's funneling in this context?
Sep 03 20:28:23 <xocolatl>	reducing the number of connections that postgres actually sees
Sep 03 20:28:48 *	xocolatl has seen bouncers pointing at other bouncers, four levels deep
Sep 03 20:29:16 <lordcirth_>	Ah, so merging requests from many clients into a few continuous connections?
Sep 03 20:29:25 <xocolatl>	yeah
Sep 03 20:29:37 <lordcirth_>	Is starting a new connection really expensive?
Sep 03 20:30:21 <xocolatl>	http://ecx.images-amazon.com/images/I/61+ss5iSfaL._SL1000_.jpg
Sep 03 20:30:27 <xocolatl>	a funnel
Sep 03 20:30:34 <Zr40>	having max_connections be so high to support thousands of idle connections is what's expensive
Sep 03 20:31:22 <Zr40>	xocolatl: what would be the use of having more than two levels of bouncers?
Sep 03 20:32:02 <xocolatl>	it may have been overkill, I don't know.  the apps had millions of connections
Sep 03 20:34:05 <xocolatl>	(popular website)
Sep 03 20:34:45 <Zr40>	sounds like either 'website' is understating the app, or 'millions of connections' is a design mistake
Sep 03 20:36:17 <xocolatl>	the website is kind of like youtube
Sep 03 20:36:35 <lordcirth_>	Ah, idle connections, I see.
Sep 03 20:36:52 <energizer>	is there a difference in what people mean by "one to many" vs "many to one"?
Sep 03 20:37:01 <xocolatl>	energizer: perspective
Sep 03 20:37:44 <xocolatl>	one client can have many orders / many orders can be for one client
Sep 03 20:38:11 <energizer>	the ddl for those would be the same, right?
Sep 03 20:38:22 <xocolatl>	yes
Sep 03 20:38:26 <energizer>	thanks
Sep 03 20:38:43 <Zr40>	lordcirth_: the connection being idle isn't really a problem, it's more like max_connections to 15000 instead of 150 because you have so many idle connections. iirc there's still operations that scale with max_connections.
Sep 03 20:39:17 <lordcirth_>	Zr40, only if the connections exist, or just by setting it?
Sep 03 20:39:17 <xocolatl>	and memory
Sep 03 20:39:21 <Zr40>	just by setting it
Sep 03 20:39:50 <Zr40>	if it didn't cost anything, it wouldn't need to be a setting
Sep 03 20:40:30 <xocolatl>	this particular one would, until we get dynamic shared memory
Sep 03 20:42:52 <peerce>	what you don't want is a 1000 of those 15000 connections making queries at the same time, that will hammer the CPU hard, and cause slower throughput then keeping the number of concurrent queries/transactions to something under 2-4X your cpu core count
Sep 03 20:43:49 <xocolatl>	lordcirth_: what peerce just said is why I said earlier that StuckMojo's max_connections should be about 200 for his 80 cores
Sep 03 20:44:11 <lordcirth_>	Good to know, thanks!
Sep 03 20:44:43 <xocolatl>	it's better to have some transactions wait in line than for everyone to storm the db at once
Sep 03 20:45:16 <lordcirth_>	If I don't have a bouncer, and I hit max_connections, clients will just wait and retry?
Sep 03 20:45:42 <xocolatl>	no, they will fail
Sep 03 20:45:51 <peerce>	we had a system with 1000s of terminal nodes generating short OLTP transactions.  the terminals didn't talk sql, they sent messages over a MQ style system to the app servers, the app servers would run a tunable number of worker threads that would fetch a message, process it, reply over the MQ with the result.
Sep 03 20:46:31 <peerce>	we get the best TPS (trnasaction per second) throughput at about 2X the database server core/thread count
Sep 03 20:47:51 <xocolatl>	my base formula is 2cpu + a handful more
Sep 03 21:20:22 <DuckyDev>	Hi guys. do you know if there exists an offline toole like dbdiagram ( https://dbdiagram.io/d ) which draws a diagram with the same style and simplicity for linux?
Sep 03 21:27:45 <snatcher>	DuckyDev: pgmodeler?
Sep 03 21:28:29 <jarlopez_>	Looking at a TCP dump of a logical replication connection, it appears that RDS issues a RST,ACK packet after a few minutes of activity, whcih forces the logical replication consumer to restart its replication connection and resume replicating from the same LSN
Sep 03 21:29:08 <DuckyDev>	I've looked at pgModeller and DBeaver which can autogenerate ER/UML Diagrams, howver they don't look very "pretty" and with the same degree of simplicity as dbdiagram.io
Sep 03 21:29:38 <jarlopez_>	What conditions might trigger this scenario?
Sep 03 21:30:36 <xocolatl>	??erd
Sep 03 21:30:37 <pg_docbot>	https://wiki.postgresql.org/wiki/Design_Tools :: https://wiki.postgresql.org/wiki/Documentation_Tools
Sep 03 21:37:37 <Myon>	jarlopez_: stupid firewalls forgetting connections after some time
Sep 03 21:39:00 <nbjoerg>	TCP idle timer?
Sep 03 21:39:10 <nbjoerg>	aka keep alive
Sep 03 21:40:44 <jarlopez_>	nbjoerg: Perhaps. From the consumer, I set keep-alive to 5min. Not sure what RDS is configured to by default
Sep 03 21:41:08 <jarlopez_>	Myon: That's a possibility. Do you have suggestions for confirming that suspicion?
Sep 03 21:41:23 <Myon>	I don't know your or Amazon's infrastructure
Sep 03 21:41:43 <Myon>	is the connection idle for a while when that happens?
Sep 03 21:41:54 <jarlopez_>	This is being run across a VPC peering connection on AWS between an EC2 instance and RDS. I'll try to do some digging.
Sep 03 21:42:22 <jarlopez_>	Myon: No, the TCP stream is active all the way to the RST,ACK packet
Sep 03 21:42:33 <Myon>	that's weird
Sep 03 21:43:37 <RhodiumToad>	you can get RST if one end closes the connection in the middle of receiving data
Sep 03 21:44:00 <RhodiumToad>	check the server logs on both ends?
Sep 03 21:45:05 <Primer>	Wait, isn't RDS one of those unknown AWS things?
Sep 03 21:45:11 <jarlopez_>	The consumer reports a TCP error: connection reset by peer. The PG logs from RDS tell me "LOG: invalid message length" followed by "unexpected EOF on standby connection". I'm not yet sure which happens first
Sep 03 21:45:40 <jarlopez_>	Primer: How do you mean?
Sep 03 21:46:07 <Primer>	When I brought up a planner issue with Aurora, people were quite adamant that Aurora != postgres
Sep 03 21:46:19 <RhodiumToad>	RDS is also not stock postgres
Sep 03 21:46:30 <RhodiumToad>	but closer to it than aurora I believe
Sep 03 21:46:51 <RhodiumToad>	jarlopez_: the consumer is stock postgres?
Sep 03 21:46:57 <Primer>	yes, this was my understanding as well. So we're OK with RDS, just not Aurora? Trying to determine where the boudnaries are
Sep 03 21:47:20 <JamesHarrison>	RDS is pretty close to stock postgres
Sep 03 21:47:23 <jarlopez_>	RhodiumToad: The consumer is a standalone Go application using the pgx library, which speaks the replication protocol
Sep 03 21:47:29 <RhodiumToad>	we're ok with either of them up to the point at which they diverge from community pg, at which point the answer becomes "take it up with amazon"
Sep 03 21:48:00 <RhodiumToad>	jarlopez_: ah. then I'd take a careful look at the requests it's sending, since it could be a client-side bug
Sep 03 21:48:29 <jarlopez_>	It's very possible that this is a networking issue. The closest to the symptoms I'm seeing is the converstation at https://www.postgresql.org/message-id/20180627131652.4zibah4oqscpkijh%40vault.lan
Sep 03 21:48:34 <Primer>	I simply didn't think there'd be any room for gray area. Good to know there is some. I'm experimenting a lot with Aurora postgres, and have refrained from bringing things up here, given my experience last time
Sep 03 21:48:58 <jarlopez_>	But in that email thread they seem to be getting multiple RST packets with identical sequence numbers
Sep 03 21:49:13 <schemanic[m]>	Hey all
Sep 03 21:49:20 <schemanic[m]>	Is it possible to write a query that takes a specific amount of time to complete?
Sep 03 21:49:32 <jarlopez_>	Primer: That's fair. I'll continue digging and try to reach out to AWS as well
Sep 03 21:49:49 <xocolatl>	schemanic[m]: you can make it quit after a certain time, but it will quit in error
Sep 03 21:49:56 <Primer>	jarlopez_: What are you using for logical replication?
Sep 03 21:49:57 <RhodiumToad>	multiple RST packets on a connection break are normal if there is data in flight at the time, the (closed) receiving end will respond to each of the in-flight packets with RST
Sep 03 21:50:22 <schemanic[m]>	My ops team is trying to stress test a postgres server and we need each connection to be running a single query for it's test duration
Sep 03 21:50:27 <RhodiumToad>	schemanic[m]: if you just want to delay and do nothing else, there's pg_sleep()
Sep 03 21:50:56 <jarlopez_>	RhodiumToad: Ah, that makes sense. In this case, the consumer application restarts and re-establishes its replication connection without problem
Sep 03 21:51:09 <xocolatl>	schemanic[m]: the pg_sleep() or pg_sleep_for() should be good enough for you
Sep 03 21:51:12 <xocolatl>	*then
Sep 03 21:51:39 <schemanic[m]>	That is perfect. Thank you
Sep 03 21:52:00 <jarlopez_>	Primer: On the RDS side, I'm using the wal2json logical decoding plugin. On the consumer side, the application establishes a replication connection and starts replication on it, sending back heartbeats at regular intervals
Sep 03 21:52:21 <rosterok>	I've got a table with about 20 million rows.  I run select * from giant_table where exists (select 1 from medium_table ...) or exists (select 1 from large_table ...).  It takes about 15 seconds when I constrain giant_table to about 1 month of data.  Are there any optimizations I can do?
Sep 03 21:52:25 <rosterok>	I could probably add two boolean columns to giant_table, but this would be slow to add and probably not the best solution.
Sep 03 21:54:52 <RhodiumToad>	rosterok: paste the explain analyze on explain.depesz.com
Sep 03 21:58:07 <rosterok>	probably a silly question, but can a run that on my small test database or does it need to be the larger database
Sep 03 21:59:08 <rosterok>	probably doesn't make much sense to do it on my small instance
Sep 03 21:59:24 <xocolatl>	which one do you want us to optimize for?
Sep 03 21:59:41 <rosterok>	exactly.  thanks :)
Sep 03 22:09:03 <rosterok>	thanks a lot for the offer, RhodiumToad.  i unfortunately have to wait for this information
Sep 03 22:40:43 <depesz>	anyone of you has ready query that will contain trigger information in "explain analyze"? if yes, could you please share yaml version of it?
Sep 03 22:43:46 <xocolatl>	depesz: https://dpaste.de/0hqR
Sep 03 22:43:59 <depesz>	thanks.
Sep 04 07:55:02 *	Disconnected ()
**** ENDING LOGGING AT Wed Sep  4 07:55:02 2019

**** BEGIN LOGGING AT Wed Sep  4 07:55:26 2019

Sep 04 07:55:26 *	Now talking on #postgresql
Sep 04 07:55:26 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 04 07:55:26 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 04 07:55:27 *	Channel #postgresql url: https://www.postgresql.org
Sep 04 07:57:12 <xocolatl>	is the temp table declared as ON COMMIT DROP or something?
Sep 04 07:57:29 <andehhh>	no
Sep 04 07:59:18 <incognito>	andehhh: and the change are already commited from the DML side when you are executing your proc ?
Sep 04 08:02:58 <andehhh>	incognito: yes
Sep 04 08:03:30 <incognito>	andehhh: can you pastebin the code ?
Sep 04 08:03:30 <andehhh>	there is a function changing the temp table and after that is a commit
Sep 04 08:04:30 <incognito>	andehhh: is there also a commit after the insertion into the out_table ?
Sep 04 08:06:00 <andehhh>	incognito: yes
Sep 04 08:07:26 <incognito>	andehhh: maybe, if we see the stored procedure code ...
Sep 04 08:07:38 <andehhh>	ahh wait
Sep 04 08:08:53 <andehhh>	its not the insert
Sep 04 08:09:14 <andehhh>	it's exactly at the second loop at FOR ... IN
Sep 04 08:10:42 <xocolatl>	I have to go now.  hopefully I'll read about this on pgsql-bugs tonight
Sep 04 08:11:19 <incognito>	my thought : if you are in read_committed transaction isolation, maybe postGIS geometries changes are not properly handled when the GIS object is toasted
Sep 04 08:11:38 <incognito>	that's why i asked about COMMITed data
Sep 04 08:14:41 <incognito>	as a workaround, i would try to put "SET TRANSACTION ISOLATION LEVEL SERIALIZABLE" at the beginning of the stored procedure commands
Sep 04 08:14:56 <andehhh>	http://dpaste.com/3PD4W2B
Sep 04 08:15:18 <andehhh>	incognito: ok will try that now
Sep 04 08:17:41 <andehhh>	 ERROR: SET TRANSACTION ISOLATION LEVEL must be called before any query
Sep 04 08:17:58 <andehhh>	it's right after the BEGIN now
Sep 04 08:18:11 <andehhh>	what would be the correct position?
Sep 04 10:15:33 *	Disconnected ()
**** ENDING LOGGING AT Wed Sep  4 10:15:33 2019

**** BEGIN LOGGING AT Wed Sep  4 10:15:56 2019

Sep 04 10:15:56 *	Now talking on #postgresql
Sep 04 10:15:56 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 04 10:15:56 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:06 2019)
Sep 04 10:15:56 *	Channel #postgresql url: https://www.postgresql.org
Sep 04 10:16:19 <Zr40>	if the constraint is declared deferrable, then validation is at least deferred to end-of-statement. If it's additionally set as deferred, then it is deferred until end-of-transaction
Sep 04 10:18:47 <petercpw>	do constraints have be be declared as deferrable
Sep 04 10:19:29 <petercpw>	if it's just a one-time transaction, can we still the check be deferred for non-deferrable constraints?
Sep 04 10:25:55 <petercpw>	is it possible to modify an existing constraint
Sep 04 10:25:59 <petercpw>	can't seem to find it in the docs
Sep 04 10:34:56 <adsf>	when creating a view, is it possible for a column to be another query? or would this be inefficient?
Sep 04 10:36:42 <Myon>	you can use any query in a view
Sep 04 10:36:54 <Myon>	if the query is slow, so is the view
Sep 04 10:37:27 <adsf>	that makes sense :) Thank you
Sep 04 10:37:49 <[patrik]>	adsf it would probably be inefficient, depending on the query.
Sep 04 10:38:14 <[patrik]>	adsf: what do you need to accomplish?
Sep 04 10:38:41 <adsf>	i need a count of rows matching a condition
Sep 04 10:38:59 <adsf>	(as well as a bunch of other things obviously)
Sep 04 10:39:28 <adsf>	i could have a trigger write that count out to one of the tables.
Sep 04 10:41:14 <[patrik]>	that would probably be inefficient. Like so i understand your model...you have a table A and then a view ontop of table A, and you want to include a count from table B of rows that fullfills some criteria in A?
Sep 04 10:42:11 <adsf>	so the view currently already does a join between table a and table b (which has a fkey for a)
Sep 04 10:42:20 <[patrik]>	ok.
Sep 04 10:42:27 <adsf>	it selects a single row via the pkey on a
Sep 04 10:42:36 <adsf>	b only returns aggregate information
Sep 04 10:43:35 <adsf>	so like, avg of b.column1, sum b.column2, (then count of matching rows on b)
Sep 04 10:43:39 <[patrik]>	ok. so you could make a bm materialized view to materialize the stuff in b. or does b change very often?
Sep 04 10:44:16 <adsf>	so i am running materialized views in other areas and this was one of the things i was considering expanding it to
Sep 04 10:44:27 <[patrik]>	if b is pretty static you could use a materialized view ontop of b, and refresh it however often is needed.
Sep 04 10:44:34 <adsf>	our mat views only refresh every 15min
Sep 04 10:44:53 <adsf>	thank you for the insights :)
Sep 04 10:45:02 <[patrik]>	well, every 15mins might be good enough, i dont know about your requirement.
Sep 04 10:45:28 <adsf>	fortunately its fine for us :) No crazy rush. Mat views have been quite performant for us so far!
Sep 04 10:46:22 <[patrik]>	yeah they usually are quite handy. heh. so just slap the counter you need in b, and use it in the view you have ontop of A already.
Sep 04 10:47:13 <adsf>	winner! Will give that a whirl.
Sep 04 10:47:35 <[patrik]>	woohoo.
Sep 04 10:48:04 <adsf>	usually i just do a bit of small scale testing with a regular view, then make a mat view out of it once all my code works :)
Sep 04 10:48:10 <adsf>	makes my life a bit easier!
Sep 04 11:00:38 <raddy>	Hello
Sep 04 11:01:22 <raddy>	I have tried importing the csv file with \copy transaction2019_sep_1_15 from /tmp/sep1.csv, but I received invalid input file
Sep 04 11:01:56 <raddy>	Even though the data has been taken from a different server with exact table setup
Sep 04 11:06:04 <[patrik]>	copy table from '/tmp/blah.csv' csv
Sep 04 11:06:19 <Myon>	what's the actual error message?
Sep 04 11:06:32 <[patrik]>	you might need to specify the delimiter too, and if your csv file contains a header row.
Sep 04 11:11:20 <Moonsilence>	Hi! How can I get notified about new Postgres minor releases by email?
Sep 04 11:15:37 <Berge>	Moonsilence: https://www.postgresql.org/list/pgsql-announce/ would be a good list
Sep 04 11:16:13 <Berge>	Or https://www.postgresql.org/news/pgsql.rss if RSS is your thing (or you setup a RSS-to-email-thing)
Sep 04 11:16:54 <macdice>	you could use a cron job that emails you when a new version is available to upgrade via your package manager
Sep 04 11:17:15 <Moonsilence>	Thanks!
Sep 04 11:17:24 <andehhh>	??bug
Sep 04 11:17:24 <pg_docbot>	https://www.postgresql.org/account/submitbug/
Sep 04 11:17:58 <Moonsilence>	I just signed up for this one here which also contains various pg news: https://postgresweekly.com/
Sep 04 11:18:13 <Berge>	Moonsilence: It's also posted to pgsql-announce
Sep 04 11:18:22 <Berge>	You didn't ask about general postgres news, though (-:
Sep 04 11:18:31 <Moonsilence>	true
Sep 04 11:28:45 <peerce>	if you really wanna get on top of postgres, subscribe and read postgresql-general and -announce ....
Sep 04 11:29:05 <peerce>	-general wil be full of stupid threads from idiots that need help, but that might be you some day....
Sep 04 11:41:52 <xocolatl>	I'm trying to find the cause of an incident that happened last month where there was a big spike in buffer_content locks, but I don't know the locking at that level well enough to know what to look for.  log_lock_waits is on but there is nothing in the logs about that, and I don't see anything about relation extension either.  any ideas for what to search for?  I saw an insert take 24 seconds around that time, too
Sep 04 11:44:15 <peerce>	wild guess from a half drunk birthday boy?   sounds like too many concurrent queries per CPU and/or storage
Sep 04 11:44:29 <xocolatl>	happy birthday :)
Sep 04 12:07:04 <andehhh>	xocolatl: Your bug report has been received, and given id #15990
Sep 04 12:07:29 <andehhh>	here it is as requested ;)
Sep 04 12:11:45 <xocolatl>	andehhh: excellent.  can't look at it until tonight, though
Sep 04 12:16:02 <lifeboy>	??paste
Sep 04 12:16:02 <pg_docbot>	https://explain.depesz.com/ :: https://pasteboard.co/
Sep 04 12:16:03 <pg_docbot>	https://www.db-fiddle.com/ :: https://paste.depesz.com/
Sep 04 12:16:03 <pg_docbot>	https://dpaste.de
Sep 04 12:22:16 <lifeboy>	I have 5 plpgsql functions that each returns a single value.  Because the are called with a select statement, the the returned value (a counter), is a single column single row.https://dpaste.de/gk4D
Sep 04 12:23:17 <lifeboy>	I want to call all five these in sequence in another function, but can't figure out how to do that, since I'm in effect getting 5 result sets returned.
Sep 04 12:23:21 <lifeboy>	postgres 10
Sep 04 12:43:32 <lifeboy>	I have created separate functions for each query so that I can return the column count for each.  I could not figure out how to run multiple queries in one function and return the result of each in the same function...
Sep 04 12:45:22 <Myon>	using OUT parameters
Sep 04 12:45:29 <Myon>	??out
Sep 04 12:45:29 <pg_docbot>	https://www.postgresql.org/docs/current/static/plpgsql-declarations.html :: https://www.postgresql.org/docs/current/static/xfunc-sql.html#XFUNC-OUTPUT-PARAMETERS
Sep 04 12:46:49 <Myon>	or simply select imprt_imp(), flag(), import_abs(), ...;
Sep 04 13:20:20 <[Terra]>	Hi all, I'd like your opinion. Am I right in that I try to minimize the number of idle connections to my postgres servers?
Sep 04 13:22:49 <Myon>	these don't hurt that much
Sep 04 13:23:01 <Myon>	you really want to minimize idle *in transaction*
Sep 04 13:23:14 <Myon>	really idle connections just consume some RAM
Sep 04 13:23:19 <[Terra]>	Well, developers here think it's good for their performance to write a daemon that opens 5 (or more) connections to postgres, issue several 'prepare statements' in it, and then they sit and wait for connection from their webfrontend.
Sep 04 13:23:31 <Myon>	yes, that's standard practise
Sep 04 13:23:33 <[Terra]>	For every app they write.
Sep 04 13:23:47 <Myon>	and 5 is usually fine, unless you have 100 of these
Sep 04 13:23:56 <[Terra]>	So I have hundreds of connections that do nothing.
Sep 04 13:24:42 <Myon>	can you ask them to make that number configurable, and then set it to 2 in the config?
Sep 04 13:25:25 <Berge>	And/or consider using a proxy such as pgbouncer.
Sep 04 13:25:48 <Berge>	If you have considerable more connections to postgres than you have CPU cores (or IO subsystem to support the load), things will slow down.
Sep 04 13:26:33 <[Terra]>	I'd live to move them to pgbouncer, but in their current setup it won't work since they expect that several 'prepare statements' handlers do exist.
Sep 04 13:27:11 <Berge>	[Terra]: Why wouldn't it work?
Sep 04 13:27:50 <[Terra]>	When pgbouncer closes the connection, the prepared statement is gone.
Sep 04 13:28:51 <[Terra]>	And yes, I've tried asking them to lower the intial number of connections too.
Sep 04 13:29:04 <Berge>	[Terra]: Depends on the pooling mode.
Sep 04 13:29:54 <Berge>	(Why are they using PREPARE?)
Sep 04 13:30:35 <[Terra]>	I've also suggested that they'd use a function instead of prepare statement
Sep 04 13:33:23 <[Terra]>	It is manageble now, but I feel that all those idle connections are a waste of resources on the postgres server.
Sep 04 13:33:46 <Berge>	Not if they just idle.
Sep 04 13:33:57 <Berge>	Prepared statements aren't used for the same things as functions, though?
Sep 04 13:36:03 <mobidrop>	they do take up connections so if you hit the max it'll block
Sep 04 13:36:07 <[Terra]>	Isn't it so that for every connection at least 'work_mem' is reserved? Of is that work_mem not allocated until it is used?
Sep 04 13:37:35 <Myon>	the latter, and it's even freed again immediately after each query
Sep 04 13:37:44 <Zr40>	work_mem is actually a limit for plan nodes, so one connection can in fact use work_mem multiple times
Sep 04 13:37:54 <Myon>	the memory usage is for internal caches ("which tables have I seen yet")
Sep 04 13:38:52 <[Terra]>	Myon: Ah, ok. In that case it is much less of a problem than I thought.
Sep 04 13:39:16 <Myon>	a typical number would be something like 16MB per persistent backend
Sep 04 13:39:25 <Myon>	but it could be more if there's many tables/objects
Sep 04 13:39:35 <[Terra]>	And by query I assume you mean transaction.
Sep 04 13:39:39 <Myon>	query
Sep 04 13:39:48 <[Terra]>	really?
Sep 04 13:39:50 <[Terra]>	ok
Sep 04 13:39:56 <Myon>	there's per-transaction memory as well, but that's usually less
Sep 04 13:40:17 <Myon>	"select from foo order by bar" needs memory to sort in the *query*
Sep 04 13:40:27 <Zr40>	previous $work database, which had terrible partitioning and long-lived connections, saw the syscache grow to ridiculous size
Sep 04 13:42:13 <[Terra]>	So, an idle connection only eats one connection out of max_connections, but doesn't hurt much otherwise?
Sep 04 13:42:39 <Zr40>	right
Sep 04 13:42:46 <Myon>	plus it's a process that eats host RAM
Sep 04 13:43:05 <Myon>	if you don't have RAM to have 1000 processes of that size, it's a problem
Sep 04 13:44:04 <[Terra]>	Yes, but the code of those processes should be shared as well.
Sep 04 13:44:53 <Myon>	the shared libraries are, the syscache is not
Sep 04 13:45:29 <Myon>	just look at a PostgreSQL server that has been running for some time and check the RES column in ps/top
Sep 04 13:45:34 <[Terra]>	Ah yes, and the syscache is where "which tables have I seen yet" is stored?
Sep 04 13:45:36 <Myon>	(or RSS, that's the same)
Sep 04 13:45:41 <Myon>	yes
Sep 04 13:51:56 <[patrik]>	lifeboy: if you have 5 functions ... f1-f5 that returns single values you should be able to create a function "fa() returns text[]" and do a select f1()::text,f2()::text,f3()::text,f4()::text,f5():text construct in your fa() function and return all of your function values in one return value. Or am I missing something?
Sep 04 13:53:27 <[Terra]>	Oh wow. I see several postgres processes with a RSS of 7500000+
Sep 04 13:54:07 <Myon>	that might also be shared memory from shared_buffers
Sep 04 13:54:20 <Myon>	sometimes that's counted in RSS as well
Sep 04 13:54:41 <Myon>	especially if the server has started recently
Sep 04 13:54:49 <[Terra]>	Ah, yes. Of course.
Sep 04 13:55:03 <[Terra]>	Well this server is been up for a wile.
Sep 04 13:55:30 <[Terra]>	Too long actually..
Sep 04 13:59:28 <[Terra]>	Anyway, thanks for the information.
Sep 04 14:12:56 <[patrik]>	lifeboy: if you create your single value functions like this...say f1() to fn...
Sep 04 14:13:22 <[patrik]>	create or replace function f1() returns text as
Sep 04 14:13:23 <[patrik]>	$$
Sep 04 14:13:23 <[patrik]>	begin
Sep 04 14:13:23 <[patrik]>	  return 1;
Sep 04 14:13:23 <[patrik]>	end;
Sep 04 14:13:27 <[patrik]>	$$ language plpgsql;
Sep 04 14:14:13 <[patrik]>	then you can create however many functions you want to return single values...and then create a function to return all the values...fa() for example...
Sep 04 14:14:43 <[patrik]>	create or replace function fa() returns text[] as
Sep 04 14:14:44 <[patrik]>	$$
Sep 04 14:14:44 <[patrik]>	declare
Sep 04 14:14:44 <[patrik]>	t text[];
Sep 04 14:14:46 <[patrik]>	begin
Sep 04 14:14:48 <[patrik]>	  select ARRAY[f1()::text,f2()::text,f3()::text] into t;
Sep 04 14:14:51 <[patrik]>	  return t;
Sep 04 14:14:53 <[patrik]>	end;
Sep 04 14:14:56 <[patrik]>	$$ language plpgsql;
Sep 04 14:15:08 <[patrik]>	and have all of them returned as an array from a single function.
Sep 04 14:15:12 <Myon>	[patrik]: stopping to wonder about weird questions if the OP doesn't follow up is a great way to save a lot of time on IRC
Sep 04 14:15:27 <[patrik]>	Myon: ok. Im done worrying.
Sep 04 14:15:33 <Myon>	welcome :)
Sep 04 14:15:40 <[patrik]>	:)
Sep 04 14:15:42 <ilmari>	[patrik]: also, please don't paste multi-line things directly in the channel, use a paste site
Sep 04 14:15:48 <ilmari>	??paste
Sep 04 14:15:48 <pg_docbot>	https://explain.depesz.com/ :: https://pasteboard.co/
Sep 04 14:15:48 <pg_docbot>	https://www.db-fiddle.com/ :: https://paste.depesz.com/
Sep 04 14:15:48 <pg_docbot>	https://dpaste.de
Sep 04 14:26:54 <Intelo>	  created_at timestamp without time zone NOT NULL,
Sep 04 14:26:54 <Intelo>	  updated_at timestamp without time zone NOT NULL,
Sep 04 14:27:08 <Intelo>	should they be auto filled? if I want it, what to do?
Sep 04 14:27:15 <Intelo>	auto filled on insertion
Sep 04 14:27:23 <Myon>	you should use WITH timestamp in most cases
Sep 04 14:27:40 <nickb>	with time zone*
Sep 04 14:27:41 <Myon>	on insertion, use DEFAULT now()
Sep 04 14:27:53 <Myon>	err yeah. "timestamptz"
Sep 04 14:28:01 <Intelo>	using rails. have to figure out syntax there then
Sep 04 14:28:20 <[patrik]>	intelo: define them to have a DEFAULT value. Like my_timestamp timestamptz not null default now()
Sep 04 14:29:03 <Zr40>	rails will provide the values itself without any assistance from the database
Sep 04 14:30:42 <enoq>	hi I've got existing int ids and I want to migrate the data to postgres
Sep 04 14:30:49 <enoq>	what datatype do I want
Sep 04 14:31:05 <[patrik]>	enoq: probably bigint
Sep 04 14:31:09 <ilmari>	integer or bigint, depending on how big/many there are
Sep 04 14:31:26 <enoq>	and then use a sequence which starts at the highest number?
Sep 04 14:31:50 <enoq>	I need to deal with both inserts using an id and inserts creating an id
Sep 04 14:31:59 <[patrik]>	that would probably work, but it depends on your application design.
Sep 04 14:32:27 <[patrik]>	create a sequence starting with the highest value+1 once you are done loading the table
Sep 04 14:32:30 <enoq>	we will try how far we get with JPA and hibernate
Sep 04 14:32:52 <Myon>	if you have a mix of inserts that use the sequence and that don't, it will be a huge mess
Sep 04 14:32:53 <[patrik]>	and use that sequnce like nextval('my_id_sequence'); as a value on new inserts.
Sep 04 14:33:57 <enoq>	Myon right, hm. we have some stupid frontend logic that generates IDs from unix timestamps and we need some time to migrate that
Sep 04 14:34:31 <enoq>	especially because the code is a mess
Sep 04 14:34:49 <[patrik]>	enoq: if those unix magic timestamp ids doesnt mean something to your datamodel, use a proper id column and add another column to store frontend converted unix magic.
Sep 04 14:35:16 <enoq>	yeah, unfortunately they do
Sep 04 14:35:19 <[patrik]>	enoq: dont share an id field for sequence generated id's and unix-magic-id's
Sep 04 14:35:49 <Myon>	or start the generated IDs at 1, it'll be some time until they conflict with the timestamps
Sep 04 14:36:04 <Myon>	somewhat ugly, but should work
Sep 04 14:37:54 <[patrik]>	or you could decode those unix-magic timestamps...are they only epoch timestamps or are there some more magic applied?
Sep 04 14:39:19 <[patrik]>	if they are just epoch timestamps you can keep generating such ids
Sep 04 14:39:30 <[patrik]>	in postgres as well... SELECT extract(epoch from now());
Sep 04 14:42:34 <[patrik]>	so then the id field would be declared as --- id bigint not null default extract(epoch from now())
Sep 04 14:47:40 <[patrik]>	what might end up sucking though is uniqueness if you have 2 sources of generating ids.
Sep 04 14:48:25 <Myon>	or insert two rows in one second
Sep 04 14:49:25 <[patrik]>	it's hibernate so that won't happen ;)
Sep 04 14:50:04 <[patrik]>	is there only second resolution on epoch timestamps?
Sep 04 14:50:20 <nickb>	I'd actually use a sequence starting at 0 with negative increment to avoid collisions
Sep 04 14:50:37 <[patrik]>	nickb: that would work.
Sep 04 14:51:09 <Myon>	[patrik]: well it's stored in an int
Sep 04 14:51:16 <[patrik]>	true
Sep 04 14:51:47 <Myon>	pg_typeof(extract(epoch from now())) says double precision
Sep 04 14:52:00 <Myon>	1567601514.54378
Sep 04 14:52:19 <Myon>	I'll just stop worrying here
Sep 04 15:25:13 <enoq>	are types in postgres global? can they clash with existing types from your db?
Sep 04 15:25:20 <enoq>	asking because of my enum named role
Sep 04 15:26:06 <RhodiumToad>	role is a keyword
Sep 04 15:26:44 <RhodiumToad>	though it's not a reserved keyword, so you can use it as a type name
Sep 04 15:27:13 <RhodiumToad>	types are schema-qualified and use the search path; the builtin types are in the pg_catalog schema which is typically first
Sep 04 15:27:35 <RhodiumToad>	however, some type names are special syntax rather than just plain names
Sep 04 15:30:53 <enoq>	so using role is fine?
Sep 04 15:31:58 <RhodiumToad>	it will work, yes
Sep 04 15:32:38 <enoq>	thank you
Sep 04 15:33:05 <enoq>	is there a way to reuse common fields btw? stuff like Address where you don't necessarily want a relation
Sep 04 15:34:01 <RhodiumToad>	domains?
Sep 04 15:34:16 <RhodiumToad>	what exactly do you want to reuse?
Sep 04 15:34:34 <enoq>	I don't want to type the fields 3 times in my CREATE TABLE statements
Sep 04 15:34:57 <enoq>	fields such as: street, zip
Sep 04 15:36:03 <Myon>	create table foo (LIKE bar), but no idea if you can combine that with other fields
Sep 04 15:37:19 <davidb2111>	Hi all! Anyone already did some rust client that connects to postgresql with Tls enabled?
Sep 04 15:44:14 <[patrik]>	enoq: store them in a jsonb then. hehe.
Sep 04 15:44:42 <enoq>	[patrik] not today :)
Sep 04 15:44:48 <[patrik]>	oh
Sep 04 15:44:58 <enoq>	migrating off nosql
Sep 04 15:47:36 <[patrik]>	but but but...Postgres runs SQLs with operators to query jsonb attributes :) its either that or repeat your fields :)
Sep 04 15:50:20 <ilmari>	davidb2111: https://crates.io/crates/postgres claims to support tls
Sep 04 16:56:05 *	Disconnected ()
**** ENDING LOGGING AT Wed Sep  4 16:56:05 2019

**** BEGIN LOGGING AT Wed Sep  4 16:56:30 2019

Sep 04 16:56:30 *	Now talking on #postgresql
Sep 04 16:56:30 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 04 16:56:30 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 04 16:56:30 *	Channel #postgresql url: https://www.postgresql.org
Sep 04 16:56:50 <anykey>	I could even do multiple "api" schemas and restrict these according to these roles
Sep 04 16:57:11 <anykey>	I just wanted to know whether that is frowned upon or not
Sep 04 16:57:41 <[patrik]>	that is what i like with using views, and if your are using login roles you can limit data visibility in the views depending on which user is logged in.
Sep 04 16:58:35 <[patrik]>	what is frowned upon by some, is hailed as beauty by others. like much of things it depends on when you were educated :)
Sep 04 16:59:31 <anykey>	from interaction with colleagues, I got the impression that "api" schemas are not common. I have only begun doing SQL for real in 2014. Five years later, I find that very few people want to do SQL in depth
Sep 04 16:59:39 <[patrik]>	some are "database is just a datastore" and hates db-based logic (becuase they dont understand it)..if you understand db-based logic, it makes all the difference, and a cleaner design if you ask me. So its all dependant on who you ask if its frowned upon :)
Sep 04 17:00:26 <anykey>	yes, but I am in the "PostgreSQL is a data processing framework and offers amazing capabilities out of the box" camp of people
Sep 04 17:00:58 <[patrik]>	anykey: then by all means go ahead and do your schema-based api design :)
Sep 04 17:01:18 <anykey>	I will try my best.
Sep 04 17:03:22 <JordiGH>	Oh, alright, I figured out how to use netstat. Found my processes hammering my db.
Sep 04 17:03:35 <[patrik]>	anykey: what is the api for? or will DOD kill me if you tell? hehe
Sep 04 17:03:48 <rosterok>	RhodiumToad: I finally got the explain up:  https://explain.depesz.com/s/cAuZ
Sep 04 17:05:11 <anykey>	[patrik]: groupware, team management, and a glorified to-do list with some company specific fields
Sep 04 17:05:34 <anykey>	[patrik]: some file mangement too
Sep 04 17:05:35 <RhodiumToad>	rosterok: here's a trick to try: in the query, add a condition  AND refs.start_timestamp < '2019-09-01 00:00:00'
Sep 04 17:06:02 <RhodiumToad>	rosterok: and try the explain again, with mergejoin still disabled
Sep 04 17:06:34 <anykey>	[patrik]: stuff that is mostly boring, actually :)
Sep 04 17:06:43 <[patrik]>	anykey: if you do  such schema based api, i would suggest you look into listen/notify to make your api-consumers aware of changes too.
Sep 04 17:07:19 <anykey>	[patrik]: hm, never used that before. Is postgresql now doing job queueing as well?
Sep 04 17:10:38 <[patrik]>	anykey: well i wouldnt say job queueing, but listen/notify can send a payload to a client. thing is if you read about it you'll see that it contains a few gotchas, but it might be useful in a scenario like yours.
Sep 04 17:11:09 <anykey>	I have noted it for research.
Sep 04 17:11:11 <anykey>	thanks.
Sep 04 17:13:35 <[patrik]>	anykey: if your api-consumers can change stuff its a good way of doing the other api-consumers aware of that change. if your api is fed from some central thing, and the clients will only consume stuff its not as useful.
Sep 04 17:14:33 <ZackTech2019>	Thanks guys I have fixed the problem
Sep 04 17:14:42 <anykey>	I think(!) it's just periodic connect-read
Sep 04 17:15:15 <anykey>	but they can of course change their data as well
Sep 04 17:15:34 <anykey>	I will look into that topic.
Sep 04 17:15:39 <[patrik]>	anykey: then its not needed, since for listen/notify to work you need to stay connected and "subscribe to a channel"...if you connect-read-disconnect it wont work.
Sep 04 17:16:06 <anykey>	ah, ok.
Sep 04 17:16:16 <[patrik]>	yeah check it out. its a cool tool in your "out of the box package" :)
Sep 04 17:16:30 <rosterok>	RhodiumToad: here's the new explain:  https://explain.depesz.com/s/Np5u.  sorry about the wait.  it takes me forever to go through someone to get these.
Sep 04 17:16:39 <anykey>	I was happy to see it gain procedures with transaction control
Sep 04 17:17:21 <[patrik]>	yeah transaction control is a big improvement. i didnt get to use it tho, since im still on older stuff.
Sep 05 09:58:43 *	Disconnected ()
**** ENDING LOGGING AT Thu Sep  5 09:58:43 2019

**** BEGIN LOGGING AT Thu Sep  5 09:59:06 2019

Sep 05 09:59:06 *	Now talking on #postgresql
Sep 05 09:59:06 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 05 09:59:06 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 05 09:59:07 *	Channel #postgresql url: https://www.postgresql.org
Sep 05 10:07:20 <enoq>	asked yesterday as well: I have a REST API that allows you to create db entries with or without ID; is there a way to combine this with an automatic ID if none is provided?
Sep 05 10:08:01 <Myon>	??identity
Sep 05 10:08:02 <pg_docbot>	https://wiki.postgresql.org/wiki/Identity_Guidelines :: https://www.depesz.com/2017/04/10/waiting-for-postgresql-10-identity-columns/
Sep 05 10:08:02 <pg_docbot>	https://wiki.postgresql.org/wiki/Logo :: https://blog.2ndquadrant.com/postgresql-10-identity-columns/
Sep 05 10:08:02 <enoq>	what I've read so far is that people use sequences and set it to the maximum value of the table upon generation
Sep 05 10:08:05 <Renter>	enoq: well, there are a lot of layers there, if you just want an unique identifier you can easily have a SERIAL field
Sep 05 10:08:32 <Renter>	But if you want something like having an unique identifier that can be set by the user/remote client, that's slightly different
Sep 05 10:08:41 <Myon>	enoq: the newer "identity" layer has better protection against messup than the plain "sequences" one
Sep 05 10:08:49 <Myon>	but it's still a mess if you mix them
Sep 05 10:09:35 <enoq>	people import older data over the rest API with existing ids
Sep 05 10:09:45 <enoq>	some create new entries over a web ui
Sep 05 10:09:55 <Renter>	uh
Sep 05 10:09:58 <peerce>	so then you just need to set the sequence to the max(id) after the import operation
Sep 05 10:10:15 <Renter>	I'd take the approach that you generate new IDs anyway, then have an old_id from the old system
Sep 05 10:10:16 <strk1>	I'm trying to understand _why_ my test finds pg_class.reltuples updated, when it does not run VACUUM or ANALYSE or CREATE INDEX on the target table, how to tell what's triggering an update of pg_class.reltuples ?
Sep 05 10:10:19 <Renter>	because you will f things up otherwise
Sep 05 10:10:39 <enoq>	peerce that makes sense
Sep 05 10:11:02 <enoq>	thanks :)
Sep 05 10:11:26 <peerce>	with newer versions of postgres, it *is* preferable to use the IDENTITY stuff instead of SERIAL.   SERIAL was the legacy way of doing an auto ID.
Sep 05 10:12:19 <Renter>	peerce: oh, that's interesting, I went back to postre this year and haven't run into IDENTITY yet
Sep 05 10:12:19 <peerce>	http://www.postgresqltutorial.com/postgresql-identity-column/
Sep 05 10:12:32 <peerce>	its a new feature in pg10+
Sep 05 10:12:41 <Myon>	it's still a sequence under the hood
Sep 05 10:12:57 <peerce>	yeah
Sep 05 10:13:31 <Myon>	strk: autovacuum instead of explicit vacuum?
Sep 05 10:15:21 <strk>	how to tell when did autovacuum kick-in ?
Sep 05 10:16:39 *	strk enabling autovacuum logging
Sep 05 10:19:03 <strk>	LOG:  parameter "log_autovacuum_min_duration" changed to "0"
Sep 05 10:19:23 <strk>	I see a single log line about autovacuum after that: LOG:  automatic vacuum of table "postgres.pg_catalog.pg_shdepend": index scans: 1
Sep 05 10:19:43 <strk>	nothing in between the statements that do see pg_class.reltuples changed
Sep 05 10:20:19 <strk>	was it an extension that allowed me to see _all_ statements (even those from plpgsql functions) ?
Sep 05 10:38:35 <afancy>	Hi, is it possible to hide "information_schema"? thanks
Sep 05 10:40:15 <Myon>	basically no
Sep 05 10:40:23 <Myon>	what problem do you want to solve?
Sep 05 10:43:42 <afancy>	Myon: I am using Hue to let our customers to browser postgresql database. But, information_schema is showing there. It would be nice if it can be hidden.
Sep 05 10:44:02 <Myon>	they will have to live with that
Sep 05 10:44:27 <Myon>	if you use pgadmin/omnidb, it won't appear under "schemas" but separately under "catalogs"
Sep 05 10:44:30 <afancy>	Myon: Okey. I have revoke the select permission on the tables
Sep 05 10:45:31 <afancy>	Myon: well, i am building a data lake, the underlying database can be any, including Hive, PostgreSQL, MySQL, etc. That is why I am using Hue.
Sep 05 10:50:45 <peerce>	'data lake' ?   :-/
Sep 05 10:50:54 <peerce>	sounds more like a slough.
Sep 05 10:52:26 <afancy>	peerce: well, whatever
Sep 05 10:53:15 <peerce>	why is this "Hue" exposing the information schema ?
Sep 05 10:53:32 <peerce>	thats, btw, something *all* SQL spec compliant databases are supposed to have.
Sep 05 11:47:29 <incognito>	peerce: plpgsql expert ? how to make a holdable dynamic cursor ?
Sep 05 11:49:34 <peerce>	I'm not even sure what you mean by a 'holdable dynamic cursor' ??
Sep 05 11:53:23 <peerce>	you mean like when the plpgsql function returns a refcursor and the client uses FETCH etc to read from that ?
Sep 05 11:54:45 <peerce>	pretty much all the allowable behavior is detailed here, https://www.postgresql.org/docs/current/plpgsql-cursors.html
Sep 05 11:57:13 <peerce>	its the 'dynamic' part I'm not sure what you mean.
Sep 05 11:58:12 <peerce>	holding onto a cursor outside the transaction is ugly, too, it requires the server to materialize the whole query and store it somewhere so you can fetch it at your leisure.   thats not terribly efficient
Sep 05 12:03:24 <incognito>	peerce: yes, let's say : a holdable refcursor
Sep 05 12:03:55 <incognito>	i cannot loop if there is a DML inside it
Sep 05 12:04:20 <incognito>	no known snapshot before at the start of the 2nd iteration of the loop
Sep 05 12:04:28 <incognito>	-before
Sep 05 12:06:08 <peerce>	um, DML in a cursor??  a  cursor is a represntation of a recordset... bunch of rows with the same fields.   how could that contain DML ??
Sep 05 12:06:32 <incognito>	peerce: insert in another table
Sep 05 12:07:05 <incognito>	 the DML and COMMIT cause the transaction block to forget the cursor; so the 2nd loop hands
Sep 05 12:07:07 <incognito>	hangs*
Sep 05 12:07:08 <peerce>	how is an insert inside a cursor ?
Sep 05 12:07:23 <incognito>	FOR rec in EXECUTE '' LOOP
Sep 05 12:07:28 <peerce>	and frankly, if you're doing that level of insanity with functions, you deserve the dogbite
Sep 05 12:07:33 <incognito>	insert into hop () values ();
Sep 05 12:07:34 <incognito>	commit;
Sep 05 12:07:36 <incognito>	end loop;
Sep 05 12:07:57 <incognito>	it's a regular way of using procedural language
Sep 05 12:08:27 <incognito>	if the cursor was not dynamic, it was not a problem, i think
Sep 05 12:08:42 <peerce>	my last $job, we moved most of our complex business logic OUT of pl*** and into the app servers, and only used pl*** for performance reasons
Sep 05 12:09:37 <peerce>	we'd been very tied to complex pl*** (mostly, Oracle pl/sql) for 10+ years and moving most of the logic OUT of the database and into the appserver layer gave us BETTER performance and better debugability
Sep 05 12:09:39 <incognito>	it's not really my goal here
Sep 05 12:10:21 <incognito>	peerce : and better interoperability with the IS
Sep 05 12:10:21 <peerce>	just saying, getting too complicated bites you in the ass.
Sep 05 12:11:03 <incognito>	well, if you see something like holdable refcursor, let me know
Sep 05 12:11:07 <incognito>	thank you indeed
Sep 05 12:11:37 <peerce>	yeah, don't wait up for that.
Sep 05 12:11:45 <incognito>	i know
Sep 05 12:12:05 *	incognito will not wait up to thank you ^^
Sep 05 12:31:32 <rokshis>	How do I make a statement of "UPDATE table SET something = var ON CONFLICT(constraint_name) DO NOTHING"
Sep 05 12:31:39 <rokshis>	is that possible?
Sep 05 12:31:43 <RhodiumToad>	no
Sep 05 12:31:54 <rokshis>	what's the alternative
Sep 05 12:32:52 <peerce>	an UPDATE that might trigger a contraint?   huh.     probably wrap it in a savepoint, if you need to continue with a larger transaction
Sep 05 12:33:20 <peerce>	??savepoint
Sep 05 12:33:21 <pg_docbot>	https://www.postgresql.org/docs/current/static/sql-savepoint.html
Sep 05 12:33:25 <rokshis>	it triggers a partial unique index
Sep 05 12:33:45 <rokshis>	i don't know, if I said it correctly, by saying "constraint_name"
Sep 05 12:33:48 <deebo>	isnt the default rollback, aka do nothing?
Sep 05 12:34:06 <peerce>	well, rollback is do nothing for the whole transaction
Sep 05 12:34:19 <rokshis>	deebo, yes. But I want to do nothing only with the rows, that actually trigger the constraint, and other ones to update
Sep 05 12:34:26 <rokshis>	it's a batch update.
Sep 05 12:34:34 <peerce>	ah, thats problematic.
Sep 05 12:34:56 <peerce>	use a WHERE clause to honly update the rows that are ok to update this way?
Sep 05 12:35:05 <rokshis>	i see, well, still thanks. Just wondered if there was any semantics for that
Sep 05 12:35:11 <rokshis>	peerce, yep, will probably do something like that
Sep 05 12:35:13 <rokshis>	will figure it out
Sep 05 12:35:15 <rokshis>	thanks guys
Sep 05 15:40:42 <xocolatl>	I'm trying to understand the buffer_mapping lock
Sep 05 15:41:04 <xocolatl>	is it looking up what buffer has the page, or is it running clocksweep to find a buffer to put the page in?
Sep 05 15:42:00 <xocolatl>	aside from the declarations, I don't see where the lock is actually taken.  I must be searching the wrong things (buffer_mapping, BufMappingLWLockTranche, and LWTRANCHE_BUFFER_MAPPING)
Sep 05 15:44:34 <ufk>	Hi! :) how can I get a value from a timestamptz column without the +?? (+03 or +00) at the end ? I want to get the value in a readable format
Sep 05 15:46:03 <xocolatl>	in which time zone?
Sep 05 15:47:08 <ufk>	currently when I select the column i see that it has +03 at the end
Sep 05 15:47:09 <xocolatl>	but if you think that's unreadable, you should probably just use to_char() and format it yourself
Sep 05 15:47:30 <Myon>	::timestamp
Sep 05 15:48:02 <Myon>	just *don't* change the datatype of the column, tz is the best default
Sep 05 15:48:06 <ufk>	awesome thanks
Sep 05 16:03:40 <xocolatl>	looks like it's for loading a new page
Sep 05 16:04:01 <xocolatl>	so if I'm seeing buffer_mapping contention, that should indicate that shared_buffers is too small, correct?
Sep 05 16:04:59 <Myon>	or too large maybe?
Sep 05 16:05:24 <Myon>	the clock sweep algorithm isn't especially smart, from what I got
Sep 05 16:06:00 <xocolatl>	I *think* the buffer_mapping lock is only held after the target buffer is found
Sep 05 16:24:16 <ufk_>	is there some kind of select I can do to see the last query that was executed (not in that same connection) ?
Sep 05 16:24:35 <Myon>	select * from pg_stat_activity;
Sep 05 16:24:44 <plujon>	I've only ever used simple data types in my postgressing.  But I now interact with a server that returns largish json blobs to me with unique keys identifying each blob.  I'm contemplating whether I should stick this data in postgres, or use another database for it, oriented around key/value json data.
Sep 05 16:24:44 <ufk_>	nah it's quick it's already finished
Sep 05 16:25:29 <plujon>	I don't really need to query into the blobs; mainly I need to retrieve them by key and convert them to [ruby] objects on demand.
Sep 05 16:25:46 <plujon>	How would a postgressor go about this?  hstore?
Sep 05 16:25:53 <Myon>	then store them as jsonb
Sep 05 16:26:10 <Myon>	no need to squeeze them into a normalized schema
Sep 05 16:26:59 <plujon>	Myon: Thanks; I should have checked the manual.  I see 8.14 describes JSON types!
Sep 05 16:27:30 <Myon>	if you absolutely don't care about the content, you could also use "text" or "bytea"
Sep 05 16:27:57 <plujon>	Would there be any benefit to using text or bytea?  Faster/smaller?
Sep 05 16:29:21 <plujon>	Meh; I'll just give jsonb a try; thanks!
Sep 05 16:40:02 <ratrace>	Hi. Looking for options for selecting randomly ordered N rows from a set with: a) lots of gaps in pkey so pkey can't be used to "hack" random,  b) heavily toasted table, c) use case that results with hundreds of thousands of tuples scaned sequentially when "ORDER BY RANDOM()" is used. One idea is to
Sep 05 16:40:32 <ratrace>	have a separate table with only two columns, a pkey/fkey into proper table, and a random number by which the table would be joined and orderd.
Sep 05 16:40:45 <Myon>	??tablesample
Sep 05 16:40:45 <pg_docbot>	http://www.dbazine.com/db2/db2-disarticles/zikopoulos2 :: https://wiki.postgresql.org/wiki/TABLESAMPLE_Implementation
Sep 05 16:40:45 <pg_docbot>	https://www.postgresql.org/docs/current/static/tablesample-method.html :: https://blog.2ndquadrant.com/tablesample-in-postgresql-9-5-2/
Sep 05 16:40:53 <Zr40>	??random
Sep 05 16:40:53 <pg_docbot>	http://blog.rhodiumtoad.org.uk/2009/03/08/selecting-random-rows-from-a-table/ :: http://www.depesz.com/2007/09/16/my-thoughts-on-getting-random-row/
Sep 05 16:40:53 <pg_docbot>	https://blog.2ndquadrant.com/tablesample-and-other-methods-for-getting-random-tuples/ :: https://www.postgresql.org/docs/current/static/functions-math.html
Sep 05 16:41:51 <Myon>	a separate table seems overcomplicated
Sep 05 16:42:13 <Myon>	if the table is heavily toasted anyway, you basically already have a separate table
Sep 05 16:45:21 <ratrace>	tablesample is of no use to me, I have very complex WHERE clauses and it apparently doesn't work with that
Sep 05 16:45:58 <ratrace>	so anway, my idea was a separate table which I'd update this "random_order" column with a random value on each update. it's not true random ordering for each select, but close enough for the use case
Sep 05 16:46:22 <ratrace>	and such a table with only two integer columns would pack a lot of rows per page so I could even periodically "shuffle" them
Sep 05 16:46:28 <Myon>	you could have that column on the original table as well?
Sep 05 16:46:52 <ratrace>	Myon: I could but updating rows would update whole 8k page just because one row occupies it
Sep 05 16:47:15 <ratrace>	with a separate table, unless I'm totally misunderstanding how paging works with postgres, I could have many rows per page
Sep 05 16:47:31 <ratrace>	and thus have lesser impact from periodically updating all of them to "reshuffle"
Sep 05 16:47:37 <Myon>	there should be at least 4 rows by page
Sep 05 16:47:53 <Myon>	check how large your rows are actually, you said there's toasting
Sep 05 16:48:22 <ratrace>	lemme read through the second group of links first
Sep 05 16:48:44 <yoshie902a>	Would like some help elminating an Or clause in this sql statement.. https://stackoverflow.com/questions/57808000/postgres-eliminate-multiple-conditions-and-reduce-to-one-without-an-or-stateme
Sep 05 16:49:05 <ratrace>	yeah the second group is based on gapless ID... so no good either.
Sep 05 16:50:41 *	Disconnected ()
**** ENDING LOGGING AT Thu Sep  5 16:50:41 2019

**** BEGIN LOGGING AT Thu Sep  5 16:54:01 2019

Sep 05 16:54:01 *	Now talking on #postgresql
Sep 05 16:54:01 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 05 16:54:01 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 05 16:54:01 *	Channel #postgresql url: https://www.postgresql.org
Sep 05 16:54:07 <ratrace>	well the separate table approach would simplify the query to only one ORDER BY random_column with that table joined to the main query.
Sep 05 16:54:31 <ratrace>	with some fkeys applied, I don't foresee any bugs but the reason I came here was to get this idea shot down so pls, bring it on :) and thanks ;)
Sep 05 16:55:12 <ratrace>	periodically reshuffling to avoid alwyas the same ordering of what's supposedly random ordering over a period of time
Sep 05 16:55:23 <ne2k>	is there a way to get psql to pretty print a query without creating a view of it and inspecting the source?
Sep 05 16:56:11 <Myon>	ne2k: pgformatter
Sep 05 16:56:27 <RhodiumToad>	ratrace: but there'd be no benefit in join+order by compared to just order by random()
Sep 05 16:56:53 <ratrace>	RhodiumToad: I'd avoid using random() at all
Sep 05 16:57:00 <RhodiumToad>	so?
Sep 05 16:57:21 <RhodiumToad>	you wouldn't gain any performance and you'd lose the actual randomness
Sep 05 16:58:52 <ratrace>	RhodiumToad: not sure I follow. Right now I have order by random which breaks indexing and causes full table scan so the engine could sort through all the results by where clause and select top 4 with the limit
Sep 05 16:59:04 <ratrace>	 /order by random()/
Sep 05 16:59:22 <RhodiumToad>	order by random doesn't "break indexing" when you have a where clause
Sep 05 16:59:31 <ne2k>	Myon, so not possible somehow while editing a query in psql?
Sep 05 17:00:03 <ne2k>	might be quite nice to add as a feature, press some shortcut key and it does whatever gets done internally when you create a view and print it out
Sep 05 17:00:31 <ratrace>	RhodiumToad: I expressd myself badly. What I meant was, I don't want it to scan through thousands of rows if I want only 4 out of the result set
Sep 05 17:00:53 <RhodiumToad>	right, but you usually can't avoid that
Sep 05 17:01:09 <ratrace>	I don't have the seq scan when I join this separate table.
Sep 05 17:01:36 <RhodiumToad>	that doesn't mean the query is any faster
Sep 05 17:02:29 <ne2k>	how can you have the top four of random order without finishing ordering all of them?
Sep 05 17:02:34 <RhodiumToad>	if the where clause is selecting 10k rows from 10m, then on average you'd have to scan 1000 rows from your separate random table for every row returned, if the plan is using that table to drive the join
Sep 05 17:04:12 <ne2k>	Myon, oh, I also see there is a thing on pg_formatter about using it to format within an editor, and there is a thing in psql to use an editor to edit a line, isn't there? so maybe those things could be combined
Sep 05 17:04:17 <ratrace>	RhodiumToad: hrm...
Sep 05 17:04:33 <Myon>	ne2k: I'd think so, yes
Sep 05 17:04:50 <ne2k>	Myon, "some assembly required" -)
Sep 05 17:04:54 <Myon>	ne2k: in vim, it's basically !Gpg_formatter<enter>
Sep 05 17:05:01 <ne2k>	ARGH, my eyes!
Sep 05 17:05:26 <Myon>	or perhaps try \setenv EDITOR pg_formatter
Sep 05 17:06:09 <ratrace>	RhodiumToad: yea I see it'll still have to join 10k rows which is not too far performance wise from a simple random()
Sep 05 17:06:20 <ratrace>	(called per row for each of the 10k)
Sep 05 17:07:54 <Myon>	\setenv EDITOR pg_format
Sep 05 17:07:56 <Myon>	\e
Sep 05 17:08:04 <Myon>	ne2k: ^ prints the query in psql
Sep 05 17:08:19 <ratrace>	RhodiumToad: I'll guess I'll have to solve this at a totally different layer. one idea was to cache the list of resulting IDs in, say, redis, for each where-clause combo (few thousands but not infinite), and then randomize 4 ids from the list and use it in query ala   where pkey in (1, 2, 3, 4);
Sep 05 17:08:36 <Myon>	http://paste.debian.net/1098930/
Sep 05 17:08:52 <ilmari>	doesn't \e expect the editor to modify the file in-place? whereas pg_format spits it out on stdout?
Sep 05 17:09:07 <Myon>	yeah
Sep 05 17:09:25 <Myon>	but maybe that's ok - the problem is that it re-executes the query as well
Sep 05 17:09:27 <ilmari>	pg_format could do with an -i option to edit it in-place
Sep 05 17:15:07 <ne2k>	ilmari: yes, I see that it runs the query again. nice, though, and fine for a simple select
Sep 05 17:20:43 <davidfetter>	xocolatl, thought this might interest you.  https://github.com/ULB-CoDE-WIT/MobilityDB/
Sep 05 17:38:17 <xocolatl>	Myon: it seems buffer_mapping is NOT held during clocksweep, so I still think contention on it only means shared_buffers is too small
Sep 05 19:14:39 *	Disconnected ()
**** ENDING LOGGING AT Thu Sep  5 19:14:39 2019

**** BEGIN LOGGING AT Thu Sep  5 19:15:06 2019

Sep 05 19:15:06 *	Now talking on #postgresql
Sep 05 19:15:06 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 05 19:15:06 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 05 19:15:06 *	Channel #postgresql url: https://www.postgresql.org
Sep 05 19:15:07 <Slade>	nice :)
Sep 05 19:15:30 <Myon>	select ('today'::timestamp at time zone 'America/Chicago') at time zone 'America/New_York';
Sep 05 19:15:50 <Myon>	I guess that's what you wanted
Sep 05 19:16:45 <RhodiumToad>	that's probably wrong
Sep 05 19:17:03 <Myon>	the last one was just for testing here
Sep 05 19:17:53 <RhodiumToad>	if you want to know what time it was in new york at the start of what is the current day in chicago, you have to do this:
Sep 05 19:18:45 <RhodiumToad>	date_trunc('day', now() at time zone 'America/Chicago') at time zone 'America/Chicago' at time zone 'America/New_York'
Sep 05 19:19:05 <Slade>	no i want to know the start of date x, at timezone y  where x and y come from different fields. in different tables :P
Sep 05 19:19:37 <RhodiumToad>	x is what data type?
Sep 05 19:19:55 <Slade>	x is a timestamptz (currently always set to UTC)
Sep 05 19:20:08 <Slade>	but i think Myon's first query shows me exactly how to do it
Sep 05 19:20:12 <RhodiumToad>	timestamptz values don't store a timezone
Sep 05 19:20:44 <Slade>	they store a +00:00 dont they?
Sep 05 19:20:54 <RhodiumToad>	so the question is: at absolute time X, in timezone Y, what is the absolute time of the start of the current day?
Sep 05 19:20:56 <Slade>	whicih i guess is different from a timezone
Sep 05 19:21:00 <RhodiumToad>	they do not
Sep 05 19:21:22 <RhodiumToad>	date_trunc('day', X at time zone Y) at time zone Y   is what you need
Sep 05 19:22:16 <Slade>	timestamp with time zone is the data type.. hum..
Sep 05 19:22:23 <Slade>	ok sounds good :)
Sep 05 19:23:10 <RhodiumToad>	pg's implementation of timestamp with time zone is not quite like how the sql spec defines it, but has the merit of being actually useful
Sep 05 19:23:48 <hoe`>	"(report.customer_server_id IN $2) AND" hmm, why would that give me a syntax error?
Sep 05 19:24:50 <RhodiumToad>	because IN expects a following list or subquery
Sep 05 19:25:04 <RhodiumToad>	and parameters can't be lists (though they can be arrays)
Sep 05 19:25:31 <RhodiumToad>	if $2 is an array, then you wanted  (report.customer_server_id = ANY ($2))
Sep 05 19:25:49 <Slade>	so, generally if you're storing startdate/enddate. is storing it as a range preferable?
Sep 05 19:26:10 <RhodiumToad>	depends how you plan to query it
Sep 05 19:26:23 <Slade>	or is it mostly irrelevant
Sep 05 19:27:10 <Slade>	are there advantages to keeping them separate?
Sep 05 19:27:29 <RhodiumToad>	sometimes, yes
Sep 05 19:27:51 <RhodiumToad>	remember you can always do a functional index on the range if you need one
Sep 05 19:29:39 <hoe`>	thanks RhodiumToad!
Sep 05 19:30:17 <Slade>	yea just trying to come up with a standard way of showing it all
Sep 05 19:34:30 <harks>	Is there something like "any(t.col) ~ 'myre.*'"?
Sep 05 19:34:57 <RhodiumToad>	where t.col is an array column?
Sep 05 19:35:02 <harks>	yes
Sep 05 19:35:19 <RhodiumToad>	not without defining a custom operator for commuted regexp match, or unnesting the array
Sep 05 19:35:21 <harks>	Do I have to unnest?
Sep 05 19:35:28 <harks>	I see
Sep 05 19:36:06 <RhodiumToad>	the custom operator isn't hard
Sep 05 19:41:12 <Slade>	heh "operator does not exist: text + text"  always forget whats allowed on various  database types :P
Sep 05 19:43:12 <[patrik]>	|| is a better operator than + for text
Sep 05 19:43:13 <[patrik]>	:)
Sep 05 21:45:17 *	Disconnected ()
**** ENDING LOGGING AT Thu Sep  5 21:45:17 2019

**** BEGIN LOGGING AT Thu Sep  5 21:45:43 2019

Sep 05 21:45:43 *	Now talking on #postgresql
Sep 05 21:45:43 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 05 21:45:43 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 05 21:45:44 *	Channel #postgresql url: https://www.postgresql.org
Sep 05 21:46:46 <localhorse>	RhodiumToad: i found these outdated instructions: https://wiki.openstreetmap.org/wiki/PostGIS/Installation#openSUSE_13.1_2 and this https://download.opensuse.org/repositories/Application:/Geo/openSUSE_Leap_15.0/  after replacing the link it seems to work now
Sep 05 21:47:05 <RhodiumToad>	what version of postgis are you getting from that though?
Sep 05 21:47:33 <muzakmonk11>	I am doing reading on partitioning and would like to better understand how partition pruning is done, what role statistics has in the execution of queries that don't include the column(s) partitioned on, and other related information. Any recommended reading?
Sep 05 21:47:38 <localhorse>	RhodiumToad: postgresql10-postgis-2.5.2-lp150.1.43.x86_64
Sep 05 21:48:38 <RhodiumToad>	ok, so that's one point release behind
Sep 05 21:49:06 <RhodiumToad>	(2.5.3 is the current 2.5 release)
Sep 05 21:49:21 <localhorse>	RhodiumToad: i want to use the same postgis version as heroku because i'm deploying to that. but i can't find which version they use for postgres 10
Sep 05 21:49:42 <RhodiumToad>	if you have it installed you can query pg_extension
Sep 05 21:50:09 <localhorse>	ah right
Sep 05 21:51:56 <localhorse>	it says extversion 2.5.2, too
Sep 05 21:52:24 <localhorse>	extversion is the column that represents the version, right?
Sep 05 21:52:58 <RhodiumToad>	yes
Sep 05 21:54:10 <muzakmonk11>	Are the statistics for all child tables/partitions examined at planning time and different plans potentially used for each child table?
Sep 05 21:54:51 <RhodiumToad>	yes
Sep 05 21:55:35 <localhorse>	RhodiumToad: is the default template used for createdb using utf8?
Sep 05 21:55:52 <RhodiumToad>	that depends how you ran initdb
Sep 05 21:56:29 <RhodiumToad>	if initdb defaulted to UTF8 (based on locale environment vars) or you explicitly did initdb -E UTF8 then yes
Sep 05 21:56:45 <RhodiumToad>	psql -l  and look at the encoding of template1 to see
Sep 05 21:58:11 <localhorse>	ah yes, it's utf8
Sep 05 21:58:17 <localhorse>	btw, when is template0 used?
Sep 05 21:58:33 <RhodiumToad>	when restoring from dumps with -C or when you explicitly specify it
Sep 05 21:58:51 <RhodiumToad>	which you're forced to do if you're changing the encoding or collation in createdb
Sep 05 21:59:49 <RhodiumToad>	the intent is that template0 should remain pristine, whereas you're allowed to modify template1 in order to set up things (such as extensions) that you want to be available in all new dbs by default
Sep 05 22:00:18 <RhodiumToad>	(and to encourage this, template0 disallows connections by default)
Sep 05 22:01:31 <localhorse>	ah thx, makes sense. you're the best :)
Sep 05 22:02:19 <RhodiumToad>	there's a special exception that allows you to use template0 as template even when changing the encoding or collation, which is not allowed for any other template
Sep 05 22:02:43 <RhodiumToad>	this is because the pristine state of template0 does not contain anything which is encoding or collation dependent.
Sep 05 22:14:00 <localhorse>	ah
Sep 05 22:20:06 <muzakmonk11>	I've got many tables with (client_id, id) as the primary key. Postgres throws an error if I try to do a hash partition on site_id, as is documented, but is there any workaround for this?
Sep 05 22:20:26 <muzakmonk11>	I can partition on client_id, but that won't help in some cases where there is a single client on the entire database. We already have 60-100 databases that have 1 to several hundred clients and want to keep standardized schema across client databases.
Sep 05 22:21:18 <peerce>	so id isn't unique ?
Sep 05 22:24:00 <Myon>	lechner: you still didn't understand how a hash function works
Sep 05 22:24:18 <Myon>	lechner: this algorithm *is* implemented, but the problem is elsewhere
Sep 06 08:11:44 *	Disconnected ()
**** ENDING LOGGING AT Fri Sep  6 08:11:44 2019

**** BEGIN LOGGING AT Fri Sep  6 08:12:10 2019

Sep 06 08:12:10 *	Now talking on #postgresql
Sep 06 08:12:10 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 06 08:12:10 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 06 08:12:10 *	Channel #postgresql url: https://www.postgresql.org
Sep 06 08:33:32 <wkalt>	RhodiumToad: thanks, will remember that
Sep 06 08:48:41 <Myon>	lechner: from normalized version components, yes. That part is defined nowhere, but I already told you that dropping leading zeros is probably all that's needed
Sep 06 09:29:56 <mazula>	hi what is wrong with my code? https://gist.github.com/minas-tirith/16ca484531cbbde3009c1f7c4624766e
Sep 06 09:31:19 <Myon>	mazula: you are creating the PK twice
Sep 06 09:31:37 <Myon>	but the ERROR in there is because you execute the INSERT twice
Sep 06 09:32:26 <peerce>	yeah, if yoou declare somethign PRIMARY KEY, it createsa  unique index already, so creating another one is just redundant
Sep 06 09:33:05 <mazula>	I created two insert ?
Sep 06 09:33:07 <mazula>	at once?
Sep 06 09:33:14 <Myon>	also, naming a non-PK index _pkey is eww
Sep 06 09:33:50 <Myon>	mazula: there are two errors, one is the create index
Sep 06 09:34:12 <Myon>	and the other ERROR is actually not an error at all, the INSERT works just fine
Sep 06 09:34:51 <mazula>	the error pop when I try to insert
Sep 06 09:34:57 <mazula>	line 15
Sep 06 09:35:01 <mazula>	the other lines work fine
Sep 06 09:35:04 <Myon>	because you already inserted
Sep 06 09:35:18 <Myon>	drop the table and try again
Sep 06 09:37:57 <adsf>	do triggers support multiple insert into arguments? I have a situation where a trigger on one table needs to insert a value into a few other tables.
Sep 06 09:38:10 <peerce>	yes, you can do that
Sep 06 09:38:22 <Myon>	you can run arbitrary code in the trigger function
Sep 06 09:38:23 <peerce>	i prefer avoiding complicated automagic trigger activated stuff
Sep 06 09:38:38 <peerce>	but thats a personal preference
Sep 06 09:38:59 <adsf>	its basically entering a unique key into somewhat of a queue table
Sep 06 09:48:01 <peerce>	well, if you'd executed those commands in an empty database, then I can see no way you would have gotten that error
Sep 06 10:01:12 <incognito>	;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;:;::;;::123;;;;:1b:23
Sep 06 10:01:14 <incognito>	b;:123
Sep 06 10:01:14 <incognito>	1:3123
Sep 06 10:01:16 <incognito>	1;;;;;:123123
Sep 06 10:01:16 <incognito>	:bnbb:
Sep 06 10:01:17 <incognito>	;;;123123
Sep 06 10:01:18 <incognito>	:bn:
Sep 06 10:01:19 <incognito>	2;;;12:23
Sep 06 10:01:21 <incognito>	bn:231;;123
Sep 06 10:01:21 <incognito>	12:3
Sep 06 10:01:23 <incognito>	bn2;;:123
Sep 06 10:01:25 <incognito>	bn:b;;;123
Sep 06 10:01:27 <incognito>	::bn:;;;123
Sep 06 10:01:29 <incognito>	::b:bn;123
Sep 06 10:01:29 <incognito>	:bnn
Sep 06 10:01:31 <incognito>	123;;;123
Sep 06 10:01:33 <incognito>	::bn:12;;;123
Sep 06 10:01:33 <incognito>	::bnbn:
Sep 06 10:01:35 <incognito>	123;;123123
Sep 06 10:01:37 <incognito>	:bn!vb;123
Sep 06 10:01:39 <incognito>	:bnb3
Sep 06 10:01:41 <incognito>	;123
Sep 06 10:01:43 <incognito>	:bnbnVC:
Sep 06 10:01:45 <incognito>	23;b;:12
Sep 06 10:01:47 <incognito>	3b:nv!b:;;;VBN./
Sep 06 10:01:49 <incognito>	vb:
Sep 06 10:01:51 <incognito>	;123
Sep 06 10:02:20 <peerce>	whats this a faceplant?
Sep 06 10:03:40 <kayront>	:D
Sep 06 10:04:13 <fleetfox>	a cat?
Sep 06 10:07:35 <peerce>	i'm more a dog person, never much cared for cats.
Sep 06 10:11:12 <lavalike>	obviously the fox blames the cat
Sep 06 10:14:35 <peerce>	https://www.youtube.com/watch?v=HYvnFdLqVL8   cat?
Sep 06 10:14:38 <peerce>	MEOW!
Sep 06 10:18:32 *	incognito killed a keyboard right now ... :'( crime weapon : hot coffee
Sep 06 10:20:57 <elmcrest>	incognito sorry to hear ... throw it into a box of rice! :D
Sep 06 10:22:08 <incognito>	elmcrest: hehe, but i dont have a box of rice enough big; and don't want to make my keyboard a new criminal
Sep 06 10:22:16 <elmcrest>	haha
Sep 06 10:22:23 <elmcrest>	well ... replace it with a water resistant :P
Sep 06 10:22:29 <incognito>	yep
Sep 06 10:22:54 <incognito>	i try to hide the proof and the first criminal actually
Sep 06 10:23:07 <incognito>	(and don't want a civil war btw)
Sep 06 10:23:38 <elmcrest>	you have to stand up for your right to coffee
Sep 06 10:24:51 <coffeeAttorney>	peerce: sorry, keyboard problem
Sep 06 10:25:12 <peerce>	black coffee shoudln't crash a keyboard
Sep 06 10:25:23 <incognito>	big milk coffee ?
Sep 06 10:25:27 <peerce>	but coffee with milk and sugar, all bets re off
Sep 06 10:25:46 <xocolatl>	pumpkin spice
Sep 06 10:25:49 <incognito>	we'll see, it goes to the sun  :)
Sep 06 10:25:55 <peerce>	if its good coffee, its evil to put dairy in it.   if its bad coffee, dairy doesn't really ehlp much
Sep 06 10:26:25 <incognito>	peerce: the badest one : instant coffee + milk + too much sugar
Sep 06 10:27:02 <fleetfox>	that is not coffee anymore :)
Sep 06 10:27:20 *	incognito tries like a dumb to type his password during 10 minutes with 123bbb;;;;;;bbb;;;123 going on the middle of it
Sep 06 10:27:27 <peerce>	i don't think I've made or drunk 'instant' coffee in like 30+ years
Sep 06 10:27:33 <peerce>	maybe 40+
Sep 06 10:27:56 <peerce>	when I go camping, I bring whole beans and a hand crank grinder.
Sep 06 10:28:05 <peerce>	and I either make Aeropress, or hand pour drip
Sep 06 10:29:09 <peerce>	at home, we use an electric mill, and a Bonavita electric drip pot w/ melita #4 filters.
Sep 06 10:31:12 <vlt>	*Melitta
Sep 06 10:31:20 <peerce>	yah, that.
Sep 06 10:32:03 <vlt>	https://en.wikipedia.org/wiki/Melitta_Bentz
Sep 06 10:32:04 <peerce>	for grinding w/o electricity, this works great! https://handground.com/
Sep 06 10:32:06 <incognito>	s, filter coffee
Sep 06 10:32:37 <incognito>	filter coffee is the best; but it disturb people who likes expresso
Sep 06 10:32:55 <peerce>	oh, i like a /good/ espresso but that requires a lot of work
Sep 06 10:33:14 <incognito>	and a lot of noise
Sep 06 10:33:20 <peerce>	we even have a very nice swiss made espresso maker, an Olympia Express
Sep 06 10:33:29 <peerce>	but its SO much work
Sep 06 10:34:26 <vlt>	Doing the lot of work to make an espresso actually *is* my coffee break. I only drink like 20 % of the coffees I make :D
Sep 06 10:35:24 <peerce>	our Bonavita drip machine at home has a thermos carafe, and I drink 2-3 mugs daily.
Sep 06 10:35:53 <flux[m]>	incognito: it seems actually that quality coffee grinders don't make _that_ much noise
Sep 06 10:36:38 <incognito>	flux[m]: maybe, but it's not the case when i go drink a coffee (i'm in Paris) in a "Bistrot"
Sep 06 10:38:38 <peerce>	good coffee still tastes good if its not hot.  we mostly use kenya aa, ethiopian harrar, various indonesians, amd evem a few select south american beans.
Sep 06 10:38:46 *	lluad discovers that an identity column generated by default doesn't have a default, in a pg_attributes.atthasdef sense
Sep 06 10:38:51 <peerce>	had some really good nicaraguan last week
Sep 06 10:38:56 <lluad>	This makes sense to me. To my code, not so much.
Sep 06 10:39:11 <peerce>	light to medium roast.
Sep 06 10:39:32 <YmrDtnJu>	i have the following setup: node db1 uses streaming replication to sync with db2. another node uses logical replication to stream some data from db1. i added db1 and db2 hostnames to host= in connection string and additionally added target_session_attrs=read-write. this way, libpq automatically uses db1. will the third node use db2 if it is promoted to master and db1 is shutdown?
Sep 06 10:40:54 <peerce>	that makes my head hurt to even read.
Sep 06 10:44:02 <peerce>	if you want to do master/slave promotion kinda stuff, you probably should put a bouncer/pooler in front that you reconfigure when the servers failover.
Sep 06 10:44:35 <YmrDtnJu>	peerce: why?
Sep 06 10:44:48 <YmrDtnJu>	libpq uses the same logic.
Sep 06 10:46:02 <indrek>	Looking for ideas: I have approximately 300M rows for currency rates (cur, timestamp, value). About 75 different currencies. I need to start querying rates with specific dates. Im thinking about 3 options: Partition by cur (75 tables), look into timescaledb or separate table daily rates (doesn't help when asking rate date and time). How would you make queries faster? More index, mode used
Sep 06 10:46:02 <indrek>	gb. Data itself is for half a year and about 20gb
Sep 06 10:47:09 <indrek>	what would be the most efficent way?
Sep 06 10:47:45 <incognito>	indrek: what are your performance requirement ?
Sep 06 10:48:07 <indrek>	Not many queries, but fast
Sep 06 10:48:15 <indrek>	maybe 10 queries per day
Sep 06 10:48:20 <indrek>	but i need to get them in ms
Sep 06 10:48:28 <indrek>	but i need to get them in milliseconds
Sep 06 10:48:49 <incognito>	indrek: the 10 queries are only about recent data ?
Sep 06 10:49:10 <indrek>	No, abut whole table
Sep 06 10:49:26 <indrek>	It might be today or it might be last year
Sep 06 10:49:35 <indrek>	and i have about 3 updates per second
Sep 06 10:49:46 <indrek>	so far it was only used for "just in case logging"
Sep 06 10:50:03 <holi0317>	Having some issue with row-level security. I am using a column deleted to indicate deleted status of a row in some table. So I've added deleted = FALSE in RLS select policy. But when I update a record to deleted = TRUE, the query fails as new row violates select policy.
Sep 06 10:50:11 <holi0317>	Is there any way to disable select policy checking new row record?
Sep 06 10:50:42 <incognito>	indrek : how much rates do you have by day ?
Sep 06 10:51:12 <fleetfox>	is it growing proportionally across all currencies?
Sep 06 10:51:37 <indrek>	about 2 rates per currency per sec
Sep 06 10:52:18 <incognito>	173k per currency per day ?
Sep 06 10:52:59 <indrek>	let me check exact number
Sep 06 10:53:26 <incognito>	indrek : the 300M rows are only for 23days ?
Sep 06 10:54:13 <incognito>	(i think we are trying to know the actual timestamp range of values)
Sep 06 10:54:20 <indrek>	2 557 203 Row per day
Sep 06 10:54:24 <indrek>	Approximately
Sep 06 10:54:56 <YmrDtnJu>	indrek: does your query use more than one currency? does one query use a wide or small range of timestamps?
Sep 06 10:55:03 <incognito>	ok so, around 60M row / month ?
Sep 06 10:55:44 <incognito>	YmrDtnJu: +1, indrek; let us know the typical most consuming query
Sep 06 10:56:05 <indrek>	No, about 77M rows per month
Sep 06 10:56:36 <indrek>	Query that i need first last rate per currency per deal
Sep 06 10:56:47 <xocolatl>	deal?
Sep 06 10:56:50 <indrek>	Query that i need first is "last rate per currency per deal"
Sep 06 10:56:53 <incognito>	i bet on : partition by week + hash index on cur column
Sep 06 10:56:54 <indrek>	Query that i need first is "last rate per currency per day"
Sep 06 10:57:37 <indrek>	incognito: with timescale? or pure pg?
Sep 06 10:57:39 <xocolatl>	I would do a partition per month with an appropriate btree index
Sep 06 10:57:55 <incognito>	pure pg
Sep 06 10:58:09 <incognito>	i don't like aggregated data
Sep 06 10:59:28 <indrek>	And then index on (timestamp, cur)
Sep 06 10:59:39 <xocolatl>	no, (cur, timestamp)
Sep 06 10:59:59 <indrek>	why in that order ?
Sep 06 11:00:27 <incognito>	well, the indexes after viewing the queries :)
Sep 06 11:00:51 <indrek>	that's true also :)
Sep 06 11:01:45 <indrek>	i expect probably query like "WHERE date = xxx"
Sep 06 11:02:11 <xocolatl>	actually (cur, (date_trunc('day', ts)))
Sep 06 11:02:15 <incognito>	indrek: could be function based
Sep 06 11:02:51 <indrek>	xocolatl: thanks
Sep 06 11:02:56 <indrek>	incognito: thanks
Sep 06 11:03:08 <indrek>	i will try to divide it into smaller tables and create indexes
Sep 06 11:05:16 <YmrDtnJu>	you can also partition over more than one column and create a hierarchy of partitions. just to mention that.
Sep 06 11:07:05 <peerce>	OTOH, imho, I don't think you ever want to create more than 20 or 50 partitions of a given table or things will get really ugly when you make queries that can't be pruned
Sep 06 11:07:10 <incognito>	indrek: if you have only 3-4 monthes of data and you are not willing to store more than a year (~1MM rows); you can also partition by day
Sep 06 11:07:32 <incognito>	oups, peerce is right
Sep 06 11:07:45 <incognito>	i was agree for 365 partitions here  :)
Sep 06 11:08:44 <peerce>	we mostly partitioned by week with 6 montsh retention
Sep 06 11:17:43 <indrek>	too many tables will be eventually
Sep 06 11:17:55 <indrek>	my plan is to keep it maybe for 10 years or so
Sep 06 11:23:25 <mazula>	it's normal this error on an id PK with serial? duplicate key value violates unique constraint "dynamic_bundle_types_pkey"
Sep 06 11:23:54 <xocolatl>	if you insert values yourself, yes
Sep 06 11:24:02 <mazula>	why
Sep 06 11:24:07 <mazula>	the id is not auto incremented?
Sep 06 11:24:41 <xocolatl>	it is, and if the increment is the same as something you inserted yourself, you get a conflict
Sep 06 11:25:45 <mazula>	I juste insert something without manually set the "id"
Sep 06 11:26:11 <xocolatl>	something set an id manually (or reset the sequence)
Sep 06 11:26:39 <mazula>	what is the difference between serial and auto_increment?
Sep 06 11:26:50 <xocolatl>	auto_increment doesn't exist
Sep 06 11:27:05 <peerce>	did you import some existing data into this table?
Sep 06 11:27:12 <peerce>	before doing this INSERT in question ?
Sep 06 11:27:18 <mazula>	there is 4 row
Sep 06 11:27:21 <mazula>	in the table
Sep 06 11:27:26 <mazula>	and I want insert 80 new rows
Sep 06 11:28:00 <peerce>	what is the currval() of the sequence prior to doing this additional 80 inserts ?
Sep 06 11:30:06 <mazula>	I don't know it's my first time with postgreslq
Sep 06 11:30:31 <peerce>	select curval('seqname');
Sep 06 11:30:46 <peerce>	where seqname is the sequence associated with this tables primary key
Sep 06 11:31:20 <peerce>	err, maybe its currval('seqname') ...   i forget
Sep 06 11:31:34 <mazula>	I use pg-format
Sep 06 11:31:40 <peerce>	whats that ?
Sep 06 11:41:29 <indrek>	perce: you mentioned that you partition by week. Do you create partitions automatically? Cron ?
Sep 06 11:41:39 <indrek>	peerce: you mentioned that you partition by week. Do you create partitions automatically? Cron ?
Sep 06 14:00:25 <localhorse>	when i have postgres running in docker but also want to run it outside of docker, it fails with this: http://dpaste.com/1XSMHNJ
Sep 06 14:00:31 <localhorse>	how can i make it work?
Sep 06 14:01:51 <Xelnor>	localhorse: allocate different TCP ports to the one in docker and to the system one :)
Sep 06 14:06:08 <localhorse>	Xelnor: but how can i make it listen on NO port at all, just file socket?
Sep 06 14:07:29 <localhorse>	both `listen_addresses` and `port` are commented out (by default it seems), but somehow it still tries to bind to that port
Sep 06 14:07:44 <localhorse>	even though this says "Omit both of these options to disable TCP/ IP connections." https://www.jamescoyle.net/how-to/3019-how-to-change-the-listening-port-for-postgresql-database
Sep 06 14:08:32 <RhodiumToad>	listen_addresses defaults to 'localhost', you can explicitly set it to '' to disable tcp
Sep 06 14:08:50 <RhodiumToad>	port is required even for local sockets, that defaults to 5432
Sep 06 14:09:22 <RhodiumToad>	and remember that 90-99% of the things you read from random websites on the internet are wrong
Sep 06 14:10:36 <RhodiumToad>	("90% of the internet is crap, and nothing guarantees that the other 10% isn't crap as well")
Sep 06 14:10:56 <RhodiumToad>	(to slightly misquote and paraphrase spaf)
Sep 06 14:12:25 <nbjoerg>	to slightly extend on that: unix domain sockets include the port in the path
Sep 06 14:17:17 <localhorse>	ah thx, works now
Sep 06 14:43:14 <afidegnum>	if i unerstand, schema is what groups tables in the database, right ?
Sep 06 14:43:44 <RhodiumToad>	it's a logical grouping of tables, yes
Sep 06 14:43:53 <afidegnum>	ok, thanks,
Sep 06 15:03:16 <Siecje>	I had a deadlock from this code. https://dpaste.de/dQYL Why would a transaction depend on another transaction?
Sep 06 15:05:27 <Myon>	the typical situation is that you are updating rows 1 and 2, and some other transaction is updating rows 2 and 1
Sep 06 15:05:35 <Myon>	both will then have to wait for each other
Sep 06 15:06:17 <Myon>	this section hash sounds like it would update several rows
Sep 06 15:10:23 <RhodiumToad>	transactions wait for other transactions as part of the row-level locking mechanism
Sep 06 15:11:09 <RhodiumToad>	in order to allow unlimited numbers of row locks to be used, row locks are recorded in the tuple header of the locked row, by the transactionid of the locking transaction (or a multixact id for a group of transactions)
Sep 06 15:11:35 <RhodiumToad>	when one transaction waits on a row lock, it finds the id of the blocking transaction and waits for it to end
Sep 06 15:11:42 <Siecje>	Is row level locking the default?
Sep 06 15:12:17 <Siecje>	Myon: Updating the hash reads fields but only modifies one column for each section.
Sep 06 15:14:00 <RhodiumToad>	row-level locking is used in all update and delete operations, all FK checks, and all select for [key share|share|no key update|update] operations
Sep 06 15:15:53 <Siecje>	So two updates to the same record in different transactions will lock forever?
Sep 06 15:16:31 <Myon>	on a single row, the second update will wait for the first to finish
Sep 06 15:16:47 <Myon>	the deadlock happens if you have several rows updated in different order
Sep 06 15:17:16 <Siecje>	So I should update sections in the same order?
Sep 06 15:17:27 <Siecje>	What if one has more records?
Sep 06 15:18:14 <RhodiumToad>	what exactly are you updating?
Sep 06 15:23:21 <dob1>	hi, is there an index that can I apply to a text column to improve search via ~* operator?
Sep 06 15:23:30 <dob1>	I have the index on lower(col) now
Sep 06 15:25:35 <RhodiumToad>	what sort of regexp are you searching on?
Sep 06 15:25:46 <RhodiumToad>	a pg_trgm index will help a bit in some cases
Sep 06 15:26:12 <dob1>	RhodiumToad, I am just asking, it's not slow it performs good
Sep 06 16:42:16 *	Disconnected ()
**** ENDING LOGGING AT Fri Sep  6 16:42:16 2019

**** BEGIN LOGGING AT Fri Sep  6 16:42:40 2019

Sep 06 16:42:40 *	Now talking on #postgresql
Sep 06 16:42:40 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 06 16:42:40 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 06 16:42:41 *	Channel #postgresql url: https://www.postgresql.org
Sep 07 11:05:28 *	Disconnected ()
**** ENDING LOGGING AT Sat Sep  7 11:05:28 2019

**** BEGIN LOGGING AT Sat Sep  7 11:05:50 2019

Sep 07 11:05:50 *	Now talking on #postgresql
Sep 07 11:05:50 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 07 11:05:50 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:06 2019)
Sep 07 11:05:51 *	Channel #postgresql url: https://www.postgresql.org
Sep 07 11:11:03 <incognito>	tangara: \dt in psql client doesn't work ?
Sep 07 11:11:27 <incognito>	\d
Sep 07 11:12:18 <tangara>	er...let me copy down the command so that I don't have to ask you guys again and again
Sep 07 11:14:34 <tangara>	@incognito...it shows only the table I have just created
Sep 07 11:14:45 <tangara>	but my other existing table is not showing up
Sep 07 11:14:50 <tangara>	when I typed that command
Sep 07 11:15:42 <tangara>	@incognito what do you think it's happening?
Sep 07 11:18:43 <incognito>	it could be many things; another schema not in search_path; another database
Sep 07 11:19:10 <incognito>	request not sent (if no feedback)
Sep 07 11:29:13 <bmintz>	running `pg_restore -O /var/backups/psql/2019-09-07/ec -t emotes | sudo -u bots psql ec` reports that ERROR:  duplicate key value violates unique constraint "emotes_id_key" DETAIL:  Key (id)=([redacted]) already exists.
Sep 07 11:29:14 <bmintz>	however
Sep 07 11:29:51 <bmintz>	sudo -u bots psql ec -c 'select count(*) from emotes' reports 0
Sep 07 11:30:02 <bmintz>	oh it's under a schema ok
Sep 07 11:30:21 <bmintz>	nvm
Sep 07 11:33:15 <xocolatl>	a redacted id XD XD XD
Sep 07 11:34:31 <tangara>	@incognito do i need to specify the name of database before I created any table or ?
Sep 07 11:34:45 <bmintz>	xocolatl what's so funny
Sep 07 11:35:00 <tangara>	why is that that using psql I can see the table but not inside pgAdmin4 ?
Sep 07 11:35:03 <incognito>	tangara: you only need to connect to the right database; psql --help
Sep 07 11:35:24 <incognito>	i'm not aware of pgAdmin4 UI atm
Sep 07 11:35:45 <tangara>	so now how do I put the new table with the other table together under the same database name?
Sep 07 11:36:47 <tangara>	it seems like i am the only one that run into million of problems...even finding a table can be so difficult
Sep 07 11:36:52 <incognito>	tangara: identify the different databases created and in this databases, identify the schema created
Sep 07 11:38:13 <incognito>	to identify the databases \l -- exclude from the result the builtin templates
Sep 07 11:38:41 <tangara>	what do you mean ?
Sep 07 11:39:11 <xocolatl>	tangara: very few people here use pgadmin
Sep 07 11:39:25 <tangara>	yes. i want to learn psql also
Sep 07 11:39:33 <incognito>	to identify the schema \dn
Sep 07 11:39:47 <xocolatl>	you will be MUCH faster with psql (unless you're on windows)
Sep 07 11:39:54 <tangara>	since now I can access the psql can you tell me how to put all tables together in the same schema or database ?
Sep 07 11:40:10 <tangara>	yes. i hate windows also
Sep 07 11:40:38 <xocolatl>	same schema: alter table foo.bar set schema baz;
Sep 07 11:40:38 <tangara>	@incognito it shows me public postgres
Sep 07 11:40:45 <xocolatl>	same database: use pg_dump
Sep 07 11:41:45 <tangara>	so in this case is the schema postgres ?
Sep 07 11:42:18 <xocolatl>	how could we possibly know that?
Sep 07 11:42:19 <incognito>	and \d and \l
Sep 07 11:44:29 <tangara>	                                                 List of databases    Name    |  Owner   | Encoding |          Collate           |           Ctype            |   Access privileges------------+----------+----------+----------------------------+----------------------------+----------------------- membership | postgres | UTF8     | English_United
Sep 07 11:44:29 <tangara>	States.1252 | English_United States.1252 | postgres   | postgres | UTF8     | English_United States.1252 | English_United States.1252 | template0  | postgres | UTF8     | English_United States.1252 | English_United States.1252 | =c/postgres          +            |          |          |                            |                            |
Sep 07 11:44:30 <tangara>	postgres=CTc/postgres template1  | postgres | UTF8     | English_United States.1252 | English_United States.1252 | =c/postgres          +            |          |          |                            |                            | postgres=CTc/postgres
Sep 07 11:44:36 <xocolatl>	don't paste here please
Sep 07 11:44:39 <xocolatl>	??paste
Sep 07 11:44:39 <pg_docbot>	https://explain.depesz.com/ :: https://pasteboard.co/
Sep 07 11:44:40 <pg_docbot>	https://www.db-fiddle.com/ :: https://paste.depesz.com/
Sep 07 11:44:40 <pg_docbot>	https://dpaste.de
Sep 07 11:44:44 <tangara>	\l shows me membership and also parents
Sep 07 11:44:54 <tangara>	so parents is supposed to be in membership
Sep 07 11:45:02 <tangara>	how do i make parents go into membship ?
Sep 07 11:47:45 <tangara>	https://dpaste.de/ULWd
Sep 07 11:48:08 <xocolatl>	you need to dump and restore it
Sep 07 11:48:55 <xocolatl>	pg_dump -t parents postgres | psql membership
Sep 07 11:48:58 <tangara>	@xocolatl are you talking to me?
Sep 07 11:49:10 <xocolatl>	you are the only one asking questions
Sep 07 11:49:21 <tangara>	ok. let me try. i hope i won't lost the tables in membership
Sep 07 11:49:36 <tangara>	because i really hate to go thru the trouble to create them again..it's so scarey
Sep 07 11:49:39 <tangara>	and i am tired
Sep 07 11:49:52 <tangara>	@xocolatl ok
Sep 07 11:49:53 <xocolatl>	you won't lose anything
Sep 07 11:50:42 <tangara>	i did what you said but it is still not showing up
Sep 07 11:50:57 <xocolatl>	so what errors did you get?
Sep 07 11:51:01 <incognito>	tangara: prepare your different sql clients (pgAdmin4, psql, jdbc) to use only this membership db if you plan to only work on it
Sep 07 11:51:03 <tangara>	no error
Sep 07 11:51:17 <tangara>	that's what I plan to do
Sep 07 11:51:31 <xocolatl>	if you didn't get an error then it worked
Sep 07 11:51:38 <tangara>	but the parents which I have created using sql script doesn't appear inside the membeship
Sep 07 11:51:51 <tangara>	and hence I asked do I need to do it like in MYSQL
Sep 07 11:52:04 <tangara>	where I have to state use membership before creating the table
Sep 07 11:52:25 <xocolatl>	you just need to connect to the right database
Sep 07 11:52:28 <tangara>	xocolatl...it doesn't
Sep 07 11:52:38 <xocolatl>	you can't "use database" like in other engines
Sep 07 11:52:40 <tangara>	cos it is still not showing inside membership
Sep 07 11:53:02 <xocolatl>	how are you looking?
Sep 07 11:53:26 <tangara>	i refreshed my pgadmin and the table doesn't appear inside with other tables
Sep 07 11:54:33 <tangara>	how ?
Sep 07 11:54:46 <xocolatl>	I don't know anything about pgadmin
Sep 07 11:55:28 <tangara>	yeah but your psql is not helping me to put membership together with parents in the same schema
Sep 07 11:55:48 <xocolatl>	pg_dump -t parents postgres | psql membership
Sep 07 11:55:55 <tangara>	i need parents to be the same schema or bringing the tables from mebership to be same as parents either way
Sep 07 11:56:14 <tangara>	i already did as you said but it is not working
Sep 07 11:56:27 <tangara>	i already told you not working
Sep 07 11:56:34 <xocolatl>	show me the full output it gave you
Sep 07 11:56:56 <tangara>	you mean \l /
Sep 07 11:56:56 <incognito>	tangara: you still have your prompt ?
Sep 07 11:57:01 <tangara>	yes
Sep 07 11:57:05 <tangara>	i did the \l
Sep 07 11:57:14 *	xocolatl cares not about \l
Sep 07 11:57:20 <tangara>	but it is not showing membership inside the schema as the parent
Sep 07 11:57:23 <xocolatl>	show me the output of  pg_dump -t parents postgres | psql membership
Sep 07 11:57:33 <incognito>	tangara: try psql membership -c "select * from parents"
Sep 07 11:57:46 <incognito>	also
Sep 07 11:58:07 <tangara>	postgres-#
Sep 07 11:58:15 <tangara>	it just give me back the prompt
Sep 07 11:58:30 <tangara>	same as when I typed the command xoclatl told me
Sep 07 11:58:38 <tangara>	it just give me a prompt back
Sep 07 11:58:49 <xocolatl>	you don't run that inside psql, you run it from the command line
Sep 07 11:58:54 <xocolatl>	ctrl+d to get out
Sep 07 11:59:52 <tangara>	can't get out
Sep 07 12:00:15 <xocolatl>	ctrl+c and then ctrl+d
Sep 07 12:00:45 <tangara>	no
Sep 07 12:00:55 <tangara>	it asked me to go out of the batch job
Sep 07 12:01:10 <xocolatl>	what batch job?
Sep 07 12:01:14 <tangara>	i remember someone helped me and asked me to type a command and then i can use psql
Sep 07 12:01:19 <tangara>	again
Sep 07 12:01:24 <tangara>	from the prompt
Sep 07 12:01:39 <xocolatl>	what operating system is this?
Sep 07 12:02:26 <incognito>	tangara: the message is when you exit the psql prompt or the shell prompt ?
Sep 07 12:02:39 <tangara>	postgres=#
Sep 07 12:02:52 <tangara>	so from here how do i get out and go to psql
Sep 07 12:03:03 <xocolatl>	you're already in psql there
Sep 07 12:03:06 <tangara>	I am running psql using this runpsql.batch
Sep 07 12:03:25 <xocolatl>	what operating system is this?
Sep 07 12:03:26 <tangara>	and it links to windows command i think
Sep 07 12:03:33 <tangara>	windows 10
Sep 07 12:03:34 <xocolatl>	so you're on windows?
Sep 07 12:03:38 <tangara>	yes
Sep 07 12:03:46 <xocolatl>	oh well
Sep 07 12:04:02 <tangara>	it doesn't work like your system ?
Sep 07 12:04:17 <tangara>	am i able to use psql ?
Sep 07 12:04:38 <tangara>	cos I googled then i came to know I have to invoke psql.runbtach then I an use
Sep 07 12:05:20 <tangara>	so how now ?
Sep 07 12:05:37 <tangara>	why is my membership not showing it is in postgres ?
Sep 07 12:07:17 <incognito>	tangara: i dont know this batch file; are you able to ask the person who helped you to type the command writed by xocolatl; maybe there are some security in this batch
Sep 07 12:07:44 <tangara>	this is my personal laptop
Sep 07 12:09:04 <tangara>	@incognito ?
Sep 07 12:09:06 <tangara>	how ?
Sep 07 12:09:23 <incognito>	tangara: yes, but i don't know this batch file
Sep 07 12:11:18 <tangara>	so how can i use psql ?
Sep 07 12:11:32 <incognito>	from a cmd prompt
Sep 07 12:11:39 <tangara>	is it be'cos i am running on windows 10 it is not possible to use psql ?
Sep 07 12:12:02 <incognito>	but you need to know the environment able to do so
Sep 07 12:12:13 <tangara>	ok. what environment ?
Sep 07 12:12:51 <incognito>	the environement required in the cmd prompt in order to launch psql
Sep 07 12:13:38 <incognito>	it sometime includes some personal information, that's why i asked you to call the person who helped you
Sep 07 12:14:13 <tangara>	what personal infor ? i already siad this is my laptop
Sep 07 12:14:48 <incognito>	user,password,folders,windowspasswd, etc...
Sep 07 12:15:00 <tangara>	i have all the passwords
Sep 07 12:15:15 <tangara>	can u tell me why you guys can use psql ?
Sep 07 12:15:46 <tangara>	i am not sure why i can't invoke psql.exe inside the postgresql folder
Sep 07 12:21:37 <tangara>	seems like nobody can help me here ?
Sep 07 12:21:49 <tangara>	i am tired i need to take a break noe
Sep 07 14:17:39 <pgwhatever>	is there a way to validate a function when creating it?
Sep 07 14:18:17 <RhodiumToad>	validate how much of it?
Sep 07 14:18:19 <pgwhatever>	For instance, if I create a function that calls another function, but that other function is not defined yet, it is created successfully, but will return an error at runtime when it is subsequently executed.
Sep 07 14:18:29 <RhodiumToad>	that's intentional
Sep 07 14:19:10 <pgwhatever>	Be nice if there was a way to mark that function as INVALID when created.
Sep 07 14:19:20 <pgwhatever>	Oracle does that
Sep 07 14:19:58 <RhodiumToad>	pg doesn't track dependencies of function body content. (hard to do sanely with arbitrary pluggable language modules)
Sep 07 14:20:17 <RhodiumToad>	most function validators just check the syntax
Sep 07 14:20:33 <pgwhatever>	makes sense, but how about this:  is there a tool that would check it after it is created?
Sep 07 14:20:57 <RhodiumToad>	only running the function could do that
Sep 07 14:21:20 <pgwhatever>	gotcha, thanks
Sep 07 14:21:26 <RhodiumToad>	and even then there's the issue of coverage
Sep 07 14:21:46 <pgwhatever>	dag, RodiumToad, you are soooo freakin helpful at this site!
Sep 07 14:22:02 <pgwhatever>	thanks for all your past input as well
Sep 07 16:04:09 <pi__>	no voice?
Sep 07 16:04:25 <RhodiumToad>	hm?
Sep 07 16:22:43 <gajus>	https://docs.timescale.com/latest/api#add_dimension
Sep 07 16:23:07 <gajus>	anyone have a clue if add_dimension would sub-partition the existing dimensions or create a partition map using the new dimension?
Sep 07 16:23:17 <gajus>	it is not clear from reading the documentation what is the behaviour
Sep 07 16:26:40 <gajus>	having a second read, it sounds like it is the former
Sep 07 17:11:12 *	Disconnected ()
**** ENDING LOGGING AT Sat Sep  7 17:11:12 2019

**** BEGIN LOGGING AT Sat Sep  7 17:11:34 2019

Sep 07 17:11:34 *	Now talking on #postgresql
Sep 07 17:11:34 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 07 17:11:34 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 07 17:11:46 *	Channel #postgresql url: https://www.postgresql.org
Sep 09 09:21:29 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep  9 09:21:29 2019

**** BEGIN LOGGING AT Mon Sep  9 09:21:50 2019

Sep 09 09:21:50 *	Now talking on #postgresql
Sep 09 09:21:50 *	Topic for #postgresql is: Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || PostgreSQL 12beta3 is out. Test. || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 09 09:21:50 *	Topic for #postgresql set by Snow-Man!~sfrost@tamriel.snowman.net (Thu Aug  8 15:05:07 2019)
Sep 09 09:21:51 *	Channel #postgresql url: https://www.postgresql.org
Sep 09 09:26:38 <schinckel>	arrity42: The one we've used in the past was pganalyse
Sep 09 09:26:54 <schinckel>	Probably with a z tho', because 'muricans
Sep 09 09:27:01 <arrity42>	how'd it work for you?
Sep 09 09:27:01 <schinckel>	;)
Sep 09 09:27:26 <schinckel>	Yeah, it was pretty good. It showed us things that didn't have indexes, but also monitored queries, and showed us the slow ones.
Sep 09 09:27:42 <arrity42>	neato i'll bookmark and check it out
Sep 09 09:27:55 <schinckel>	The aggregation wasn't perfect though: it didn't do a good job with some of our functions that have variadic args
Sep 09 09:28:09 <arrity42>	luckily i don't have anything like that at the moment
Sep 09 09:31:33 <schinckel>	Oh, we are on a free plan with them, and still getting some potentially actionable information, as it turns out.
Sep 09 09:32:03 <arrity42>	nice!
Sep 09 10:11:56 <indrek>	hello
Sep 09 10:12:48 <indrek>	im inserting one table into another and to INSERT INTO newtable SELECT * FROM oldtable ORDER col_x.
Sep 09 10:13:01 <indrek>	when i select not select * ,ctid from newtable
Sep 09 10:13:53 <incognito>	indrek: what is your question ?
Sep 09 10:13:56 <indrek>	i have rows 1,3,2 with ctid-s (0,23)(0,24)(0,25)
Sep 09 10:14:14 <incognito>	the CTIDs are changing when you are changing relation
Sep 09 10:14:28 <indrek>	the order by is wrong
Sep 09 10:14:49 <incognito>	in the new table ?
Sep 09 10:14:52 <indrek>	if input is order by, then why isn't the rows in new table in order 1,2,3
Sep 09 10:14:54 <indrek>	yes
Sep 09 10:15:13 <incognito>	was that an empty table ?
Sep 09 10:15:20 <indrek>	yes
Sep 09 10:16:21 <indrek>	55960695	(0,32)
Sep 09 10:16:21 <indrek>	55960705	(0,33)
Sep 09 10:16:21 <indrek>	55960699	(0,34)
Sep 09 10:16:21 <indrek>	55960685	(0,35)
Sep 09 10:16:21 <indrek>	55960684	(0,36)
Sep 09 10:16:21 <indrek>	55960719	(0,37)
Sep 09 10:16:25 <incognito>	maybe a truncate table newtable; and vacuum full newtable; before retrying the insert select could change the order of the CTIDs
Sep 09 10:16:45 <indrek>	after insert ?
Sep 09 10:17:09 <incognito>	no, to replay the whole INSERTIONs
Sep 09 10:17:13 <incognito>	with good CTIDs
Sep 09 10:17:29 <indrek>	i even dropped table
Sep 09 10:17:33 <indrek>	nothing
Sep 09 10:22:08 <xocolatl>	indrek: why do you care what order the rows are in?
Sep 09 11:04:51 <[patrik]>	order is not a property of the data. so no relational databases guarantees order of inserted data.
Sep 09 11:06:11 <[patrik]>	you need to use order by in the select retrieving the data
Sep 09 11:10:52 <dreinull>	I have a Join like this: JOIN schueler ON (infos.klasse = schueler.klasse) or infos.klasse is null
Sep 09 11:11:32 <dreinull>	but I want something like: ....(infos.klasse ILIKE '%schueler.klasse%')...
Sep 09 11:11:37 <dreinull>	how can I do that?
Sep 09 11:22:56 <dreinull>	so that it includes that particular string. I expect to have infos.klasse to have a comma separated list
Sep 09 11:23:46 <Myon>	dreinull: you can put whatever boolean into the join claus
Sep 09 11:23:50 <Myon>	e
Sep 09 11:24:57 <Myon>	your schema violates 1NF, btw, you should expand that comman list into separate rows in a new table
Sep 09 11:25:00 <Zr40>	that said, this reeks of bad schema design
Sep 09 11:25:02 <Zr40>	... well, that
Sep 09 11:25:17 <[patrik]>	dreinull: would such a construct suit your need? select s1.* from s1 join s2 on (s1.data LIKE s2.data) and s1.data like '%Foo%';
Sep 09 11:26:56 <Myon>	also, that NULL check smells like a outer (left?) join
Sep 09 11:27:13 <Myon>	(which also smells like bad schema design)
Sep 09 11:41:14 <enoq>	is this good practice? CREATE DOMAIN unsigned_bigint AS bigint CHECK(VALUE >= 0);
Sep 09 11:43:18 <Zr40>	do you expect to get attempts to insert negative values?
Sep 09 11:44:45 <enoq>	it would be bad if it happened
Sep 09 11:48:32 <Myon>	I'd call it nonnegative, but yeah
Sep 09 11:50:41 <enoq>	thank you
Sep 09 12:37:27 <jotauve>	Hi
Sep 09 12:38:16 <jotauve>	I've two tables (every table have a customer_id column), I want to create a view  with a sum of some rows from table A and some rows from B, grouped by customer_id , how can I do it?
Sep 09 12:40:12 <Myon>	jotauve: do you have a query that does what you want?
Sep 09 12:44:33 <harks>	Is there a way to get this simple operation faster? https://paste.depesz.com/s/yE
Sep 09 12:45:19 <incognito>	harks intersect
Sep 09 12:47:23 <harks>	incognito How does intersect help here?
Sep 09 12:48:46 <harks>	Are you suggesting (A union all B ) except (A intersect B ) ?
Sep 09 12:49:41 <incognito>	harks: i was suggesting but this will not be faster
Sep 09 12:51:07 <incognito>	maybe a select count(coalesce(a.str,b.str)) FROM ... FULL OUTER JOIN ON a.str=b.str where a.str is null or b.str is null
Sep 09 12:52:33 <incognito>	harks: btw, it could be interesting to view the diff. of the explain plan of the 2 queries
Sep 09 12:53:05 <harks>	I'm not sure what you are suggesting.
Sep 09 12:54:22 <incognito>	the 2 sets except where the intersect (a=b)
Sep 09 12:57:06 <harks>	Do you mean (A union all B ) except (A INNER JOIN B ) ?
Sep 09 12:58:06 <incognito>	no
Sep 09 12:58:56 <incognito>	(a full outer join b) where (a.str is null or b.str is null)
Sep 09 12:59:10 <incognito>	where the join condition is not satisfied
Sep 09 13:13:20 <harks>	That's a speedup my nearly 30 %. Thanks!
Sep 09 13:13:58 <arrity42>	ohhhh i finally understand lateral joins
Sep 09 13:14:12 <arrity42>	damn i should write an article i have the perfect before and after for the perfect way of understanding them
Sep 09 13:21:45 <avances123>	hi, I have a db with a simple
Sep 09 13:21:48 <avances123>	hi, I have a db with a simple  usage, each 10min, a big (11MB) row is inserted into a table. I have a SSD disk. Which checkpoint/fsync parameters can I tune, to speed to the best, this write?, I mean, y have 9:58 min without any writes,
Sep 09 13:21:50 <avances123>	and I can split this row or change any aplication logic. Thanks for the advices!
Sep 09 13:22:47 <Myon>	avances123: you aren't even close to having any problems
Sep 09 13:22:53 <Myon>	doesn't make sense to optimze that
Sep 09 13:26:29 <tuxinator>	For postgresql using physical replication, is it possible to pull and to push?
Sep 09 13:26:45 <Myon>	it's basically already that
Sep 09 13:26:54 <Myon>	what problem do you have?
Sep 09 13:30:54 <tuxinator>	the current setup comes from a former employee and it is "external server" connects to "internal server" for replication, however it would make the whole thing easier and more secure if it would be like "internal server" connects to "external server" to repicate
Sep 09 13:31:16 <tuxinator>	especially as "internal server" is the rw master and "external server" only the readonly copy
Sep 09 13:33:01 <mage_>	that's how streaming replication works
Sep 09 13:33:01 <Myon>	the connection is initiated from the standby side (put changes are pushed out over that connection)
Sep 09 13:34:26 <tuxinator>	as the master "knows whats written at the moment" doesn't it make sense to send write replicas directly from master to readonly slave?
Sep 09 13:34:44 <Myon>	"put changes are pushed out over that connection"
Sep 09 13:34:50 <Myon>	but*
Sep 09 13:39:04 <tuxinator>	so it is a requirement that the slave initiates the connection?
Sep 09 13:39:20 <Myon>	yes
Sep 09 13:39:50 <Myon>	the primary server doesn't even know the address of the standbys
Sep 09 13:42:44 <tuxinator>	strange, i don't get it why technicaly this should be required
Sep 09 13:43:08 <Zr40>	a replication client is just that, a client
Sep 09 13:43:44 <enoq>	if you don't expect many writes/updates, how good is this solution to handle both autoincrement ids and existing ids https://stackoverflow.com/a/22411460
Sep 09 13:44:12 <Zr40>	theoretically there's nothing fundamentally preventing the primary server from initiating the connection, but nobody was bothered enough to implement it that way
Sep 09 13:44:57 <Myon>	the plain simple explanation is that it isn't implemented
Sep 09 13:45:07 <tuxinator>	Zr40 haha :D
Sep 09 13:45:21 <tuxinator>	Myon: thanks for all that explanations!
Sep 09 13:45:37 <enoq>	tbh I'm kinda wondering how people handle REST PUT and POST calls with sequences
Sep 09 13:46:07 <tuxinator>	in our Setup, master is internal, slave external, so that feature qould make sense
Sep 09 13:46:09 <Berge>	enoq: Handle?
Sep 09 13:46:21 <enoq>	Berge it's a common use case to have both PUT and POST
Sep 09 13:46:31 <Zr40>	what's there to handle?
Sep 09 13:46:37 <Berge>	enoq: sure, but what's the issue with it?
Sep 09 13:46:41 <enoq>	both can create a new entry in your db
Sep 09 13:46:43 <enoq>	ids
Sep 09 13:46:48 <enoq>	POST needs to generate ids
Sep 09 13:46:56 <enoq>	PUT has an id per REST spec
Sep 09 13:46:57 <Myon>	INSERT ... RETURING id
Sep 09 13:47:16 <Berge>	enoq: Yes, sort of akin to UPDATE and INSERT
Sep 09 13:47:29 <enoq>	which isn't the problem
Sep 09 13:47:36 <ufk__>	i
Sep 09 13:47:38 <enoq>	the problem is sequence conflicts
Sep 09 13:47:38 <ufk__>	Hi
Sep 09 13:47:50 <Berge>	enoq: If clients are allowed to create their own IDs?
Sep 09 13:48:02 <enoq>	Berge they are for PUT
Sep 09 13:48:10 <Berge>	Then a sequence likely isn't what you're looking for
Sep 09 13:48:12 <enoq>	since a client can just pass a new id in the URL
Sep 09 13:48:13 <Zr40>	tuxinator: we have a case like that. We use ssh to tunnel the connection, so the external server can just connect to localhost, carrying the connection over the ssh session established by the internal server
Sep 09 13:48:31 <Myon>	nice hack
Sep 09 13:48:37 <Berge>	enoq: It's not common for clients to be able to create their own surrogate IDs like that, is it?
Sep 09 13:48:59 <enoq>	Berge what do you mean?
Sep 09 13:49:21 <ilmari>	either the entity has a natural key provided by the client, or it has a surrogate key generated by the server
Sep 09 13:49:25 <ilmari>	usually not a mix
Sep 09 13:49:28 <Berge>	Exactly
Sep 09 13:49:32 <ilmari>	the entity _type_ that s
Sep 09 13:49:35 <ilmari>	*that is
Sep 09 13:50:03 <Zr40>	enoq: don't use PUT when you meant POST
Sep 09 13:50:24 <enoq>	Zr40 you mean only allow PUT for updating records
Sep 09 13:50:27 <Berge>	enoq: Perhaps you can provide an example
Sep 09 13:50:52 <Zr40>	enoq: replacing records, or creating one for which the identifier is determined externally
Sep 09 13:51:28 <blip99>	hey guys.  I'm trying to learn and use some code to do an upsert.  I don't get what a constraint is when it comes to ON CONFLICT ON CONSTRAINT <constraint+name> DO UPDATE <...>
Sep 09 13:51:43 <blip99>	The field I want to upsert to is jsonb
Sep 09 13:52:07 <blip99>	there's no unique property set on the field
Sep 09 13:52:11 <tuxinator>	Zr40: we do the same, however SSH-Tunnels from a insecure Zone back to a secure one, don't solve a problem, they actually make it worse
Sep 09 13:52:16 <tuxinator>	:D
Sep 09 13:52:25 <enoq>	the real issue there is that these ids then become database internal and you need to find an additional unique key for every record
Sep 09 13:52:27 <blip99>	and I don't know what the constraint name should be for that field?
Sep 09 13:52:27 <Berge>	blip99: UPSERT only makes sense if you have a conflict
Sep 09 13:52:42 <Zr40>	enoq: that makes no sense
Sep 09 13:52:46 <Berge>	blip99: Do have a conflict, you need some set of things that can be in conflict.
Sep 09 13:52:49 <blip99>	am i supposed to manually create a constraint?  I guess I don't get what constraints are properly :D
Sep 09 13:52:49 <enoq>	so what should GET /shops/1 return
Sep 09 13:52:50 <Berge>	(-:
Sep 09 13:52:52 <perry123007>	Hi all, question about INHERITS.  I have a table that I fill every hour with hunderds of entries. Old entries are all deprecated and deleted. Is it a good solution to have two inherited tables and fill table_1, truncate table_2 and then swap them, so I always read form table_1? Both tables with disabled vacuum.
Sep 09 13:53:09 <Berge>	blip99: Typically a UNIQUE contraint or primary key
Sep 09 13:53:19 <blip99>	Berge, ok - how can I say "if there's any value in my 'data' column, consider that a conflict and overwrite it'?
Sep 09 13:53:22 <enoq>	should it return a record with a database generated id "1" or with your second id called external_id "1"
Sep 09 13:53:44 <blip99>	Berge, exactly, my data column isn't a key, nor has UNIQUE set
Sep 09 13:53:56 <Berge>	blip99: Then how will you know which row to UPDATE?
Sep 09 13:53:59 <Myon>	perry123007: prefer partitioning over inheritance
Sep 09 13:54:18 <Myon>	but both have downsides, like renaming will lock the entire hierarchy
Sep 09 13:54:19 <blip99>	unique_id  |  data (jsonb)
Sep 09 13:54:23 <Zr40>	enoq: same thing. The difference is semantic - either you (the database) assigns all ids and you create new records using POST and replace existing ones with PUTH, or the client assigns all ids and you don't use a sequence
Sep 09 13:54:36 <Zr40>	s/PUTH/PUT/
Sep 09 13:54:43 <blip99>	Berge, ^   Inserting into that table, when data already exists - replace (upsert)
Sep 09 13:54:59 <enoq>	Zr40 absolutely
Sep 09 13:55:23 <Berge>	blip99: What's unique_id here?
Sep 09 13:55:29 <Berge>	The jsonb field?
Sep 09 13:55:54 <blip99>	id (unique primary key) | data (jsonb)
Sep 09 13:55:58 <Zr40>	enoq: don't mix the two models for the same resource
Sep 09 13:56:12 <Berge>	blip99: Then your constraint is the primary key
Sep 09 13:56:19 <blip99>	aaaaaah
Sep 09 13:56:28 <perry123007>	Myon: from what I understood, inheritance is way of partitioning
Sep 09 13:56:30 <blip99>	my bad
Sep 09 13:57:04 <Myon>	perry123007: the old way, PG10+ has a better one
Sep 09 13:58:23 <Myon>	perry123007: btw, if you never delete anything, it doesn't make sense to disable autovacuum, so better leave it on (because footgun)
Sep 09 13:58:28 <blip99>	Berge, and in that case should I manually create a constraint? I saw online examples where they do ON CONFLICT ON CONSTRAINT tablename_columnname_key
Sep 09 13:58:44 <Myon>	(and if you do delete things, you also want autovacuum to be on)
Sep 09 13:58:45 <blip99>	I guess this is special syntax tablename_columnname_key
Sep 09 13:58:50 <Berge>	blip99: A primary key _is_ a constraint
Sep 09 13:59:27 <Berge>	The syntax isn't special. Postgres will autogenerate constraint names if they're not given.
Sep 09 13:59:31 <ufk>	i need to group by on one column while postgresql asks me to add all the other columns.. is there a way to overcome this ? I have a shared id on each group of rows from the resulted query and I want the first row for each group of rows that share the same id
Sep 09 13:59:39 <Berge>	On the form tablename_columnname_pkey, iirc
Sep 09 13:59:43 <blip99>	Berge, ah I see.  but how come my DB client doesn't show any constraints defined, it's automatic somehow?
Sep 09 14:00:03 <Berge>	blip99: \d tablename i psql and pastebin the results
Sep 09 14:00:07 <perry123007>	Myon: Ok, thanks
Sep 09 14:00:08 <enoq>	Zr40
Sep 09 14:00:20 <enoq>	Zr40 thank you I think I've figured it out
Sep 09 14:00:49 <enoq>	we will migrate from client ids to database generated ids at some point, init the sequence with the highest id and only use PUT for updates
Sep 09 14:01:14 <enoq>	db imports will be done over separate API calls that rewrite the ids
Sep 09 14:01:41 <enoq>	basically replace ids given by the client with ids generated by the db
Sep 09 14:01:44 <Berge>	enoq: Your clients can easily DoS your service by providing an ID that's the largest that can fit in your datatype, then
Sep 09 14:02:49 <enoq>	Berge right, but that shouldn't happen (it's in local network)
Sep 09 14:03:05 <enoq>	company internal use only
Sep 09 14:03:06 <Berge>	If you can trust your clients, then sure
Sep 09 14:08:19 <ufk>	I want to use window functions for using first_value and let_value based on the data that I ordered and paritioned by it. so lets say if the 'partition by' is returning 6 rows, I only want to handle the first row of each parititon since I use first_value and last_value() functions anyways so the rows are just duplicated. how can I ovecome this ?
Sep 09 14:11:36 <Myon>	maybe you want GROUP BY instead of OVER (PARTITION BY...) ?
Sep 09 14:12:22 <arrity42>	or a subquery join limit 1
Sep 09 14:12:33 <arrity42>	order by desc etc.
Sep 09 14:13:06 <arrity42>	maybe lateral to avoid doing 2 subqueries
Sep 09 14:13:28 <Myon>	your new tool :)
Sep 09 14:13:49 <arrity42>	it's actually interesting the ufk just asked that
Sep 09 14:14:02 <arrity42>	i had to do something almost identical 10 minutes ago
Sep 09 14:15:18 <arrity42>	for a list of threads, i needed either the last reply of the thread, or the thread title, for each
Sep 09 14:16:25 <arrity42>	well my use-case doesn't matter so i won't elaborate too much on it, but needless to say ufk my situation was very close to yours, and i ended up doing a subquery that did order by desc limit 1
Sep 09 14:16:43 <arrity42>	bed time night night
Sep 09 14:21:44 <Gibheer>	is there a way to cast a macaddr type to a bytea to get the last 3 bytes of the mac address?
Sep 09 14:24:54 <Gibheer>	I want to run % on the mac address to let the same mac address be handled by the same thread. But somehow I can't find a function to extract the last 3 bytes from the mac address to maybe get an integer out of it
Sep 09 14:28:39 <gcbirzan>	Gibheer: If you just want an identifier, you can do & '0:0:0:ff:ff:ff'::macaddr.
Sep 09 14:34:05 <Gibheer>	gcbirzan: my plan was to start n threads and then look for mac addresses where the address % n = id
Sep 09 14:46:03 <azuri5>	is it safe to run pg_stat_statements_reset() on a production DB?
Sep 09 14:46:50 <azuri5>	I just upgraded our DB to postgres 11 and the read IOPS are high even after running "analyze verbose"
Sep 09 14:46:58 <Gibheer>	thank you gcbirzan, your hint gave me a different idea select (substring('00:11:22:33:44:55' from 16))::int % 8;
Sep 09 14:49:52 <Myon>	azuri5: if you don't need the old stats, it's safe
Sep 09 14:50:08 <azuri5>	Myon: thanks
Sep 09 14:55:45 <ufk>	Myon, I can't do group by.. cause i need data from first and last row in the partiiton
Sep 09 14:57:39 <ufk>	i can't do limit 1 because then i will get only one line.. and i need one line per shared id
Sep 09 14:58:21 <MatheusOl>	ufk: depending on what you want exactly, you can use row_number (or rank)
Sep 09 14:58:23 <Myon>	then you should maybe show an example
Sep 09 14:58:31 <jerware>	hello
Sep 09 14:58:47 <MatheusOl>	ufk: one doing ORDER BY ... ASC and another doing ORDER BY ... DESC, then get `rn_asc = 1 OR rn_desc = 1`
Sep 09 14:58:57 <MatheusOl>	Unless I misunderstood (quite possible)
Sep 09 15:01:54 <ufk>	ok i have a list of calls.. each call got its own unique id. each call can have more then one line with direction 1 or 2 (related to routing). in each group of call ids, i sort the rows by direction, call time and then for each group i need to get info from the first and last line
Sep 09 15:01:56 <jerware>	SELECT foo UNION SELECT bar ORDER BY id;  -- The ORDER BY doesn't work here.
Sep 09 15:02:16 <ufk>	so i do PARTITION BY a.call_id order by a.direction, a.call_time RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
Sep 09 15:02:56 <ufk>	since I already use first_value and last_value functions, for each call_id  i need to return only one line, and it returns the same amount of lines that I had to begin with. how can I limit it to one line ?
Sep 09 15:03:21 <Myon>	jerware: this should work, but you can't have a ORDER BY on the foo part
Sep 09 15:03:58 <jerware>	(SELECT foo UNION SELECT bar) ORDER BY id;  -- What I'm trying to do.  I want to order the resulting table
Sep 09 15:04:23 <Myon>	that's SELECT foo UNION SELECT bar ORDER BY id;
Sep 09 15:05:44 <MatheusOl>	ufk: you want the fist and last line per call_id? Or just one line?
Sep 09 15:06:54 <MatheusOl>	ufk: e.g. Would that solve your problem? SELECT * FROM (SELECT ..., row_number() OVER(PARTITION BY a.call_id ORDER BY a.direction, a.call_time) AS rn_asc, row_number() OVER(PARTITION BY a.call_id ORDER BY a.direction DESC, a.call_time DESC) AS rn_desc FROM ... ) t WHERE (t.rn_asc = 1 OR t.rn_desc = 1) ??
Sep 09 15:08:35 <ufk>	actually yeah
Sep 09 15:08:51 <ufk>	:) thanks
Sep 09 15:09:45 <MatheusOl>	yw
Sep 09 15:12:49 <tuxinator>	Myon: an idea how much such a replication feature the direction i asked for would cost to develope?
Sep 09 15:14:37 <Myon>	tuxinator: first you'd need to teach the primary to know the addresses of all standby servers
Sep 09 15:15:00 <Zr40>	tuxinator: you've explained what you want, but not why you want it
Sep 09 15:15:18 <Myon>	tuxinator: drafting that, and implementing... a few weeks, spread over several months
Sep 09 15:16:35 <Myon>	tuxinator: a viable option would be to put a pg_receivewal instance into your DMZ, and have the standby server "connect" there
Sep 09 15:16:54 <Myon>	aka cascading replication
Sep 09 15:17:00 <Myon>	that's already a thing now
Sep 09 15:17:19 <tuxinator>	Myon, still the Server in DMZ would need to connect back
Sep 09 15:17:45 <Myon>	that's what DMZs are good for
Sep 09 15:18:30 <tuxinator>	Myon, not really, always better to not connect back and actually we do not need to as we do no writes externaly
Sep 09 15:18:41 <tuxinator>	Myon: always a question of security
Sep 09 15:19:01 <tuxinator>	Myon: we have a Budget for Opensource Developement so we could consider to sponsor
Sep 09 15:19:25 <Myon>	tuxinator: post a draft on pgsql-hackers and ask if anyone is interested
Sep 09 15:19:37 <tuxinator>	Myon: good idea
Sep 09 15:20:13 <tuxinator>	Myon: and thanks for the suggestions, however at our site it's a term of hight security so we try to allways avoid connecting back
Sep 09 15:21:02 <Myon>	also, what works now is to push the wal *files* out, and have the standby read from there
Sep 09 15:21:13 <Myon>	then it's file-based instead of streaming
Sep 09 15:22:30 <tuxinator>	Myon: File-Based = Logical replication right?
Sep 09 15:22:56 <Myon>	you asked for "physical" replication
Sep 09 15:23:33 <tuxinator>	Myon: so what is file-based?
Sep 09 15:24:02 <tuxinator>	Myon: found some doc's , will read through
Sep 09 15:25:03 <Myon>	that's replication in 16MB increments
Sep 09 15:26:20 <blip99>	hi all, i'm trying to understand some postgres code, they do a: DO UPDATE set (product_id, data) = (EXCLUDED.product_id, EXCLUDED.data)
Sep 09 15:26:40 <blip99>	what does the EXCLUDED accomplish?  (this is part of code that should be upserting)
Sep 09 15:27:29 <blip99>	I see excluded in many examples online
Sep 09 15:27:55 <Myon>	you are looking for "on conflict do what I mean"
Sep 09 15:28:04 <Myon>	in practise, you have to spell that out
Sep 09 15:28:16 <blip99>	yes
Sep 09 15:28:44 <blip99>	ON CONFLICT ON CONSTRAINT product_pkey DO UPDATE set (product_id, data) = (EXCLUDED.product_id, EXCLUDED.data)
Sep 09 15:29:17 <blip99>	Myon, ^   I just don't understand the do update set - is it over-writing? Who is this EXCLUDED table/schema
Sep 09 15:29:42 <Myon>	that's the data that failed to INSERT
Sep 09 15:29:49 <blip99>	aaah
Sep 09 15:29:55 <blip99>	ok perfect.
Sep 09 15:30:06 <blip99>	it failed to insert because of a conflict, so we FORCE inserted it :)
Sep 09 15:30:12 <blip99>	great
Sep 09 15:30:16 <Myon>	it's not automatic because you might want to increment some count instead
Sep 09 15:32:40 <blip99>	ah I see. makes sense
Sep 09 15:32:47 <blip99>	thanks
Sep 09 16:17:37 <c355e3b>	Has anyone ever seen pg_dump not include `DEFAULT ...` when dumping tables?
Sep 09 16:19:52 <ilmari>	c355e3b: only if the columns don't have defaults
Sep 09 16:20:27 <c355e3b>	Ok, ill keep digging into it
Sep 09 16:20:43 <ilmari>	are you sure it's dumping the table you think it is?
Sep 09 16:20:49 <ufk>	is there something like timezone() function that does inverse timezone ? :)
Sep 09 16:20:50 <c355e3b>	Yeah its including the table
Sep 09 16:20:59 <ilmari>	are you sure it's the _right_ table?
Sep 09 16:21:08 <c355e3b>	and psql shows me the columns
Sep 09 16:21:33 <c355e3b>	I'm telling it to dump the entire schema without data
Sep 09 16:21:35 <ilmari>	what does \d <schema>.<table> show a default value?
Sep 09 16:22:15 <c355e3b>	`nextval('dcv.batch_estimate_closures_id_seq'::regclass)`
Sep 09 16:22:21 <c355e3b>	So its there
Sep 09 16:22:58 <ilmari>	pg_dump might be setting the default separately using ALTER TABLE ... ALTER COLUMN ... SET DEFAULT
Sep 09 16:27:12 <rzglx>	i didn't see a good explanation for it in the docs-- is there a page explaining the storage characteristics of text data type?  or any data type in general?
Sep 09 16:27:46 <rzglx>	for example, in sql server a varchar(n) data type is defined as n bytes + 2 bytes.  all the things i see for pgsql only describe the maximum for text
Sep 09 16:27:47 <Myon>	rzglx: the keyword is "varlena" storage
Sep 09 16:28:11 <Myon>	varchar and text are the same
Sep 09 16:28:18 <rzglx>	oh ok
Sep 09 16:28:37 <Myon>	also, it's not "n", but whatever number of bytes are required for n *characters* (think utf-8)
Sep 09 16:29:15 <Myon>	and it's not 2, but 1 or 4, and optional compression of the data
Sep 09 16:29:29 <Myon>	??varlena
Sep 09 16:29:30 <pg_docbot>	http://www.varlena.com/GeneralBits
Sep 09 16:29:32 <Myon>	??toast
Sep 09 16:29:32 <pg_docbot>	https://www.postgresql.org/docs/current/static/storage-toast.html
Sep 09 16:29:37 <rzglx>	right sorry, that was for single byte encoding sets only
Sep 09 16:29:45 <rzglx>	thanks for the handy links i'll look through them
Sep 09 16:30:46 <RhodiumToad>	in short it's 1 byte + content length for lengths up to 126 bytes of content, and 4 bytes + content length otherwise
Sep 09 16:31:04 <Myon>	the first link looks like vanity, I'm pondering deleting it
Sep 09 16:31:24 <RhodiumToad>	the short varlenas do not have to be aligned, but the long ones are aligned to 4 bytes
Sep 09 16:32:45 <RhodiumToad>	varlena values in columns of storage types other than "plain" can be toasted (replaced with a pointer to data chunks in a separate toast table) but the threshold for this is based on the size of the whole row, not individual columns
Sep 09 16:34:07 <Myon>	?forget http://www.varlena.com/GeneralBits
Sep 09 16:34:08 <pg_docbot>	Forgot 1 url
Sep 09 16:35:13 <rivyn>	RhodiumToad:  My apologies - I got pulled away last week after I'd asked about the malloc error I have been running into.  Did you have any further insight after I pasted t he meminfo?
Sep 09 16:35:25 <RhodiumToad>	no
Sep 09 16:35:38 <RhodiumToad>	oh, maybe I did
Sep 09 16:35:49 <RhodiumToad>	are you monitoring the committed memory amount at all?
Sep 09 16:36:08 <RhodiumToad>	when you have overcommit off you really need to do that, because trying to monitor "free" memory is misleading
Sep 09 16:36:21 <RhodiumToad>	you can have large amounts of committed memory that is not free
Sep 09 16:36:34 <RhodiumToad>	er, that is not removed from the "free" count
Sep 09 16:38:35 <harks>	Is there a flag to tell pg_dump to omit the schema name?
Sep 09 16:38:53 <RhodiumToad>	no
Sep 09 16:55:16 <CalimeroTeknik>	can this 'natural sort' be used without "create operator class"? http://www.rhodiumtoad.org.uk/junk/naturalsort-hack.sql
Sep 09 16:55:32 <RhodiumToad>	use the other one, not that one
Sep 09 16:56:33 <RhodiumToad>	and no, it can't
Sep 09 16:56:56 <CalimeroTeknik>	I guess the other one is http://www.rhodiumtoad.org.uk/junk/naturalsort.sql (C locale only)
Sep 09 16:57:26 <RhodiumToad>	these days there might be equivalents in ICU, I haven't checked
Sep 09 16:58:42 <xocolatl>	yes, icu is the way I would do this
Sep 09 16:58:51 <xocolatl>	it's even the example in our docs, I believe
Sep 09 16:59:51 <xocolatl>	"CREATE COLLATION numeric (provider = icu, locale = 'en@colNumeric=yes')"
Sep 09 17:01:03 <CalimeroTeknik>	the reason why I'm looking at this, is that naturalsort-hack can't work on amazon RDS
Sep 09 17:01:20 <RhodiumToad>	do they support ICU?
Sep 09 17:01:53 <RhodiumToad>	(needs pg 10+)
Sep 09 17:02:29 <CalimeroTeknik>	I have a 10.6 server on their infrastructure ready to test this [currently reading docs to figure out what icu is]
Sep 09 17:03:00 <RhodiumToad>	I think the  CREATE COLLATION numeric (provider = icu, locale = 'en-u-kn-true');  is the preferred form?
Sep 09 17:04:26 <CalimeroTeknik>	how does this all relate to natural sort in the end?
Sep 09 17:04:52 <RhodiumToad>	you'd use COLLATE "numeric" in the column definition, or  ORDER BY foo COLLATE "numeric"
Sep 09 17:05:38 <CalimeroTeknik>	why the name "numeric" rather than say, "natural"?
Sep 09 17:06:13 <RhodiumToad>	call it whatever you choose
Sep 09 17:06:36 <RhodiumToad>	the name after CREATE COLLATION is the name that appears in the COLLATE clause, but it's otherwise arbitrary
Sep 09 17:07:01 <RhodiumToad>	it's the locale= parameter that defines how it actually behaves
Sep 09 17:08:21 <CalimeroTeknik>	oh, I see, and this 'en-u-kn-true' locale does the weird right thing of ordering by numerical / non-numerical chunks then! wonderful
Sep 09 17:16:09 <RhodiumToad>	it's the -kn-true tag which specifies the numeric behavior
Sep 09 17:17:56 <dreinull>	Myon, [patrik] thanks for your help. I haven't thought of putting it on different rows instead of comma separation. That's a good idea. Why is the null check bad? Do you have a better pattern for instead?
Sep 09 17:17:58 <CalimeroTeknik>	and I take it that 'en@colNumeric=yes' is an alternative, more explicit notation
Sep 09 17:19:04 <Myon>	dreinull: not necessarily bad, depends on the schema and the query (and if the schema is good for that kind of query)
Sep 09 17:19:32 <rivyn>	RhodiumToad, how can I check committed memory?
Sep 09 17:19:46 <dreinull>	well, I decided that no setting is valid for all.
Sep 09 17:19:50 <rivyn>	Is it better to have overcommit enabled?
Sep 09 17:20:07 <rivyn>	I thought that overcommit disabled was supposed to be safer for PostgreSQL
Sep 09 17:20:24 <RhodiumToad>	rivyn: check the committed_as value from meminfo
Sep 09 17:20:25 <rivyn>	to avoid oom-killer
Sep 09 17:20:41 <RhodiumToad>	right, but the downside is that you need to watch committed memory usage vs. swap space
Sep 09 17:20:59 <rivyn>	I see committed_AS as ~9GB
Sep 09 17:21:05 <RhodiumToad>	also you probably want to increase overcommit_ratio to 90
Sep 09 17:21:30 <RhodiumToad>	with overcommit off, malloc fails when it would cause committed_as to exceed commitlimit,
Sep 09 17:21:47 <rivyn>	ok, how does swap factor in?
Sep 09 17:21:50 <RhodiumToad>	which by default is swap size + RAM*(overcommit_ratio/100)
Sep 09 17:22:03 <rivyn>	I could allocate more swap easily enough
Sep 09 17:22:19 <RhodiumToad>	so by default, with overcommit_ratio at 50%, you're limited to swap + half of ram
Sep 09 17:22:58 <RhodiumToad>	I would increase swap size to at least RAM size, as well as increasing overcommit_ratio
Sep 09 17:23:22 <indrek>	What should be reasonable speed to get data from those tables, if you expect to have total around 300M rows of data. Lets say average developer machine. With tables and query like this
Sep 09 17:23:22 <indrek>	https://pastebin.com/19me7dDU
Sep 09 17:23:27 <RhodiumToad>	yes, that means you'll have a ton of swap space allocated that you'll never, ever, use - that's the penalty of disabling overcommit
Sep 09 17:23:55 <indrek>	Here is the explain also of that query
Sep 09 17:23:56 <indrek>	https://explain.depesz.com/s/AHF7
Sep 09 17:24:00 <rivyn>	so would it be recommended to enable overcommit?
Sep 09 17:24:12 <rivyn>	what's the best practice for dedicated PG servers?
Sep 09 17:24:40 <RhodiumToad>	rivyn: safest way is disable overcommit, provide plenty of swap, monitor committed space usage
Sep 09 17:25:10 <RhodiumToad>	also increase overcommit_ratio unless you are short of RAM
Sep 09 17:26:20 <rivyn>	ok
Sep 09 18:32:35 <susenj>	I have a linux host running postgresql. What command should I run to list the existing databases without going to the psql prompt?
Sep 09 18:32:52 <ilmari>	susenj: psql -l
Sep 09 18:33:21 <susenj>	it says `psql: FATAL:  role "root" does not exist`
Sep 09 18:34:03 <depesz>	susenj: try not to work on root.
Sep 09 18:34:04 <ilmari>	sudo -u postgres psql -l
Sep 09 18:34:16 <ilmari>	susenj: which linux distro?
Sep 09 18:34:31 <susenj>	ilmari: Centos 7
Sep 09 18:34:59 <susenj>	depesz, Agree but I was going to automate something and I don't want to go into psql prompt
Sep 09 18:36:11 <ilmari>	susenj: add -l to whatever psql invocation you use to get to the prompt
Sep 09 18:36:17 <ilmari>	then it'll just output the list of databases and exit
Sep 09 18:36:37 <susenj>	perfect. Exacly what I wanted. Thanks a lot
Sep 09 18:36:47 <ilmari>	you possibl want -AXt as well, if you're processing the output
Sep 09 18:43:25 <davidfetter_work>	hi
Sep 09 18:43:35 <susenj>	Wow.. thanks ilmari. so, it removes the formatting..right?
Sep 09 18:46:07 <ilmari>	susenj: -X stops it reading .psqlrc, which might set all sorts of output format switches, -A switches to unaligned mode, -t removes the column header row
Sep 09 18:47:12 <susenj>	Awesome, how about tables? If I want to list the table names in a particular db? Inside psql prompt, we have `\dt;` but not sure about outside
Sep 09 18:49:56 <davidfetter_work>	psql -AtqX '\dt'
Sep 09 18:50:50 <susenj>	davidfetter_work `psql: FATAL:  database "\dt" does not exist`
Sep 09 18:51:08 <davidfetter_work>	susenj, or if you're in an application, SELECT table_name FROM information_schem.tables WHERE table_type='BASE TABLE' AND table_schema::text !~ '^(pg_*|information_schema)'
Sep 09 18:51:19 <davidfetter_work>	psql -AtqXc '\dt'
Sep 09 18:51:21 <davidfetter_work>	sorry
Sep 09 18:51:35 <davidfetter_work>	and that's just for the default db, so
Sep 09 18:51:44 <davidfetter_work>	psql -AtqX -d your_db -c '\dt'
Sep 09 18:53:26 <susenj>	Thanks davidfetter_work. it worked like a charm. Where is this flags cheatsheet? I think I would need this
Sep 09 18:53:32 <xocolatl>	if we're playing golf, I submit  psql -AtqXc '\dt' your_db
Sep 09 18:53:49 <ilmari>	\\dt is shorter than '\dt'
Sep 09 18:54:14 <ilmari>	psql -AXtqc\\dt your_db
Sep 09 18:54:17 <breinbaas>	space betwee c and ' can go
Sep 09 18:54:22 <breinbaas>	right :)
Sep 09 18:55:12 <davidfetter_work>	ugh
Sep 09 18:55:21 <davidfetter_work>	golf is an anti-pattern :P
Sep 09 18:55:40 <breinbaas>	fun-pattern is shorter!
Sep 09 18:56:37 <davidfetter_work>	susenj, the man page has all of them, unfortunately in alphabetical order, which makes them handy as a reference, less so when you need several and don't know what they are.
Sep 09 18:57:50 <susenj>	cool. I will check that too. and thanks ilmari for the shortcut
Sep 09 18:58:44 <ilmari>	susenj: if this is going in a longer-lived script, use the long form of the options
Sep 09 18:59:03 <davidfetter_work>	legibility++ :)
Sep 09 19:00:59 <susenj>	@ilma
Sep 09 19:01:13 <susenj>	ilmari, noted
Sep 09 19:01:53 <ilmari>	golf and clever short option combos are handy for muscle memory and everyday use, not "real" code
Sep 09 19:02:04 *	ilmari uses 'ls -lart' and 'ps faux' a lot
Sep 09 19:10:23 <davidfetter_work>	same here, although I've stopped using "lart" in all other contexts
Sep 09 19:27:20 <davidfetter_work>	??cdc
Sep 09 19:27:20 <pg_docbot>	http://debezium.io/
Sep 09 19:27:54 <davidfetter_work>	anybody used change data capture utilities other than debezium?
Sep 09 19:28:43 *	davidfetter_work was pretty underwhelmed by AWS DMS, and is a little gun-shy as a result
Sep 09 19:29:10 <lluad>	I'm consuming change data from PG into my own app, using a tiny Go library.
Sep 09 19:29:29 <davidfetter_work>	is the library free software?
Sep 09 19:29:38 <lluad>	It is, yeah.
Sep 09 19:29:47 *	davidfetter_work looking to redact a logical copy of a DB in real time
Sep 09 19:29:52 <davidfetter_work>	linky?
Sep 09 19:29:55 <lluad>	Let me see if I can find it (it's not my lib, I'm just using it)
Sep 09 19:32:08 <lluad>	https://github.com/kyleconroy/pgoutput
Sep 09 19:34:40 <davidfetter_work>	?learn cdc https://github.com/kyleconroy/pgoutput
Sep 09 19:34:40 <pg_docbot>	Access denied
Sep 09 19:34:50 <davidfetter_work>	ugh. can somebody with permission do that? :)
Sep 09 19:35:13 <RhodiumToad>	?learn cdc https://github.com/kyleconroy/pgoutput
Sep 09 19:35:13 <pg_docbot>	Successfully added URL with 1 keyword
Sep 09 19:35:17 <davidfetter_work>	thanks!
Sep 09 19:35:54 <davidfetter_work>	so about that parsing hex characters into binary...is this to make COPY work faster on byteas?
Sep 09 19:37:29 <RhodiumToad>	it was just because I was experimenting with parsing the file
Sep 09 19:37:49 <davidfetter_work>	:)
Sep 09 19:37:50 <RhodiumToad>	whether COPY (or byteain generally) needs improvement is another matter
Sep 09 19:38:16 <RhodiumToad>	I suspect that byteain's performance is lost in the noise, but I haven't profiled it
Sep 09 19:38:32 <lluad>	You might find some useful stuff at https://wiki.postgresql.org/wiki/Logical_Decoding_Plugins too
Sep 09 19:38:38 <davidfetter_work>	what would you guess is causing the slowness?
Sep 09 19:38:52 <davidfetter_work>	thanks, lluad!
Sep 09 19:38:57 <RhodiumToad>	what slowness?
Sep 09 19:39:01 <davidfetter_work>	in COPY
Sep 09 19:39:11 <RhodiumToad>	uh
Sep 09 19:39:16 <RhodiumToad>	who said it was slow?
Sep 09 19:39:49 <davidfetter_work>	"I suspect that byteain's performance is lost in the noise, but I haven't profiled it" seems to imply that there's something dominating the time it takes that's not byteain
Sep 09 19:40:40 <RhodiumToad>	well copy has to do a whole bunch of stuff
Sep 09 19:41:01 <RhodiumToad>	reading lines, splitting into fields, calling the input functions, forming tuples, writing them to the table
Sep 09 19:41:41 <whartung>	but thats (mostly) CPU dominated which is less of a problem on modern systems, where I/O is still the primary barrier, yes?
Sep 09 19:42:03 <davidfetter_work>	turns out a lot of DB loads are now CPU-bound
Sep 09 19:42:22 <davidfetter_work>	people are often not on spinning rust
Sep 09 19:43:14 <davidfetter_work>	RhodiumToad, so at least in theory, it should be possible to skip a lot of that by making stuff in the form COPY BINARY expects
Sep 09 19:43:29 *	davidfetter_work looks at the docs for this
Sep 09 19:44:15 <RhodiumToad>	in theory, but what I found for COPY TO was that the BINARY path actually had higher overheads (that shouldn't be so much of an issue for COPY FROM but who knows)
Sep 09 19:44:47 <RhodiumToad>	in particular, Ryu made COPY TO of a table with a lot of float columns run faster in text mode than in binary mode (!)
Sep 09 19:44:58 <whartung>	!!!
Sep 09 19:45:10 <davidfetter_work>	that is so cool :)
Sep 09 19:45:10 <whartung>	Thatis not intuitive.
Sep 09 19:45:23 <davidfetter_work>	have you seen the ryu code?
Sep 09 19:47:03 <RhodiumToad>	whartung: float output in some cases is now faster than bigint output
Sep 09 19:47:18 <whartung>	nice
Sep 09 19:47:19 <RhodiumToad>	though that might say more about bigint output than anything else
Sep 09 19:47:34 <whartung>	by bigint you mean 64b or > 64b int?
Sep 09 19:47:42 <RhodiumToad>	64 bit
Sep 09 19:48:04 <whartung>	thats interesting
Sep 09 19:48:14 <whartung>	floats are 80 bit (right?)
Sep 09 19:48:17 <RhodiumToad>	64
Sep 09 19:48:21 <whartung>	youd think theyd be competeitive
Sep 09 19:48:23 <whartung>	ok
Sep 09 19:48:45 <RhodiumToad>	80 bit was for i387-style long doubles, used for intermediate values
Sep 09 19:49:50 <whartung>	just youd think that the FP to ascii conversion would be comutationlly more complex (however marginally) than INT -> ascii
Sep 09 19:49:58 <RhodiumToad>	ryu gets its speed from a very clever algorithm plus a bunch of micro-optimizations
Sep 09 19:51:02 <whartung>	I wondr at waht point its cheaper to look up values than calculate them when rendering numbers.
Sep 09 19:51:15 <RhodiumToad>	the algorithm combines the process of finding the shortest decimal fraction value within the rounding interval with the process of multiplying by powers of 10 to convert to decimal
Sep 09 19:51:37 <RhodiumToad>	it does use a lookup table, but not a huge one
Sep 09 19:51:53 <whartung>	right, you wouldt need a huge one  diminishing returns very quickly
Sep 09 19:52:04 <DrEeevil>	whartung: you might enjoy reading libgmp ;)
Sep 09 19:52:36 <whartung>	is that the big decimal library?
Sep 09 19:52:40 <RhodiumToad>	ryu's lookup tables are (for 64-bit) 292 and 326 entries of 128 bits each
Sep 09 19:52:52 <whartung>	nice
Sep 09 19:55:27 <whartung>	yea I had never heard of ryu
Sep 09 19:55:49 <RhodiumToad>	https://dl.acm.org/citation.cfm?id=3192369
Sep 09 19:56:53 <RhodiumToad>	thing is, other fast floating-point output functions have tended to be a whole lot more complex
Sep 09 19:57:21 <davidfetter_work>	egad
Sep 09 19:57:26 <davidfetter_work>	here I thought ryu was complex
Sep 09 19:57:33 <RhodiumToad>	not at all
Sep 09 19:57:50 <RhodiumToad>	I guess it's complex by comparison to integer output, but by float standards it's not
Sep 09 20:01:27 <davidfetter_work>	you kinda implied that integer output might be improved, too. probably not worth doing, unless people start to complain
Sep 09 20:03:42 <RhodiumToad>	some of the optimizations that ryu uses, like 2-digit output tables, would work well for integers
Sep 09 20:04:02 *	davidfetter_work reading https://tia.mat.br/posts/2014/06/23/integer_to_string_conversion.html
Sep 09 20:04:34 <RhodiumToad>	I focused on floats because there was evidence that it was an actual pain point for users with large volumes of float data
Sep 09 20:04:47 <davidfetter_work>	right
Sep 09 20:04:55 <RhodiumToad>	as in, result transfers being 3x slower than they should have been
Sep 09 20:05:16 <davidfetter_work>	ouch
Sep 09 20:26:30 <Orez>	is it possible to return raw bytes from a `psql -Atf` call? right now i'm `encode(bytea, 'base64')`-ing my result and then piping through base64 -D
Sep 09 20:32:41 <davidfetter_work>	Orez, is it just one column?
Sep 09 20:32:58 <Orez>	yes, one column one row
Sep 09 20:33:10 <davidfetter_work>	so you don't need it NULL-terminated
Sep 09 20:33:14 <xocolatl>	COPY can do that, I think
Sep 09 20:34:51 <davidfetter_work>	I tried COPY...BINARY, and it includes more stuff
Sep 09 20:34:59 <davidfetter_work>	what should I be doing instead?
Sep 09 20:35:52 <xocolatl>	what was your test case?
Sep 09 20:36:04 <davidfetter_work>	davidfetter=# COPY (SELECT * FROM pwnd WHERE passwd = digest('', 'sha1')) TO stdout BINARY;
Sep 09 20:36:07 <davidfetter_work>	PGCOPY
Sep 09 20:36:10 <davidfetter_work>	
Sep 09 20:36:12 <davidfetter_work>	o M:[2!GDTime: 108.037 ms
Sep 09 20:36:28 <davidfetter_work>	I have a few sha1's expressed as byteas in that DB
Sep 09 20:37:25 <xocolatl>	ah right, it has the whole varlena info in there
Sep 09 20:37:36 <xocolatl>	Orez: I don't think it's possible
Sep 09 20:37:40 <RhodiumToad>	binary format includes a bunch of stuff like field lengths and so on
Sep 09 20:37:47 <RhodiumToad>	it's not expected to print well on a tty
Sep 09 20:39:19 <RhodiumToad>	Orez: you'd have to either post-process the copy-binary format, or stick with base64 or hex
Sep 09 20:39:37 <RhodiumToad>	psql isn't exactly aimed at binary work
Sep 09 20:41:00 <Orez>	dang, that's a shame. i dont know a whole lot about this, but i wonder if there's some encoding i could set on the db to return a "text" of raw bytes?
Sep 09 20:41:21 <RhodiumToad>	how raw?
Sep 09 20:41:28 <RhodiumToad>	do they include null bytes?
Sep 09 20:41:36 <Orez>	ah yeah potentially
Sep 09 20:41:39 <RhodiumToad>	then no
Sep 09 20:42:01 <RhodiumToad>	do you have a specific need to use psql rather than some other client?
Sep 09 20:43:12 <davidfetter_work>	how about decode() ?>
Sep 09 20:44:15 <RhodiumToad>	post-processing the COPY binary format should be pretty simple btw
Sep 09 20:44:33 <davidfetter_work>	byteaout ?
Sep 09 20:44:34 <Orez>	RhodiumToad: no real *need*, just trying to see how far i can push this idea heh. was curious if i could write an `imgcat` function in psql
Sep 09 20:44:53 <RhodiumToad>	ahh
Sep 09 20:45:18 <RhodiumToad>	if you want to take a bytea value from the db and write it to a file on the client, that's actually easier
Sep 09 20:46:11 <RhodiumToad>	(if you're not worried about absolute performance)
Sep 09 20:46:28 <RhodiumToad>	the trick is to create a lo object from the bytea and use \lo_export to write it to a file
Sep 09 20:46:56 <RhodiumToad>	something like,
Sep 09 20:49:16 <RhodiumToad>	begin;
Sep 09 20:49:44 <RhodiumToad>	select lo_from_bytea(img) as o_oid from images where id='whatever' \gset
Sep 09 20:49:57 <RhodiumToad>	\lo_export :o_oid filename.jpg
Sep 09 20:49:59 <RhodiumToad>	rollback;
Sep 09 20:50:17 <RhodiumToad>	(or replace the rollback with  select lo_unlink(:'o_oid'); commit;
Sep 09 20:50:39 <RhodiumToad>	er sorry
Sep 09 20:50:45 <RhodiumToad>	lo_from_bytea(0,img)
Sep 09 20:53:24 <Orez>	ah interesting. that's good to know
Sep 09 20:53:51 <Orez>	thanks all!
Sep 09 20:59:06 <gfree76>	help buffer
Sep 09 20:59:16 <gfree76>	lol
Sep 09 21:47:24 <theseb>	Why does this simple view def give a syntax error?  https://pastebin.com/izHEeWEA
Sep 09 21:48:09 <test91423>	hi. I'm running into an issue where postgres claims 'ERROR:  null value in column "senderid" violates not-null constraint'. however, if I do "SELECT * from table WHERE senderid IS NULL;" I don't get any hits. what am I doing wrong?
Sep 09 21:48:44 <theseb>	Here is the error: https://pastebin.com/ez0t6ep9
Sep 09 21:51:13 <nbjoerg>	theseb: create view ... as select ...
Sep 09 21:51:42 <nbjoerg>	theseb: do you want to create a *table*?
Sep 09 21:51:53 <theseb>	nbjoerg: yea....what if i have data in database to do a select with?  i just want to define a table and then add some rows!? ;)
Sep 09 21:52:07 <theseb>	nbjoerg: i mean...what if i DO NOT have data
Sep 09 21:52:10 <theseb>	to do select with?
Sep 09 21:52:40 <nbjoerg>	theseb: a view by nature is always a query
Sep 09 21:52:52 <nbjoerg>	theseb: if you want a place to temporarily stash data, that's a temp table
Sep 09 21:53:33 <theseb>	nbjoerg: i'm on Apache Spark and when i tried create temp table it said "those aren't supported...try views instead!?!!?!?"
Sep 09 21:53:34 <theseb>	Ahhh!
Sep 09 21:54:26 <nbjoerg>	I can only help you with the Mechanicsburg kind of sparks, sorry
Sep 09 22:00:24 <jdv79>	i have a table with an pgarray of ids for which i have a seperate lookup table.  how could i go about mapping the pgarray over that lookup?
Sep 09 22:01:12 <theseb>	nbjoerg: thanks
Sep 09 22:01:15 <theseb>	again
Sep 09 22:18:26 <jdv79>	nevermind - figured it
Sep 09 22:47:20 <theseb>	help! How can i store and access values WITHOUT touching the database? Why do i care?... i want to do the equivalent of a for loop (i.e. run some SQL code and change some values for each loop)....problem is I don't have perms to create a temporary table! errr.
Sep 09 22:47:58 <nbjoerg>	theseb: pl/pgsql?
Sep 09 22:48:09 <theseb>	perhaps SQL has something like named constants?.... MY_VAR = "foo"; ?
Sep 09 22:48:33 <peerce>	plpgsql does
Sep 09 22:48:34 <nbjoerg>	what's wrong with using your language of the day? e.g. python or whatever?
Sep 09 22:48:48 <peerce>	and change some values, meaning, update stuff ?
Sep 09 22:48:56 <peerce>	or just return modified values ?
Sep 09 22:53:32 <theseb>	nbjoerg: actually....i do have a python script involved here....that isn't a bad idea
Sep 09 22:54:31 <peerce>	theseb; again, when you say, 'store and access' and 'change some values for each loop', do you mean, to do an UPDATE of an existing table with these changed values ?  or what?
Sep 09 22:56:24 <theseb>	peerce: : i have an SQL script that grabs data from a database....i want to get data for say...company X, then company Y, then company Z, etc.
Sep 09 22:57:05 <theseb>	peerce: the database is not storing the specific stuff for companies X, Y and Z....but i don't want to create 3 massive SQL scripts that only vary in a few values that are different for each company
Sep 09 22:57:11 <theseb>	peerce: does that clear it up?
Sep 09 22:58:01 <peerce>	not really.    why can't this just be a SELECT statement that fetches stuff IN (companyX, companyY, companyZ)  ?
Sep 09 22:59:50 <theseb>	peerce: does sql let you do a select from raw data?....e.g. select name, ticker from ( ("GOOG" 333.333) ("IBM" 22.22) ...) etc.?
Sep 09 23:00:00 <theseb>	peerce: something like that WOULD do the trick nicely
Sep 09 23:00:18 <peerce>	theseb; so the data isn't even in the database??
Sep 09 23:00:35 <theseb>	peerce: that's the problem....i have a few values that i can't add to the DB
Sep 09 23:00:55 <peerce>	so where is this data if its not in the database ?
Sep 09 23:01:43 <theseb>	peerce: i have to search and get it from other sources
Sep 09 23:02:02 <peerce>	so why do yuou need to use a sELECT, you've already got it in your app?  just use it.
Sep 09 23:03:34 <theseb>	peerce: is it possible to do a select from a tuple of values instead of a table like my example above?.. select name, ticker from ( ("GOOG" 333.333) ("IBM" 22.22) ...) etc.?
Sep 09 23:03:45 <theseb>	peerce: or some similar goo?
Sep 09 23:03:53 <peerce>	why would you send data to the database just to read it back  ?
Sep 09 23:04:11 <peerce>	[if you're not storing it in the database, I mean...]
Sep 09 23:04:13 <RhodiumToad>	theseb: yes
Sep 09 23:04:44 <RhodiumToad>	select name,ticker from (values ('GOOG',333.333),('IBM',22.22),...) as v(name,ticker);
Sep 09 23:04:56 <Xgc>	<table value constructor>
Sep 09 23:04:59 <RhodiumToad>	though arrays are more efficient
Sep 09 23:05:06 <RhodiumToad>	or json
Sep 09 23:05:48 <theseb>	RhodiumToad: nice...beautiful
Sep 09 23:06:16 <theseb>	peerce: there are more complicated SQL scripts i need to run for various changing values
Sep 09 23:06:24 <RhodiumToad>	select key as name, value as ticker from json_each('{"GOOG":333.333, "IBM":22.22, ...}');
Sep 09 23:07:15 <RhodiumToad>	the advantage of using json is that the data can be passed in as a single literal; long VALUES clauses are quite inefficient to parse
Sep 09 23:07:44 <RhodiumToad>	er, probably want json_each_text there
Sep 09 23:08:12 <RhodiumToad>	or you can do things like,
Sep 09 23:08:57 <RhodiumToad>	select name, ticker from json_to_recordset('[{"name":"GOOG", "ticker":333.333}, {...}, ...]') as j(name text, ticker numeric);
Sep 09 23:15:18 <peerce>	so you're doing joins with this app supplied data and other existing tables?     hopefully there's not too much of this supplied data, json or otherwise, as that sort of construct is not indexed, so complex operations on it may not perform very well
Sep 09 23:18:14 <theseb>	peerce: here is my SQL code... https://pastebin.com/YydDrUgD ... see those hex strings? I basically want to run the same code and just modify those hex strings each time .. and store the result of each run
Sep 09 23:18:20 <theseb>	peerce: that's the deal....nothing hidden
Sep 09 23:19:46 <RhodiumToad>	you have "..." there which should be '...'
Sep 09 23:20:18 <RhodiumToad>	if these are binary values why are you storing them as hex rather than bytea?
Sep 09 23:20:55 <theseb>	RhodiumToad: dunno..i didn't set up the database
Sep 09 23:24:36 <peerce>	why not just pass those as arguments to the queries then?     WHERE .... = LOWER($1) AND ... = LOWER($2) AND ...
Sep 09 23:26:38 <theseb>	peerce: because i've never learned the WHERE before? but i think that will be perfect
Sep 09 23:26:47 <theseb>	time to learn it...;)
Sep 09 23:26:52 <theseb>	i <3 SQL
Sep 09 23:26:58 <theseb>	SQL forevar
Sep 10 01:19:12 *	ChanServ gives channel operator status to xocolatl
Sep 10 01:19:16 *	xocolatl has changed the topic to: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 10 01:19:21 *	ChanServ removes channel operator status from xocolatl
Sep 10 01:23:04 <davidfetter_work>	w00t!
Sep 10 02:15:09 <ningu>	is there a guide somewhere to dos and don'ts for working with json data?
Sep 10 02:15:37 <ningu>	I don't particularly _want_ to use json for an upcoming project but we'll be dealing with a certain thing that comes in a variety of formats so it's kind of the only/best choice
Sep 10 02:16:00 <ningu>	but I'd like to be sure I understand better what tradeoffs I'll be making, like, if I want to query it a certain way, if that will be really annoying or what
Sep 10 02:16:06 <ningu>	or if there are ways to make it a little easier
Sep 10 02:16:29 <RhodiumToad>	querying for containment is reasonably flexible with a gin index
Sep 10 02:16:49 <RhodiumToad>	querying for things like inequality comparison on deeply nested keys is a pain
Sep 10 02:17:49 <ningu>	RhodiumToad: how about "full text search" within a json record? meaning like match any value (not key)?
Sep 10 02:18:04 <RhodiumToad>	that's extremely painful
Sep 10 02:18:12 <ningu>	what about if it's not nested?
Sep 10 02:18:21 <RhodiumToad>	still painful
Sep 10 02:18:26 <ningu>	hrm ok
Sep 10 02:19:01 <xocolatl>	hey RhodiumToad, do you remember a while ago I asked about C-level scanning through an index or seqscan?  you said only CLUSTER made a clumsy decision about that and I should go through SPI.  I just found systable_beginscan (in src/backend/access/index/genam.c) that seems to do what I was looking for.
Sep 10 02:19:08 <ningu>	so how would you handle that case? you have records that are basically freely defined key/value pairs, and you need to store them (associated with some more regularly structured data), and you need to query to see if any value matches
Sep 10 02:19:24 <ningu>	RhodiumToad: I mean something like EAV can do it, I just know everyone says no (and I understand why)
Sep 10 02:20:13 <RhodiumToad>	xocolatl: that only seqscans when forced to
Sep 10 02:21:08 <ningu>	well honestly, it wouldn't be that hard to denormalize and add a field that has a concatenation of all the values
Sep 10 02:21:36 <ningu>	and just use it for matching, then use the actual data (in json) for display
Sep 10 02:23:33 <ningu>	RhodiumToad: is that maybe the least bad option for this?
Sep 10 02:23:49 <ningu>	could use trgm index maybe
Sep 10 02:23:53 <RhodiumToad>	depends what the objective is
Sep 10 02:24:18 <ningu>	the objective is mainly to determine, "does any value of these key/value pairs contain the given search term(s)?"
Sep 10 02:24:34 <RhodiumToad>	yes, but why
Sep 10 02:25:48 <ningu>	each set of key/value pairs is a dictionary entry. there's a word in some language being described, and its translation into english or similar major language. but there can be a lot of fields like gloss, definition, etymology, notes, etc. the search will be for someone saying, give me all the words in the database that contain the word "drink" somewhere in their entry
Sep 10 02:26:00 <ningu>	we need to preserve the original structure of the entry in case the user wants to inspect it manually
Sep 10 02:26:20 <ningu>	we will extract as much regular data as possible but that will be limited by the way these are structured
Sep 10 02:27:11 <RhodiumToad>	where "drink" is matches exactly as a word, or stemmed tsearch-style, or matched by substrings?
Sep 10 02:27:37 <ningu>	yeah, I thought of that. matches exactly as a word is definitely the simplest case and probably all we need to support.
Sep 10 02:27:56 <ningu>	it would be a regex where the word is flanked by \b, or similar.
Sep 10 02:28:06 <ningu>	so like it would match "drink-water" but not "drinking"
Sep 10 02:28:14 <RhodiumToad>	have a function that returns a flat list of all the words in the entry as an array, and index that too
Sep 10 02:28:27 <ningu>	hmm... ok
Sep 10 02:28:38 <ningu>	so it will come down to just whether the array contains the word, cool
Sep 10 02:29:02 <ningu>	that would work well
Sep 10 02:29:13 <ningu>	what about the stemming case, just curious?
Sep 10 02:30:19 <RhodiumToad>	well then you'd return it as a tsvector not an array
Sep 10 02:31:45 <ningu>	ah ok, right, but I guess you'd generate the tsvector from all the unique words in the record
Sep 10 02:49:32 <velix>	Sorry for abusing you as doc, but what was the function to count NULLIF over columns?
Sep 10 02:50:03 <velix>	oh wait, let's do it the easy way: COUNT(*) FILTER (WHERE a IS NULL) ;)
Sep 10 02:51:28 <velix>	num_nulls()!
Sep 10 02:52:47 <peerce>	count(1)-count(field)
Sep 10 02:53:02 <peerce>	since count(field) is the number of not nulls...
Sep 10 02:53:12 <RhodiumToad>	num_nulls is for multiple arguments, not an aggregate
Sep 10 02:53:19 <RhodiumToad>	count(*)-count(field)
Sep 10 02:53:25 <peerce>	yeah, thats why I suggested the other.
Sep 10 02:53:47 <RhodiumToad>	or  count(nullif(field is null,false))
Sep 10 02:53:53 <velix>	so num_nulls(col1, col2, col3) doesn't work?
Sep 10 02:54:02 <RhodiumToad>	or  sum((field is null)::integer)
Sep 10 02:54:17 <RhodiumToad>	velix: it does work, but I thought you wanted an aggregate?
Sep 10 02:54:18 <xocolatl>	eww on that last one
Sep 10 02:54:30 <RhodiumToad>	oh, you said "over columns"?
Sep 10 02:54:44 <RhodiumToad>	so count() was the wrong approach anyway
Sep 10 02:54:52 <velix>	RhodiumToad: Yes, not over rows. But since we're talking about it: can't I sum(num_nulls) ?
Sep 10 02:55:04 <RhodiumToad>	only by casting to integer
Sep 10 02:55:07 <velix>	sure
Sep 10 02:55:18 <RhodiumToad>	oh wait, it is integer
Sep 10 02:55:21 <velix>	:D
Sep 10 02:55:54 <velix>	PostgreSQL has solutions for problems I don't have. Now that's cool.
Sep 10 03:46:20 <maybefbi>	has anyone used postgrest? what do you think of it?
Sep 10 03:50:52 <peerce>	I haven't, and I think its a solution in search of a problem.   I think the REST thing belongs between the clients and the appserver where you implemnt your business logic.    the app server should use direct SQL connections since it can and they give the best performance.
Sep 10 03:51:41 <maybefbi>	peerce: by direct SQL connections do you mean no ORM?
Sep 10 03:51:57 <peerce>	I haven't found an ORM yet that didn't end up pissing me off.
Sep 10 03:52:03 <maybefbi>	i agree.
Sep 10 04:16:15 *	wfpkhc wanders on in
Sep 10 04:16:17 <wfpkhc>	hello
Sep 10 04:16:39 <RhodiumToad>	good evening
Sep 10 04:16:47 *	wfpkhc hugs RhodiumToad!
Sep 10 04:16:52 <wfpkhc>	your still around! yaay!
Sep 10 04:18:00 <wfpkhc>	may i please ask the following: if i have a left join on a table  - and im using a "Case when COLUMN is null"  - is there anyway to tell if it "is  null and the column id is a certain id?
Sep 10 04:18:43 <RhodiumToad>	er, yes?
Sep 10 04:18:48 <RhodiumToad>	what is the whole query?
Sep 10 04:19:18 <wfpkhc>	errrm let me write it out in a simple form its a ugly query  like its creator - 1 second please
Sep 10 04:23:44 <wfpkhc>	https://dpaste.de/m8zy
Sep 10 04:24:33 <wfpkhc>	https://dpaste.de/rxAh
Sep 10 04:24:43 <wfpkhc>	there is a more cleaneded up one
Sep 10 04:25:50 <wfpkhc>	what i would like to try and do is - if the alias is null and the id == 5 then display the word "system" else display unknown
Sep 10 04:30:09 <RhodiumToad>	hm
Sep 10 04:30:37 <wfpkhc>	i have to go
Sep 10 04:30:40 <wfpkhc>	im sorry
Sep 10 04:30:44 <wfpkhc>	will come back later
Sep 10 04:30:54 *	wfpkhc hugs RhodiumToad and runs off hoping he remembers me
Sep 10 04:30:58 <RhodiumToad>	easy enough with another CASE
Sep 10 05:24:22 <Exuma>	how can I combine 2 table results (from 2 matviews) into another view
Sep 10 05:24:31 <Exuma>	ideally i could map columns
Sep 10 05:24:40 <Exuma>	or somehow exclude columns from one
Sep 10 05:24:46 <Exuma>	but i want A + B results
Sep 10 05:26:02 <FunkyBob>	you mean like a UNION ?
Sep 10 05:27:18 <Exuma>	FunkyBob hmm perhaps
Sep 10 05:27:26 <Exuma>	?? union
Sep 10 05:27:26 <pg_docbot>	https://www.postgresql.org/docs/current/static/queries-union.html
Sep 10 05:29:12 <Exuma>	oh awesome, yeah jus tlike that. thanks FunkyBob
Sep 10 05:33:43 <Exuma>	UNION ALL actually
Sep 10 06:05:23 <davidfetter>	hi
Sep 10 06:07:36 <davidfetter>	in a system where there's a lot of Ruby on Rails, does it make sense to wrap a simple pwnd(sha1 bytea) function that returns bool in a https endpoint, or a special-purpose DB endpoint?
Sep 10 07:15:56 <indrek>	How do you understand what is postgersql actually doing? I have table A and daily summary table B. I inserted rows to table A from old table A (i did partitions) and now when i look at \dt+ i see that size is not changing any more in Tables A, but it's table B size is changing. Table B gets updated with after trigger. So my question is what is pg actually doing?
Sep 10 07:16:34 <indrek>	Is it triggering all after insert triggers after all data is inserted? Or it's actually doing an insert and triggering insert to table B?
Sep 10 07:36:09 <davidfetter>	indrek, with native partitioning, only the leaves can have data
Sep 10 07:36:41 <davidfetter>	is there some reason the current query to pg_stat_activity shows up in pg_stat_activity by default?
Sep 10 07:36:46 <indrek>	yes, only the leaves have data
Sep 10 07:37:06 *	davidfetter constantly putting pid <> pg_backend_pid() in WHERE clauses, and it's kinda tedious
Sep 10 07:37:24 <indrek>	the query i executed shows up in pg_backend_pid
Sep 10 07:38:02 <indrek>	with a state active
Sep 10 07:38:11 <indrek>	so i suppose, it's running
Sep 10 07:38:15 <davidfetter>	the query on pg_stat_activity in the current session? yes, and that's pretty useless as far as I've seen so far
Sep 10 07:38:26 <indrek>	yes
Sep 10 07:38:31 *	davidfetter thinks it shouldn't be part of that view
Sep 10 07:38:38 <indrek>	why?
Sep 10 07:39:03 <davidfetter>	what conceivable use would seeing the query on pg_stat_activity that you just wrote have?
Sep 10 07:40:11 <indrek>	to see it it's waiting on some locks etc
Sep 10 07:40:43 <davidfetter>	have you ever seen such a query wait on locks in its own output?
Sep 10 07:41:12 <indrek>	not on it's own
Sep 10 07:41:22 <indrek>	but others waiting on your query
Sep 10 07:42:37 <davidfetter>	you've seen this?
Sep 10 07:43:00 <davidfetter>	by the time it produces rows, such conflicts are by definition resolved :P
Sep 10 07:44:12 <indrek>	actually i have
Sep 10 07:44:28 <indrek>	because years ago someone made a script on update
Sep 10 07:44:32 <indrek>	and it didn't have filter
Sep 10 07:44:44 <indrek>	so with each row it updated all rows and it was like a snowball
Sep 10 07:44:54 <indrek>	finally i had so many rows to update that it took minutes
Sep 10 07:54:33 <davidfetter>	ok
Sep 10 07:54:47 *	davidfetter still thinks it wouldn't hurt to leave it out
Sep 10 08:12:27 <davidfetter>	hrm. I've got a hash index and a unique b-tree index on the same column. before, the hash index was built, queries like SELECT EXISTS(SELECT 1 FROM pwnd WHERE passwd=digest('foobar','sha1')) would use the b-tree index and run in <= 3ms
Sep 10 08:12:50 <davidfetter>	with the hash index there, such queries use it instead and run in <= 5ms :/
Sep 10 08:13:08 <aypea[0]>	wow. that said, why both?
Sep 10 08:13:19 <davidfetter>	i wanted to see which was faster
Sep 10 08:13:23 <aypea[0]>	ah :)
Sep 10 08:13:30 <davidfetter>	there's only that query that's going to run against this DB
Sep 10 08:14:18 <aypea[0]>	only place I've used hash indexes in was where it made inserts a ton faster (eventually, once the table got big enough). not sure if it was due to structure or size (the hash index was around 1/3rd the size of btree)
Sep 10 08:14:46 <aypea[0]>	didn't care about search speed as that, at least, made the system work and saved a few TB of disk
Sep 10 08:15:15 <davidfetter>	theoretically, this is perfect for hash indexes--equi-joins on fixed strings only, the hash index is ~3/4 of the size of the B-tree index
Sep 10 08:15:55 <davidfetter>	practically, though, the b-tree index is faster
Sep 10 08:16:17 <aypea[0]>	my use-case was equality comparisons of fixed-width strings (generally) :)
Sep 10 08:16:45 <davidfetter>	pretty similar here. I'm comparing byteas (sha1s of pwnd passwords)
Sep 10 08:16:59 <aypea[0]>	hahahaha. same except I went with sha512 :)
Sep 10 08:17:03 <davidfetter>	they are of course fixed length
Sep 10 08:17:21 <davidfetter>	I got this big DB of sha1s of leaked passwords
Sep 10 08:17:34 <davidfetter>	three wits used "correct horse battery staple"
Sep 10 08:17:49 <aypea[0]>	xkcd passwd :)
Sep 10 08:17:53 <davidfetter>	exactly
Sep 10 08:19:19 <aypea[0]>	it was a cunning plan but, I believe, the list of 4 likely human friendly words is a lot smaller than the list of 4 words.
Sep 10 08:20:21 *	davidfetter wonders how the de-duplicated b-tree indexes Anastasia and Peter are working on would do in this case
Sep 10 08:20:39 <aypea[0]>	de-duplicated?
Sep 10 08:21:12 <davidfetter>	per the latest commit message, "Add deduplication to nbtree"
Sep 10 08:22:01 <davidfetter>	at least in theory, this shouldn't benefit from that particular optimization, but it might benefit from other ones they're tucking into this patch
Sep 10 08:22:48 *	davidfetter just a little lazy about recompiling, reloading 20something GB of stuff, then indexing it
Sep 10 08:24:03 <aypea[0]>	hrm. can't see the commit in gitlandia.
Sep 10 08:43:00 <davidfetter>	patch hasn't landed yet
Sep 10 08:43:39 <davidfetter>	Message-ID: <CAH2-Wzm=9TnAFGCDfvsBVC5zYonQqeLMmYpnx=xZ3nyXOeHjNA@mail.gmail.com>
Sep 10 08:44:21 <davidfetter>	https://www.postgresql.org/message-id/flat/CAH2-Wzm%3D9TnAFGCDfvsBVC5zYonQqeLMmYpnx%3DxZ3nyXOeHjNA%40mail.gmail.com#c0c84272254c067ac467a5c75ab32dd9
Sep 10 08:44:23 <aypea[0]>	ah
Sep 10 08:46:09 <aypea[0]>	thanks for the url :)
Sep 10 08:46:56 <davidfetter>	most of what's in that thread is *way* over my head. I follow these things just enough to put them in the weekly newsletter, as best I understand them
Sep 10 08:47:06 <davidfetter>	sometimes, my best isn't quite good enough
Sep 10 08:54:27 <aypea[0]>	same :)
Sep 10 09:50:32 <ysch>	davidfetter: Which PostgreSQL version was it (for hash indexes testing), just curious? And approx. index sizes, RAM, and were those index-only scans for b-tree?
Sep 10 10:02:19 <johnny_b>	hello
Sep 10 10:02:20 <davidfetter>	ysch, 13 as of a couple of weeks ago
Sep 10 10:02:47 <davidfetter>	16GB for hash, 21GB for b-tree
Sep 10 10:03:13 <davidfetter>	16G RAM
Sep 10 10:03:20 <davidfetter>	and yes
Sep 10 10:03:39 <davidfetter>	johnny_b, good
Sep 10 10:03:41 <davidfetter>	sorry
Sep 10 10:03:57 <johnny_b>	np, it happens 8)
Sep 10 10:06:21 *	davidfetter bumps a couple of chuck berry tracks
Sep 10 10:09:02 <johnny_b>	davidfetter: nice 8)
Sep 10 10:09:38 <ysch>	davidfetter: "Yes" for IOS, right? Thanks for the info!
Sep 10 10:09:53 <johnny_b>	i'd like to use the hungarian hunspell files for full text search but it can't be imported because the UTF-8 formatted affix file has non-UTF-8 chars (https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=737049). how can i make them work?
Sep 10 10:09:54 <davidfetter>	right
Sep 10 10:10:22 <peerce>	what charset are these non-utf8 chars in ?
Sep 10 10:10:41 <peerce>	and is there some consistent way you can tell whats what?
Sep 10 10:10:45 <Myon>	that file is totally crazy
Sep 10 10:11:15 <johnny_b>	Myon: have you seen it?
Sep 10 10:11:31 <Myon>	johnny_b: I'm the one commenting in the last message in that bug
Sep 10 10:11:43 <johnny_b>	fabulous
Sep 10 10:12:31 <johnny_b>	it seems that comments are still in ISO-8859-1 or 2
Sep 10 10:12:31 <Myon>	I don't know the affix file format in detail, but I'd think you could just replace <ff> by some other character
Sep 10 10:15:03 <johnny_b>	i see
Sep 10 10:21:20 *	Disconnected ()
**** ENDING LOGGING AT Tue Sep 10 10:21:20 2019

**** BEGIN LOGGING AT Tue Sep 10 10:21:44 2019

Sep 10 10:21:44 *	Now talking on #postgresql
Sep 10 10:21:44 *	Topic for #postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 10 10:21:44 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Tue Sep 10 01:19:16 2019)
Sep 10 10:21:44 *	Channel #postgresql url: https://www.postgresql.org
Sep 10 10:24:01 <Myon>	"just" appears to be a horrible mess
Sep 10 10:24:14 <denaras>	Hey - we are at 11.5 Postgresql, and we wonder how we can scale our database. Most usage is for read heavy workloads. We are thinking to setup stream replications and have one or two replicas. I just wonder is there anything better for 11.5 (compared to 9.3)?
Sep 10 10:24:17 <Myon>	REP ^orr orv # orrvadsz->orvvadsz
Sep 10 10:24:17 <Myon>	AF 1478
Sep 10 10:24:17 <Myon>	AF Vj<D7>L<D3>n<F2><E9><E8><B3><C4><E4>TtYc<B8><BC>l # 1
Sep 10 10:24:42 <Myon>	denaras: that'd be the way for read scaling, yes
Sep 10 10:24:59 <Myon>	or simply put in more cores and more RAM
Sep 10 10:26:14 <denaras>	Myon: thanks, we are taking right now backups from primary server for 10 hours :(. So yeah replica will solve this. I just wonder few more questions
Sep 10 10:26:38 <denaras>	How we know what replica is in sync? How we can resync? What will happen if let say I turn off replica for 1 day and start again ?
Sep 10 10:26:48 <Myon>	denaras: pg_stat_replication
Sep 10 10:26:56 <Myon>	??replication
Sep 10 10:26:56 <pg_docbot>	http://momjian.us/main/writings/pgsql/replication.pdf :: http://wiki.postgresql.org/wiki/Streaming_Replication
Sep 10 10:26:56 <johnny_b>	Myon: yes, unfortunately it is 8(
Sep 10 10:26:56 <pg_docbot>	http://wiki.postgresql.org/wiki/Binary_Replication_Tutorial :: http://repmgr.org/
Sep 10 10:26:56 <pg_docbot>	https://www.postgresql.org/docs/current/static/protocol-replication.html :: https://wiki.postgresql.org/wiki/Trigger-based_Replication
Sep 10 10:26:56 <pg_docbot>	https://www.postgresql.org/docs/current/static/high-availability.html :: https://www.postgresql.org/docs/current/static/warm-standby.html#SYNCHRONOUS-REPLICATION
Sep 10 10:26:59 <pg_docbot>	https://www.postgresql.org/docs/current/static/standby-settings.html#RECOVERY-MIN-APPLY-DELAY
Sep 10 10:27:10 <denaras>	thanks
Sep 10 10:27:11 <Myon>	johnny_b: is aspell any better?
Sep 10 10:28:54 <johnny_b>	Myon: i dunno , hunspell was the first choice because it's up-to-date
Sep 10 10:29:03 <denaras>	Myon: what would happen if we stop replica for 1 day and we start it again? Will we need to start over - clear data folder, do pg_basebackup and setup everything again ?
Sep 10 10:29:21 <Myon>	denaras: you need to keep WAL for one day around
Sep 10 10:29:32 <Myon>	using "replication slots" or a "wal archive"
Sep 10 10:40:13 <peerce>	denaras; note that a day of WAL couold ee a lot of storage ona busy write intensive server
Sep 10 10:41:03 <peerce>	with replication slots, that day would be stored in your pg_wal folder under the pgdata main data directory.
Sep 10 10:41:30 <peerce>	with a wal archive, it could be on a seperate storage server
Sep 10 10:48:30 <blip99>	guys how do i reference fields in a jsonb field?
Sep 10 10:48:49 <blip99>	tablename.data->name doesn't cut it
Sep 10 10:49:05 <Myon>	->'name'
Sep 10 10:49:08 <incognito>	is there are a builtin tool a parallelize the copy of pg backup ? (with : compression)
Sep 10 10:49:25 <Myon>	pg_dump -Fd -j8
Sep 10 10:49:30 <davidfetter>	pg_dump -Fd -j $(nproc)
Sep 10 10:49:39 <incognito>	network copy also
Sep 10 10:49:45 <Myon>	add -h
Sep 10 10:49:54 <incognito>	yes, i only see pg_dump -j8 && rsync ...
Sep 10 10:49:55 <blip99>	Myon, ah. perfect. and seems like ->> removes the quotes from the string
Sep 10 10:50:01 <Myon>	the compression is on the local side only, though
Sep 10 10:50:15 <davidfetter>	blip99, ->> gets text instead of JSON[B]
Sep 10 10:50:23 <blip99>	thanks david
Sep 10 10:50:51 <incognito>	no extension that are automatically doing : pg_start_backup() && rsync .. compressed ... && pg_end_backup() ?
Sep 10 10:51:50 <Myon>	pgbackrest
Sep 10 10:54:13 <xocolatl>	it would be nice if pg_basebackup had a -j option
Sep 10 10:55:05 <davidfetter>	^^^^^^
Sep 10 10:55:15 <davidfetter>	aiui, people are working on it :)
Sep 10 10:55:59 <incognito>	note for myself: read pg_backrest doc :)
Sep 10 10:56:18 <xocolatl>	incognito: barman can also do it
Sep 10 11:01:54 <pascalou>	hello
Sep 10 11:03:19 <pascalou>	max conn is 200, all roles connlimit is -1 ; all db are datallowconn =t, why would  get a to many connection with a user?
Sep 10 11:03:49 <xocolatl>	because you have 200 connections already?
Sep 10 11:03:54 <pascalou>	no , 2
Sep 10 11:04:09 <xocolatl>	what is the exact error?
Sep 10 11:05:16 <pascalou>	user=sssss,db=nnnnn,app=[unknown],client=xxxxx FATAL:  too many connections for database "zzzzz"
Sep 10 11:09:02 <ysch>	pascalou: What's datconnlimit for the database "zzzzz" (in pg_database)?
Sep 10 11:09:56 <davidfetter>	I was hoping to make printing integers more efficient. make goes without errors, but "make check" bombs out immediately with a segfault. http://paste.ubuntu.com/p/x3w7BmrCG3/
Sep 10 11:09:59 <pascalou>	ok , thanks
Sep 10 11:10:09 <pascalou>	that was it
Sep 10 11:10:33 <Myon>	I'm not sure if I knew about that parameter
Sep 10 11:11:03 <pascalou>	we usualy never set it , usually we set datallowconn=f
Sep 10 11:12:35 <xocolatl>	you used to have to update the catalogs to set it
Sep 10 11:12:41 <xocolatl>	so I wrote a patch :)
Sep 10 11:16:00 <davidfetter>	anybody seeing something obvious that I missed?
Sep 10 11:32:51 <johnny_b>	Myon: there are other problematic byte sequences too, not just 0xff
Sep 10 11:33:41 <Myon>	johnny_b: there must be source code for that file, that's definitely not written by hand
Sep 10 11:34:01 <Myon>	johnny_b: maybe fixing the "compiler" not to use funky bytes is feasible
Sep 10 11:34:13 <Myon>	given that only Hungarian has that problem
Sep 10 11:34:42 <johnny_b>	yeah 8(
Sep 10 11:56:40 <incognito>	
Sep 10 12:34:08 <zerowalker_w>	hi, anyone on windows or know how to make ssl work? I am self signing a certificate and giving it a ssl_key_file and ssl_cert_file file and ssl=on. But it won't start and i get no logs, it will start if i don't give nay key/cert but with unknown authority which make sense. Any idea what i can be missing, i am using openssl. "openssl req -new -x509 -days 365 -nodes -text -out server.crt   -keyout server.key -subj "/CN=localhost""
Sep 10 12:34:44 <zerowalker_w>	it's probably obvious, i always have problems with certificates, especially on postgres, so i have probably asked this before:(
Sep 10 12:36:50 <Myon>	zerowalker_w: try launching postgres.exe directly
Sep 10 12:37:05 <Myon>	somethings that gives some error messages that don't appear in the normal log
Sep 10 12:37:15 <Myon>	(are you looking into the correct log file?)
Sep 10 12:37:43 <zerowalker_w>	i am just looking in the logs folder, i wil ltry opening it directly
Sep 10 12:37:58 <zerowalker_w>	though thing is it's a service
Sep 10 12:38:43 <zerowalker_w>	log* folder
Sep 10 12:38:43 <Myon>	postgres.exe -D c:\whatever\data\directory
Sep 10 12:41:33 <zerowalker_w>	hmm i may have fixed it, doesn't make any sense though unless i failed and then messed it up, fixed it, but didn't re-messit lol xd
Sep 10 12:42:39 <zerowalker_w>	okay it seems to work now, the error was i used " instead of ', which i didn't at first, but i later changed the crt/key file and destination
Sep 10 12:42:57 <zerowalker_w>	so i am guessing some of those changes actually fixed it, but cause i used the " all those times it never worked
Sep 10 12:43:15 <zerowalker_w>	using postgres.exe helped identify that the config was bad in this case, thanks!
Sep 10 12:43:28 <zerowalker_w>	gotta go, cya
Sep 10 12:52:39 <Myon>	cheers
Sep 10 13:00:02 <enoq>	hi, how different is postgis to internal spatial stuff?
Sep 10 13:00:08 <enoq>	asking because I saw that JPA has spatial support via postgis
Sep 10 13:00:10 <enoq>	and I'd like to avoid it because I think it will make issues when updating it and running it inside docker
Sep 10 13:00:40 <capitol>	what is "internal" in this context?
Sep 10 13:00:46 <[patrik]>	hi guys. when im meassuring replication lag it seems to be mostly 0 which is perfect, but sometimes i see a small lag (under 100kb) that goes towards zero. How much replication lag is good threshold before saying "this is a problem"?
Sep 10 13:01:37 <enoq>	internal as in: built in Point type and ST_Distance
Sep 10 13:01:47 <enoq>	which is basically all that I need
Sep 10 13:02:28 <Myon>	enoq: the builtin types don't have a ST_Distance function
Sep 10 13:02:55 <Myon>	enoq: there's some very simple implementation in the earthdistance extension, though
Sep 10 13:03:26 <enoq>	point <@> point
Sep 10 13:03:28 <Myon>	[patrik]: we can't tell how much one transaction is worth at your side
Sep 10 13:03:38 <[patrik]>	oh true
Sep 10 13:04:06 <Myon>	enoq: that's straight distance. Germany-New Zealand is about 13000km. Is that ok?
Sep 10 13:04:50 <enoq>	Myon: you mean on a sphere?
Sep 10 13:05:06 <[patrik]>	Myon: i'll just save a table and see where it trends. Some kbs of replication lag is fine. But if it keeps trending upwards instead of towards 0 then that indicates a problem.
Sep 10 13:05:07 <enoq>	as opposed to on earth
Sep 10 13:05:22 <Myon>	enoq: in plain space, no surfaces involved
Sep 10 13:05:34 <enoq>	oh, 2d space?
Sep 10 13:05:46 <Myon>	3d space. right through the middle of the planet
Sep 10 13:05:53 <enoq>	ah, crap
Sep 10 13:06:26 <enoq>	Myon: same issues for cubic stuff?
Sep 10 13:06:35 <Myon>	it'll be ok for small areas
Sep 10 13:06:40 <[patrik]>	Myon: thanks.
Sep 10 13:07:05 <Myon>	(postgis does the same, btw, if you use the "geometry" type, it's all planar coordinates)
Sep 10 13:07:24 <Myon>	only the "geography" type has real 3d distances
Sep 10 13:08:14 <Myon>	[patrik]: there is synchronous replication, but don't use it until you really understand the availability restrictions
Sep 10 13:09:11 <KekSi>	if you're going to read "a shitload" and never want reads to be stale then that's your ticket
Sep 10 13:09:41 <enoq>	Myon: and I suppose the geography type also assumes that the earth is a perfect sphere
Sep 10 13:09:44 <KekSi>	inserts are going to be hella slow with it anyhow
Sep 10 13:09:54 <Myon>	enoq: WGS84
Sep 10 13:10:30 <enoq>	are there performance implications when choosing that type?
Sep 10 13:10:37 <Myon>	it's slower, sure
Sep 10 13:10:45 <enoq>	is there some rough estimate?
Sep 10 13:10:56 <enoq>	order of magnitudes
Sep 10 13:11:07 <Myon>	depends on what kind of queries you are doing
Sep 10 13:11:22 <enoq>	select a table and order by distance from a given point
Sep 10 13:11:36 <enoq>	table has 2000k entries atm
Sep 10 13:11:41 <enoq>	ehm
Sep 10 13:11:44 <enoq>	2000*
Sep 10 13:11:58 <enoq>	we don't expect anything over 10000 really
Sep 10 13:12:00 <Myon>	I have no idea, but in many cases the pure algorithmic cost will be much smaller than the overall query cost (round trip time etc)
Sep 10 13:12:12 <enoq>	ok, good to hear
Sep 10 13:13:10 <enoq>	would you recommend going for postgis then?
Sep 10 13:13:53 <Myon>	avoiding it and inventing new code instead of using whatever your app might already have for postgis sounds like a bad plan
Sep 10 13:14:56 <enoq>	area in question will probably be germany, switzerland and austria (maybe italy)
Sep 10 13:15:43 <Myon>	maybe (flat) UTM coordinates are good enough then
Sep 10 13:16:46 <Berge>	enoq: Test with your own data
Sep 10 13:17:05 <Berge>	We're using the geography type wherever we can, and its performance is adequate for what we do
Sep 10 13:17:48 <enoq>	the meh thing is that JPA spatial only seems to support geometry https://docs.jboss.org/hibernate/orm/5.4/userguide/html_single/Hibernate_User_Guide.html#spatial-configuration-dialect
Sep 10 13:19:12 <Myon>	geometry has much more operators
Sep 10 13:19:20 <Myon>	but you can cast around
Sep 10 13:19:53 <enoq>	would be interesting if using geography on schema level works just fine with the geometry stuff
Sep 10 13:20:14 <Myon>	possibly, but not all functions/operators will work
Sep 10 13:21:31 <enoq>	another possiblity of course would be to order in application code
Sep 10 13:26:03 <enoq>	is there a difference in precision regarding "point <@> point" and ST_Distance for Geometries
Sep 10 13:26:12 <enoq>	since postgis and docker seems like a major pain
Sep 10 13:28:08 <Myon>	using the same coordinate system, it should be the same
Sep 10 13:30:51 <enoq>	thanks, will talk to coworkers
Sep 10 13:31:45 <mazula>	it's the correct way to use enum ? https://gist.github.com/minas-tirith/d552d2b2b43e0e41ffac360f4e2d9726
Sep 10 13:32:26 <ilmari>	mazula: you need to specify both the column name and the type name (which should probably not be the same) in the CREATE TABLE statement
Sep 10 13:33:15 <ilmari>	mazula: also, only use enums if you know that a) you'll never want to remove a value (you can only add and rename) b) you'll never want to attach any metadata to the values
Sep 10 13:33:34 <mazula>	types VARCHAR(255), like that?
Sep 10 13:34:08 <ilmari>	mazula: no the type name is the name you used in CREATE TYPE ... AS
Sep 10 13:34:19 <Myon>	.oO(length limits are for dbase users)
Sep 10 13:36:14 <mazula>	ilmari Can i do that? https://gist.github.com/minas-tirith/d552d2b2b43e0e41ffac360f4e2d9726
Sep 10 13:36:27 <enoq>	is there a reason why postgis is separate btw?
Sep 10 13:36:49 <Myon>	PostgreSQL is extensible, so it doesn't have to be in core
Sep 10 13:37:17 <Myon>	and postgis has different release cycles and a huge stack of geo libs as dependencies
Sep 10 13:37:31 <Myon>	it really wouldn't fit into one tarball
Sep 10 13:37:38 <enoq>	ah so the geo libs are the main issue
Sep 10 13:38:06 <Myon>	plus the people working on it don't have to be database experts (and the PostgreSQL developers don't have to be geo experts)
Sep 10 13:39:01 <enoq>	mysql actually ships geography now
Sep 10 13:39:10 <ilmari>	mazula: no. use the type name you used in the CREATE TYPE statement in the CREATE TABLE statement
Sep 10 13:39:36 <ilmari>	mazula: create type my_enum as enum (...); create table my_tabl (my_column my_type);
Sep 10 13:44:18 <mazula>	ilmari ok like that ? https://gist.github.com/minas-tirith/d552d2b2b43e0e41ffac360f4e2d9726
Sep 10 13:50:50 <ilmari>	mazula: what happened when you tried?
Sep 10 13:51:02 <ilmari>	mazula: that's two type names.
Sep 10 13:51:07 <ilmari>	also, don't use varchar(255), use text
Sep 10 13:52:35 <ysch>	??dont
Sep 10 13:52:36 <pg_docbot>	https://wiki.postgresql.org/wiki/Don%27t_Do_This
Sep 10 13:52:37 <Myon>	"types types_values not null"
Sep 10 13:52:52 <ysch>	mazula: Better read the whole link above.
Sep 10 13:53:40 <mazula>	https://gist.github.com/minas-tirith/d552d2b2b43e0e41ffac360f4e2d9726
Sep 10 13:54:20 <Myon>	did you try that?
Sep 10 13:54:40 <ilmari>	mazula: you have two type names on the `types` column
Sep 10 13:54:41 <mazula>	"types types_values not null" ?
Sep 10 13:55:25 <[patrik]>	Myon: I dont need the synchronus replication. I'm fine with the replication that i have, but as you say its important to understand - and I'm still learning things. Thanks for the info.
Sep 10 13:55:30 <Myon>	mazula: that's what line 6 should look like
Sep 10 13:55:40 <mazula>	yes
Sep 10 13:55:45 <mazula>	it's good?
Sep 10 13:55:51 <Myon>	did you try it?
Sep 10 13:56:50 <mazula>	not yet
Sep 10 22:51:48 *	Disconnected ()
**** ENDING LOGGING AT Tue Sep 10 22:51:48 2019

**** BEGIN LOGGING AT Tue Sep 10 22:52:12 2019

Sep 10 22:52:12 *	Now talking on #postgresql
Sep 10 22:52:12 *	Topic for #postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 10 22:52:12 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Tue Sep 10 01:19:16 2019)
Sep 10 22:52:12 *	Channel #postgresql url: https://www.postgresql.org
Sep 10 22:55:02 <ningu>	this seems to work: https://pastebin.com/sgSujpJt
Sep 10 22:56:00 <davidfetter_work>	cool
Sep 10 22:56:40 <ningu>	davidfetter_work: how would one normally index all the words? I guess it's the same case as indexing all the words in a regular text value?
Sep 10 22:57:24 <davidfetter_work>	yep. I'm pretty rusty on how text search works, but I believe that's the idea.
Sep 10 22:57:38 <ningu>	yeah, I'm rusty too, but I know where to look
Sep 10 23:07:13 <velix>	https://i.imgur.com/RJvc1BX.png
Sep 10 23:16:53 <axsuul>	If I have a compound index XYZ on three columns foo,bar,baz and then I need to query only on bar,baz columns, will it use index XYZ or should I create another compound index on bar,baz?
Sep 10 23:17:36 <xocolatl>	it will likely not be used
Sep 10 23:19:04 <axsuul>	thanks
Sep 10 23:20:18 <xocolatl>	if that's your access pattern, you could just create the index on bar,baz,foo
Sep 10 23:26:02 <axsuul>	xocolatl : ah interesting, so I actual have the following queries:   (1) bar,baz    (2) foo      .. so does that mean i can get away with just bar,baz,foo (unique) and foo   indexes?
Sep 10 23:26:25 <xocolatl>	yes
Sep 10 23:26:44 <axsuul>	nice, thanks!
Sep 10 23:27:18 <xocolatl>	remember, tables don't need to be indexed, queries do.  you have to look at your queries to determine the best indexes
Sep 10 23:29:32 <axsuul>	thanks, will keep that mind, finding it kind of hard to know which queries use what indexes. if I try to use EXPLAIN, it can give a false positive if my dataset isnt large enough =/
Sep 10 23:30:08 <xocolatl>	yeah, you can't optimize a toy dataset
Sep 10 23:30:17 <xocolatl>	I mean, you *can*, but you get toy optimizations
Sep 10 23:38:44 <davidfetter_work>	??mfa
Sep 10 23:38:44 <pg_docbot>	Nothing found
Sep 10 23:38:46 <davidfetter_work>	hrm.
Sep 10 23:38:48 <davidfetter_work>	??2fa
Sep 10 23:38:48 <pg_docbot>	Nothing found
Sep 10 23:39:07 <davidfetter_work>	anybody managed to get an MFA working to auth to pg?
Sep 10 23:39:43 *	mlt- still doesn't know what MFA is
Sep 10 23:40:43 <xocolatl>	multi-factor authentication
Sep 10 23:40:51 <davidfetter_work>	sorry, "multi-factor authentication", which usually means a time-based one-time password combined with some other credential
Sep 10 23:41:14 <xocolatl>	or the museum of fine arts in boston, take your pick
Sep 10 23:41:19 <nbjoerg>	davidfetter_work: any specific reason for that mechanism?
Sep 10 23:41:35 <nbjoerg>	davidfetter_work: e.g. compared to rsa-based auth of a yubikey?
Sep 10 23:41:46 <nbjoerg>	which might not work oob, but seems much easier to integrate
Sep 10 23:41:47 <mlt->	so...like using Google Authenticator app to login to PG?
Sep 10 23:42:08 <nbjoerg>	I think you could do that with pam based though
Sep 10 23:42:34 <xocolatl>	pam can pretty much do everything
Sep 10 23:42:56 <xocolatl>	if you don't run up against authentication_timeout, of course
Sep 10 23:43:18 <nbjoerg>	the primary question for me is what the goal is
Sep 10 23:43:40 <xocolatl>	fun, I'd guess
Sep 10 23:43:44 <xocolatl>	like the twitter fdw
Sep 10 23:43:48 *	davidfetter_work work sat a security company
Sep 10 23:43:52 <davidfetter_work>	works at*
Sep 10 23:44:34 <nbjoerg>	making sure that administrators are using 2FA: I think getting libpq to use ccid is much easier
Sep 10 23:44:43 <nbjoerg>	e.g. pkcs#11
Sep 10 23:45:36 <davidfetter_work>	I use the git FDW each week to write up the newsletter :)
Sep 10 23:46:03 <davidfetter_work>	well, I hacked on it a bit because I haven't yet figured out how to improve performance in a cleaner way
Sep 10 23:47:24 <mlt->	TIL...git fdw
Sep 10 23:48:05 <davidfetter_work>	it's pretty fun
Sep 10 23:48:42 <davidfetter_work>	it's possible that git alone could do what I want, but I like saying things like GROUP BY and agg(... ORDER BY ...)
Sep 10 23:48:43 <nbjoerg>	hm. that would make a certain amount of sense for a use case I have
Sep 10 23:52:40 <davidfetter_work>	I basically filed off some pieces of the git FDW that were slowing me down. I don't care about the diffs between commits for what I'm doing, and it was doing them ssllllllloooooooowwwwwwwwly
Sep 11 00:31:13 <berndj>	xocolatl, do you know if the caveat about constants vs variables as mentioned in https://www.postgresql.org/message-id/20020807143850.GB30003%40gerf.org still applies? (re indexing inet column for use with <<=)
Sep 11 00:32:33 <RhodiumToad>	it only applies partially.
Sep 11 00:32:58 <RhodiumToad>	if the block is a parameter, then it is treated as a constant if the query is getting a custom plan
Sep 11 00:33:34 <berndj>	my concern is that the <<=-using query i'm looking at taming is inside a stored procedure, comparing an inet-valued column to an input to the function
Sep 11 00:33:47 <RhodiumToad>	(the query gets a custom plan unless it's a prepared statement, or statement inside a function, that gets executed more than 4 times and the plan cache comes up with a generic plan that doesn't look more expensive)
Sep 11 00:34:12 <RhodiumToad>	procedure in what language?
Sep 11 00:35:03 <berndj>	almost certainly plpgsql (it looked sql-ish when i was looking at it earlier at the office)
Sep 11 00:35:30 <RhodiumToad>	the difference between language plpgsql and language sql can be critical
Sep 11 00:35:39 <RhodiumToad>	it was a procedure with multiple statements?
Sep 11 00:35:46 <berndj>	yes
Sep 11 00:35:52 <RhodiumToad>	plpgsql then.
Sep 11 00:35:56 <berndj>	hang on a sec i'll get it for sure
Sep 11 00:36:15 <berndj>	yeah, plpgsql
Sep 11 00:36:15 <xocolatl>	sql can have multiple statements...
Sep 11 00:36:28 <RhodiumToad>	that means you are at some risk of getting a generic plan, but if the table is at all large and the ranges selective, it's likely that it'd stick to custom plans
Sep 11 00:36:37 <RhodiumToad>	yes, but usually doesn't
Sep 11 00:37:02 <RhodiumToad>	berndj: what specific evidence do you have for the current form being slow?
Sep 11 00:37:10 <berndj>	200 kilorows in the table containing inet-valued cells
Sep 11 00:38:31 <berndj>	RhodiumToad, a) the procedure takes of the order of a second to figure out what i think should be "fast", b) running the internal query i'm focusing on inside the proc takes of the order of a second to return the couple dozen rows derived from the ~200krow table
Sep 11 00:39:06 <RhodiumToad>	what exactly was the query you ran manually?
Sep 11 00:39:50 <berndj>	any other complications? this bit in focus is inside a CREATE TEMP TABLE ... AS (SELECT DISTINCT ... FROM (SELECT DISTINCT ... <<= ... UNION SELECT ... >>=))
Sep 11 00:40:13 <berndj>	what's the canonical pastebin for this #?
Sep 11 00:40:21 <RhodiumToad>	??paste
Sep 11 00:40:21 <pg_docbot>	https://explain.depesz.com/ :: https://pasteboard.co/
Sep 11 00:40:21 <pg_docbot>	https://www.db-fiddle.com/ :: https://paste.depesz.com/
Sep 11 00:40:21 <pg_docbot>	https://dpaste.de
Sep 11 00:40:44 <RhodiumToad>	dpaste.de best for non-sql, paste.depesz.com has sql formatting, explain.depesz.com is for explains
Sep 11 00:42:41 <berndj>	ok, not such a long query after all. it is: SELECT lai.account FROM iplistip ili JOIN "lnkAccountIPList" lai ON lai.iplist = ili.iplist WHERE inet('104.47.33.58') <<= ili.ip;
Sep 11 00:42:55 <RhodiumToad>	ahh
Sep 11 00:43:06 <RhodiumToad>	so ili.ip is actually a range?
Sep 11 00:43:07 <berndj>	oh, that fixed IP is just some example i grabbed from logs; in the proc it's actually a variable 'addr'
Sep 11 00:43:56 <berndj>	yes, the table is formally ranges, but just eyeballing it the vast majority are /32 ranges
Sep 11 00:44:44 <RhodiumToad>	that doesn't really matter
Sep 11 00:44:53 <berndj>	and the only indexes covering the 'ip' column are "iplistip_pkey" PRIMARY KEY, btree (iplist, ip) and "iplistip_ip_idx" btree (ip) which seems sucky
Sep 11 00:45:06 <RhodiumToad>	the optimization in the message you linked to only applies to column <<= constant, not to constant <<= column
Sep 11 00:45:20 <RhodiumToad>	what pg version are you using?
Sep 11 00:45:46 <RhodiumToad>	recent versions have some spgist support for ip range lookups, otherwise ip4r is what you need
Sep 11 00:45:53 <berndj>	psql reports 9.3.24, with no whine about differing server version
Sep 11 00:46:00 <RhodiumToad>	show server_version;
Sep 11 00:46:14 <berndj>	yeah, it's the same
Sep 11 00:46:27 <RhodiumToad>	ok. pretty sure that's too old for spgist inet support
Sep 11 00:47:05 <RhodiumToad>	ip4r will work though, if you can install it (it's a separate module)
Sep 11 00:47:57 <berndj>	i ran i managed to do CREATE INDEX ON iplistip USING GIST (ip inet_ops); without error on a 9.5.14 box though, but it doesn't have a 100krow iplistip table to make it interesting
Sep 11 00:53:55 <RhodiumToad>	yeah, there's some support for gist but last I looked it was a lot less efficient than ip4r; the spgist support in more recent versions may be better
Sep 11 00:54:28 <RhodiumToad>	9.3 lacks even rudimentary gist support for inet
Sep 11 00:56:17 <agohoth>	Hello
Sep 11 00:56:17 <berndj>	i could just punt until everything gets upgraded including PG to at least 9.5 (which is pending anyway), and keep firefighting until then
Sep 11 00:56:41 <blaenk>	im trying to figure out the syntax for adding a column and setting the default in one go, is this ok: ALTER TABLE  ADD COLUMN x BIGSERIAL DEFAULT nextval('existing_serial'::regclass);
Sep 11 00:56:52 <blaenk>	because the manual doesn't seem to show the DEFAULT clause or I'm misreading it
Sep 11 00:57:02 <agohoth>	I have a developer at work who tuned pg 9.6  and then I used an online config page https://pgtune.leopard.in.ua/#/ and got vastly different results
Sep 11 00:57:08 <RhodiumToad>	do you want the new column to be populated for existing rows?
Sep 11 00:57:09 <agohoth>	mine has smaller values for almsot eveyrhtign
Sep 11 00:57:21 <blaenk>	RhodiumToad: yes preferably, but i could do that separately if necessary
Sep 11 00:57:23 <agohoth>	also she didnt enable any autovac
Sep 11 00:57:28 <blaenk>	for now I'm wondering about going forward for new rows
Sep 11 00:57:43 <blaenk>	(but also if you would be so kind I'm interested in knowing that too!)
Sep 11 00:57:49 <xocolatl>	agohoth: autovacuum is enabled by default
Sep 11 00:57:51 <RhodiumToad>	agohoth: what values for work_mem, effective_cache_size, shared_buffers, maintenance_work_mem?  and how much RAM and how many CPUs do you have?
Sep 11 00:58:27 <RhodiumToad>	blaenk: bigserial implies a newly created sequence. if you want to use an existing sequence, the column type should be bigint
Sep 11 00:58:52 <RhodiumToad>	blaenk: alter table ... add x bigint default nextval('blah');  works
Sep 11 00:59:12 <blaenk>	RhodiumToad: ohh okay, this is for a table partition I'm creating un-attached, the partitioned table has a BIGSERIAL and before I can attach this table I need to have that column too, does that make sense? I'm not sure
Sep 11 00:59:27 <RhodiumToad>	blaenk: if you want to avoid populating existing rows, then you add the column and the default separately.
Sep 11 00:59:46 <blaenk>	so if I make it BIGINT then it won't match the schema for the partitioned table will it? or is BIGSERIAL still just  BIGINT but implies a new sequence
Sep 11 00:59:50 <blaenk>	I vaguely remember reading something like that
Sep 11 00:59:59 <RhodiumToad>	blaenk: bigserial and bigint are the same type
Sep 11 01:00:06 <blaenk>	perfect thank you
Sep 11 01:00:20 <agohoth>	16 g ram aws instance 4 cpu
Sep 11 01:00:21 <RhodiumToad>	blaenk: bigserial is just shorthand for something like  bigint not null default nextval('newly created sequence');
Sep 11 01:00:23 <agohoth>	I think ssd
Sep 11 01:00:36 <agohoth>	I gave that config page 14g ram so 2 for os linux
Sep 11 01:01:03 <blaenk>	RhodiumToad: thank you so much. if a table already had BIGSERIAL, how would I go about like redoing it with the existing serial? do I have to drop the column and re-add it?
Sep 11 01:02:14 <RhodiumToad>	blaenk: no, you'd change the defaults and use alter sequence ... owned by ...
Sep 11 01:02:26 <blaenk>	thank you I will read into that
Sep 11 01:02:32 <RhodiumToad>	agohoth: so what are the values in question?
Sep 11 01:04:14 <agohoth>	https://paste.pics/e2d48cd6e9ab4b1828f929e6f26291ec
Sep 11 01:04:25 <sgt_disco>	If I want to create a unique index where a column is only unique based on equivalent values of another column, how would I go about adding that constraint?
Sep 11 01:05:54 <ash_worksi>	is there a way for me to find out what's causing `RESET ALL` to show up in `pg_running` every so often?
Sep 11 01:05:54 <agohoth>	it seems to optimize for a large effective cache size
Sep 11 01:06:08 <agohoth>	10g or so
Sep 11 01:07:36 <RhodiumToad>	effective_cache_size needs to be large, it doesn't actually allocate anything
Sep 11 01:07:51 <agohoth>	oh autovac is enabled by default ? ok
Sep 11 01:08:07 <agohoth>	now
Sep 11 01:08:33 <RhodiumToad>	agohoth: what are the values from your developer, not from pgtune?
Sep 11 01:08:45 <agohoth>	Damn I closed by work machine a mac
Sep 11 01:08:47 <agohoth>	hold on
Sep 11 01:14:22 <localhorse>	hey
Sep 11 01:15:24 <localhorse>	i have a self-referencing table and i want to `select *, count(children)` (from the same table), how can i do that?
Sep 11 01:15:36 <localhorse>	i tried with a self join but i only get rows that actually have children
Sep 11 01:15:42 <RhodiumToad>	where "children" includes all the descendents?
Sep 11 01:15:43 <ningu>	left join
Sep 11 01:15:51 <RhodiumToad>	or just the immediate level of descent?
Sep 11 01:15:57 <localhorse>	RhodiumToad: only immediate
Sep 11 01:16:03 <RhodiumToad>	left join, then
Sep 11 01:16:07 <localhorse>	i tried that
Sep 11 01:16:36 <RhodiumToad>	select p.id, count(c.id) from mytable p left join mytable c on (c.parent_id=p.id) group by p.id;
Sep 11 01:17:36 <localhorse>	ah, thanks :)
Sep 11 01:19:25 <localhorse>	RhodiumToad: but now it gets tricky: the first occurrence of mytable should be a subselect and the right one is the full table
Sep 11 01:19:44 <RhodiumToad>	why should it be a subselect?
Sep 11 01:19:45 <localhorse>	what's the right syntax to use a subselect there instead?
Sep 11 01:20:08 <localhorse>	RhodiumToad: because i'm filtering the rows first, who i want to return (including their childcount)
Sep 11 01:20:22 <localhorse>	they should be filtered first, not afterwards
Sep 11 01:20:23 <RhodiumToad>	just add a where clause then
Sep 11 01:20:39 <RhodiumToad>	you can't filter by childcount before computing the childcount, obviously
Sep 11 01:20:54 <agohoth>	https://paste.pics/6fa0f7a502c7a8487959a7fed92273eb
Sep 11 01:21:00 <localhorse>	i don't want to filter by childcount tho, but by something else, involving a gist index
Sep 11 01:21:11 <agohoth>	here are the developer configs
Sep 11 01:21:13 <ningu>	localhorse: why can't you put it in the where clause?
Sep 11 01:21:36 <localhorse>	i'll try
Sep 11 01:21:55 <localhorse>	the subselect is a postgis query
Sep 11 01:22:00 <RhodiumToad>	agohoth: I would say that your developer's values are mostly too high
Sep 11 01:22:58 <agohoth>	on an amazon linux x5large 16 g ram 4 cpu  would you dedicate 14g of ram to postgresql? leaving 2 for the linux os?
Sep 11 01:23:10 <RhodiumToad>	agohoth: however, pgtune's suggested work_mem is probably too small. I would have started with 224MB
Sep 11 01:23:22 <agohoth>	I did a presentation about this config and the config page and the pgtunerl.pl  no one was impressed
Sep 11 01:23:35 <blaenk>	RhodiumToad: so I'd ALTER TABLE  ALTER COLUMN  SET DEFAULT next('existing_sequence'), and then ALTER SEQUENCE  OWNED BY  for what? I'm not sure I understand the documentation for this situation
Sep 11 01:23:43 <RhodiumToad>	agohoth: you can't really dedicate RAM to pg. pg wants a lot of the memory available to be available to the OS for disk caching
Sep 11 01:23:47 <agohoth>	This is used in data analytics....I picked mixed type apps in that config page...
Sep 11 01:23:53 <agohoth>	oh
Sep 11 01:24:00 <blaenk>	to remind you, the situation is to change a column with existing rows to use a different sequence, so they should get new numbers rather than the ones it already had
Sep 11 01:24:52 <RhodiumToad>	agohoth: the problem with the 4GB work_mem specified in the developer's configs is that it would take only one moderately complex query to drive the machine into swap
Sep 11 01:25:33 <localhorse>	RhodiumToad, ningu: the subselect starts like this `SELECT p.*, ST_Distance(p.loc, r.loc)::float4 dist FROM points p, (select ST_SetSRID(ST_MakePoint($1, $2), 4326)::geography) as r(loc)` i don't know how to add the `count(c.id)` to this select because it already contains a subselect
Sep 11 01:26:05 <ningu>	is that parent or child?
Sep 11 01:26:13 <localhorse>	i mean i don't know where to add the `join`
Sep 11 01:26:22 <RhodiumToad>	localhorse: you could always do  select ... from (select ...) as p left join ...
Sep 11 01:26:37 <blaenk>	ok I think I would do ALTER SEQUENCE existing_sequence OWNED BY tbl.thecolumn
Sep 11 01:27:28 <agohoth>	hmmm
Sep 11 01:27:49 <agohoth>	she also has some values for variables the pgtune doesn't seem to care about
Sep 11 01:27:59 <RhodiumToad>	pgtune's suggestions are far from ideal
Sep 11 01:28:08 <agohoth>	oh really?  oh wow
Sep 11 01:28:21 <blaenk>	maybe I was confusing in my question, basically I want another table to re-use the same sequence, but rows have already been written with the values from a different sequence, how can I wipe those and force them to take on values of the existing sequence?
Sep 11 01:28:24 <agohoth>	How do I learn the proper configuration?
Sep 11 01:28:28 <blaenk>	should i just drop the column and re-add it
Sep 11 01:28:35 <blaenk>	seems straightforward that way
Sep 11 01:28:43 <RhodiumToad>	1000 is the default for max_files_per_process, fwiw
Sep 11 01:28:58 <peerce>	blaenk; 'values from an existing sequence', is there some relation between the rows in table 1 and table 2 ?
Sep 11 01:29:24 <blaenk>	so it's a partitioned table that has the sequence, and i want child/partitions to use that same sequence, so that they all take on distinct values
Sep 11 01:29:27 <RhodiumToad>	temp_buffers of 8GB is somewhat suspect - if you have more than one connection working with a large temp table that could well push you into swap again
Sep 11 01:29:34 <blaenk>	otherwise if each partition has its own sequence then i'll probably have duplicate values
Sep 11 01:29:57 <RhodiumToad>	however, if you know that only one connection at a time, at most, is making heavy use of temp tables then it might be justified
Sep 11 01:30:04 <peerce>	if you created the partitions properly, they should all be using the same seuqnece from the beginning
Sep 11 01:30:08 <peerce>	before you populated them
Sep 11 01:30:53 <RhodiumToad>	agohoth: the most important settings are effective_cache_size (where the actual value is quite non-critical, just make it something like 75% of total RAM),
Sep 11 01:31:07 <RhodiumToad>	shared_buffers, work_mem, and maintenance_work_mem
Sep 11 01:31:16 <blaenk>	so what I'm doing is for a partition I create it separately to match the schema and then I ATTACH it, but I was using BIGSERIAL for this column which was creating its own sequence, what should I have used instead?
Sep 11 01:31:40 <RhodiumToad>	for shared_buffers, opinions vary quite widely on the best value
Sep 11 01:32:01 <peerce>	blaenk; https://www.postgresql.org/docs/current/ddl-partitioning.html#DDL-PARTITIONING-DECLARATIVE
Sep 11 01:32:12 <RhodiumToad>	for work_mem and maintenance_work_mem, start with something like  work_mem=RAM/(16*cpus) and maintenance_work_mem=RAM/12
Sep 11 01:32:26 <localhorse>	RhodiumToad: ningu: i get invalid reference to FROM-clause entry for table "p"
Sep 11 01:32:42 <RhodiumToad>	localhorse: for what query exactly
Sep 11 01:32:43 <agohoth>	hmm
Sep 11 01:32:58 <blaenk>	peerce: so I'll omit the sequence column and it'll automatically be added when I ATTACH?
Sep 11 01:33:04 <peerce>	blaenk; when you create the partitions via CREATE TABLE parent_xxx PARTIION OF parent
Sep 11 01:33:12 <peerce>	it should inherit the same sequence.
Sep 11 01:33:26 <RhodiumToad>	agohoth: for max_wal_size, a lot depends on the write load on the db and your intended recovery time after a crash - 2GB is not unreasonable
Sep 11 01:34:02 <blaenk>	peerce: the thing is I'm working with a table that was imported by spark, and then just transforming that. I guess I can create a new table and copying into it but it just seemed unnecessary
Sep 11 01:34:10 <peerce>	whats 'spark' ?
Sep 11 01:34:12 <RhodiumToad>	agohoth: small random_page_cost (i.e. slightly more than 1) is reasonable for SSD storage, increasing cpu_tuple_cost to 0.1 or so may also be worthwhile
Sep 11 01:34:36 <localhorse>	RhodiumToad: this query https://www.irccloud.com/pastebin/xq7al4UG/
Sep 11 01:35:00 <RhodiumToad>	agohoth: large effective_io_concurrency can help reduce latency on cloud-type systems when you do a lot of bitmap indexscans
Sep 11 01:35:10 <blaenk>	peerce: basically it got imported by a different process, the table already exists in the db by the point at which I'm trying to make it match the partitioned table schema so that I can attach it
Sep 11 01:35:18 <blaenk>	http://spark.apache.org/
Sep 11 01:35:39 <RhodiumToad>	localhorse: you didn't put all of the subquery inside where I told you to put it
Sep 11 01:35:46 <blaenk>	I was kindly told how to make the table use the existing sequence, that's fine going forward
Sep 11 01:35:50 <peerce>	if you're using manual partitioning, the child tables should have been created with INHERITS
Sep 11 01:36:07 <blaenk>	manual partitioning? I'm using declarative partitioning
Sep 11 01:36:07 <RhodiumToad>	localhorse: select p.id, count(c.id) from (/* put your entire original query here */) p left join ...
Sep 11 01:36:23 <peerce>	then when you created it, specifying PARTITION OF ...   should have taken care of this.
Sep 11 01:36:26 <blaenk>	and again I can't have created the tables, it just already exist out of my control, unless I create a new table and copy everything
Sep 11 01:36:26 <localhorse>	ah
Sep 11 01:36:29 <agohoth>	work meme 219M
Sep 11 01:36:33 <agohoth>	hmm
Sep 11 01:36:44 <blaenk>	the problem isn't how to solve it going forward, RhodiumToad helped me with that
Sep 11 01:36:47 <agohoth>	tuner says only 9 wow big disparity
Sep 11 01:37:04 <agohoth>	9.6 postgresql by the way
Sep 11 01:37:07 <blaenk>	what I'm wondering is in general, how to 'redo' rows that have already used one sequence, to now use another. im thinking i should just drop the column and recreate it
Sep 11 01:37:20 <peerce>	that won't update all the rows
Sep 11 01:37:35 <agohoth>	want to see the pgtune for data ware house scenario?
Sep 11 01:37:37 <blaenk>	oh really? if the column literally no longer exists?
Sep 11 01:37:40 <peerce>	and if you do force a update of all the rows that will double the size of the table via churn
Sep 11 01:37:47 <agohoth>	is 14 of 16 g ram ona aws vm ok for dedicated pg box linux?
Sep 11 01:37:57 <peerce>	since 'update' is implemented as insert,delete
Sep 11 01:38:12 <blaenk>	what 'update' are you referring to, UPDATE?
Sep 11 01:38:25 <peerce>	anything that modifies a value in an existing row, but yes, UPDATE
Sep 11 01:38:40 <blaenk>	I'm saying I'll ALTER TABLE  DROP COLUMN , and then I'll ADD COLUMN again but with the new DEFAULT sequence
Sep 11 01:38:40 <localhorse>	RhodiumToad: the original query excluding ORDER BY and LIMIT, right?
Sep 11 01:38:43 <peerce>	UPDATE sometable SET id=nextval('sequencename');
Sep 11 01:38:51 <peerce>	that will do the same thing.
Sep 11 01:38:51 <agohoth>	it chaged the max wal to 8g in data warehouse config vs the mixed app config
Sep 11 01:38:53 <localhorse>	those go at the end of the whole query
Sep 11 01:38:55 <blaenk>	ah
Sep 11 01:39:15 <blaenk>	but it doesn't seem like I have an alternative, I'm in this situation now
Sep 11 01:39:19 <RhodiumToad>	localhorse: right
Sep 11 01:39:20 <peerce>	the UPDATE is actually a little more efficient, as deleted columns are never actually removed, they just get ignored in future transactions
Sep 11 01:39:45 <peerce>	and the ADD COLUMN will always add the new column to the end of the tuples
Sep 11 01:39:53 <RhodiumToad>	agohoth: the max wal size really only affects how frequent checkpoints are when you're writing large volumes of data
Sep 11 01:40:14 <RhodiumToad>	agohoth: for things like bulk loading you want checkpoints to be reasonably infrequent
Sep 11 01:40:31 <blaenk>	peerce: so given that I'm _already_ in this situation, what is the least bad approach?
Sep 11 01:40:55 <blaenk>	I need to migrate tables to use a different sequence and redo their sequence values
Sep 11 01:41:10 <RhodiumToad>	agohoth: for mixed or OLTP workloads you usually want them a bit more frequent to avoid long recovery times
Sep 11 01:41:51 <peerce>	alter table alter column, change the default value to the new sequence, then UPDATE table SET column=nextval('seqname');
Sep 11 01:41:52 <agohoth>	hmmmm
Sep 11 01:42:06 <blaenk>	thanks peerce
Sep 11 01:42:07 <peerce>	so nothing else in your database even references these sequences ?
Sep 11 01:42:11 <agohoth>	Where did you learn howto tune?  the wiki? or testing? or?
Sep 11 01:42:12 <peerce>	this data doesn't have a natural key ?
Sep 11 01:42:23 <blaenk>	these sequences aren't keys
Sep 11 01:42:28 <peerce>	huh?
Sep 11 01:42:32 <blaenk>	what I said?
Sep 11 01:42:37 <blaenk>	there are natural keys
Sep 11 01:42:41 <RhodiumToad>	experience and knowing how the code works (also helping people here)
Sep 11 01:42:43 <peerce>	normal use of sequence is to be a primary key
Sep 11 01:42:49 <blaenk>	then this is not a normal use
Sep 11 01:42:50 <peerce>	a surrogate key
Sep 11 01:43:02 <peerce>	then whats the point of even having a random sequence unrelated to the data ?
Sep 11 01:43:06 <blaenk>	it's for spark to read 'partitions' of the db
Sep 11 01:43:13 <blaenk>	ranges of rows
Sep 11 01:43:13 <localhorse>	RhodiumToad: now i get: error: column "p2.loc" must appear in the GROUP BY clause or be used in an aggregate function https://www.irccloud.com/pastebin/Be57P02T/
Sep 11 01:43:19 <blaenk>	in parallel
Sep 11 01:44:03 <peerce>	so you're partitioning by this sequence yet its not the PK nor is it even assigned as the partitioning value yet ?   huh.
Sep 11 01:44:07 <RhodiumToad>	localhorse: you can't select p2.* like that
Sep 11 01:44:16 <localhorse>	why not?
Sep 11 01:44:33 <localhorse>	i also want to select `dist` in the overall query btw
Sep 11 01:44:44 <agohoth>	I wonder about the number of connections too....
Sep 11 01:45:13 <localhorse>	RhodiumToad: do i need to spell out all columns verbatim?
Sep 11 01:45:17 <RhodiumToad>	localhorse: just group by p2.id, p2.dist  then (and select only those two plus the count)
Sep 11 01:45:41 <agohoth>	I notice at the analytics group I work in now, they use a huge mishmash fo differnt java and dataabse tools
Sep 11 01:45:44 <localhorse>	but i want all columns of p, dist and child count
Sep 11 01:46:01 <peerce>	localhorse; but aren't there more than one column with the same id and dist ?
Sep 11 01:46:04 <agohoth>	Are there shops that do data analytics with only postgresql?
Sep 11 01:46:08 <peerce>	er, more than one row, I mean...
Sep 11 01:46:19 <localhorse>	peerce: hence the group by p2.id
Sep 11 01:46:51 <agohoth>	as a former linux admin I hate to see data in so many different places
Sep 11 01:46:53 <RhodiumToad>	localhorse: is points.id the pkey of points?
Sep 11 01:46:55 <localhorse>	so i need to group by <all columns of p2> ?
Sep 11 01:46:58 <localhorse>	yes
Sep 11 01:47:01 <agohoth>	I always suspect the devs are hiding bad performacne
Sep 11 01:47:20 <localhorse>	can i do it with `distinct`?
Sep 11 01:47:34 <RhodiumToad>	localhorse: no
Sep 11 01:47:46 <RhodiumToad>	localhorse: it might be better to rearrange the query like this:
Sep 11 01:48:43 <RhodiumToad>	select p2.*, c.count from (...) p2 left join (select q.parent_id, count(*) from points q group by q.parent_id) c on (p2.id=c.parent_id);
Sep 11 01:48:57 <RhodiumToad>	note no group by at the outer level, though you can add ORDER BY
Sep 11 01:49:18 <RhodiumToad>	er, use nullif(c.count,0) rather than just c.count there
Sep 11 01:49:56 <localhorse>	RhodiumToad: but then it's potentially fetching a huge number of rows for the inner query, no?
Sep 11 01:50:05 <localhorse>	it's joining the whole table with itself
Sep 11 01:50:11 <localhorse>	not just my subselect
Sep 11 01:52:35 <theseb>	Why does this given an error?... with foo (a, b, c) as (select 4, 5, 6)
Sep 11 01:52:35 <theseb>	     select * from foo;
Sep 11 01:53:05 <theseb>	i'm learning CTEs
Sep 11 01:54:11 <RhodiumToad>	what error?
Sep 11 01:54:48 <RhodiumToad>	localhorse: if you find it slower that way, then try this instead:
Sep 11 01:55:07 <theseb>	RhodiumToad: it says where the "a" is....it wasn't expeting that there
Sep 11 01:55:20 <RhodiumToad>	localhorse:  select p2.*, c.count from (...) p2 left join lateral (select count(*) from points q where q.parent_id=p2.id) c on true;
Sep 11 01:55:28 <localhorse>	RhodiumToad: i'm getting: Unexpected null for non-null column https://www.irccloud.com/pastebin/dZdNovsK/
Sep 11 01:55:31 <Xgc>	theseb: Make sure you're using a database which support this.  You've been asking general questions in every DB channel and #sql.
Sep 11 01:56:05 <RhodiumToad>	localhorse: getting that from where?
Sep 11 01:56:10 <ningu>	theseb: with foo as select (4, 5, 6)
Sep 11 01:56:13 <localhorse>	from this query
Sep 11 01:56:44 <RhodiumToad>	theseb: what's the exact error message
Sep 11 01:56:56 <RhodiumToad>	localhorse: from what piece of software
Sep 11 01:57:12 <localhorse>	postgres
Sep 11 01:57:45 <theseb>	RhodiumToad: fixed thanks to ningu!.. with foo as select (4, 5, 6)
Sep 11 01:57:46 <theseb>	works
Sep 11 01:57:50 <localhorse>	or diesel rather
Sep 11 01:58:04 <localhorse>	the rust lib to talk to postgres
Sep 11 01:58:19 <RhodiumToad>	there's no such error in the postgresql source code
Sep 11 01:58:44 <RhodiumToad>	with foo(a,b,c) as (select 4,5,6) select * from foo;   is valid syntax
Sep 11 01:59:01 <Xgc>	theseb: The SQL you just showed is not valid.
Sep 11 01:59:01 <ningu>	theseb: what database are you using?
Sep 11 01:59:19 <RhodiumToad>	particularly useful in the form  with foo(a,b,c) as (values (1),(2),(3)) select * from foo;
Sep 11 01:59:24 <localhorse>	RhodiumToad: then it's from diesel. it tries to deserialize the result of the queries into strongly typed structs
Sep 11 01:59:34 <theseb>	ningu: Spark SQL
Sep 11 01:59:37 <localhorse>	and it got a null for a non-null column
Sep 11 01:59:41 <localhorse>	from this query
Sep 11 01:59:52 <theseb>	Xgc: really?
Sep 11 01:59:53 <theseb>	hmm
Sep 11 02:00:15 <ningu>	theseb: well you can't have "with ..." without select after it
Sep 11 02:00:16 <RhodiumToad>	localhorse: coalesce(c.count, 0)  <-- should have been that, not nullif
Sep 11 02:00:21 <Xgc>	theseb: Yes, not in any database I know and not valid standard SQL either.
Sep 11 02:00:25 <localhorse>	ah!
Sep 11 02:00:32 <RhodiumToad>	localhorse: but if it's assuming that a nullif() result is not null, then it's broken
Sep 11 02:00:57 <localhorse>	no, it's not assuming. i told it that the child_count is a non-nullable type
Sep 11 02:00:59 <Xgc>	theseb: You must be using some odd database if that was the exact SQL you executed with no errors.
Sep 11 02:01:10 <RhodiumToad>	Xgc: maybe you missed the second part of their original query?
Sep 11 02:01:19 <theseb>	RhodiumToad: here is the command and error you wanted... https://pastebin.com/mu1MCAxb
Sep 11 02:01:58 <RhodiumToad>	theseb: well that's clearly not postgres so it's not our problem, is it?
Sep 11 02:02:25 <RhodiumToad>	theseb: I cut+paste your query into postgres and it runs fine:
Sep 11 02:02:45 <RhodiumToad>	https://dpaste.de/mZoe
Sep 11 02:02:56 <Xgc>	Right. He using something strange.
Sep 11 02:03:09 <Xgc>	RhodiumToad's SQL is obviously fine.
Sep 11 02:04:08 <localhorse>	RhodiumToad: thanks, with coalesce, it works :)
Sep 11 02:04:26 <localhorse>	RhodiumToad: but am i right that it's slow because it joins the whole table with itself before filtering it down to my subset?
Sep 11 02:04:45 <RhodiumToad>	localhorse: the lateral one or the non-lateral one?
Sep 11 02:04:55 <localhorse>	the non-lateral one
Sep 11 02:05:12 <RhodiumToad>	localhorse: whether it's actually slow will depend a lot on your data sizes, for which explain analyze is your friend
Sep 11 02:05:16 <theseb>	RhodiumToad, Xgc: wait...SQL is supposed to be standard and the same everywhere innit?
Sep 11 02:05:22 <ningu>	theseb: no
Sep 11 02:05:28 <RhodiumToad>	theseb: HAHAHAHAHA
Sep 11 02:05:36 <ningu>	no two implementations are identical
Sep 11 02:05:57 *	Xgc smiles
Sep 11 02:06:03 <theseb>	good lord it was made in the 1970s...they haven't got their act together? didn't ANSI standard fix all this?
Sep 11 02:06:06 <RhodiumToad>	theseb: there is a standard, but (a) something like 80% of the standard is "optional features", and (b) virtually nobody implements even the standard part the same way
Sep 11 02:06:40 <RhodiumToad>	theseb: in particular WITH is one of those optional features
Sep 11 02:07:19 <theseb>	RhodiumToad: wait...with other langs like C and Python etc......you need to follow very strict guidelines to be called a compiler for lang $X
Sep 11 02:07:31 <theseb>	i want to complain to the management
Sep 11 02:07:57 <ningu>	the history and ecosystem of databases is kind of different
Sep 11 02:08:11 <ningu>	there are a small number of implementations, each with a long history and large number of users
Sep 11 02:08:19 <RhodiumToad>	https://wiki.postgresql.org/wiki/PostgreSQL_vs_SQL_Standard  <-- there's an incomplete list of the ways that postgresql explicitly violates even the parts of the spec that it tries to implement in the standard way
Sep 11 02:08:32 <Xgc>	theseb: C is another with implementations that vary widely, and almost none support the entire standard, usually not for at least 10 or 20 years after the standard is released.
Sep 11 02:08:33 <RhodiumToad>	??comparison
Sep 11 02:08:34 <pg_docbot>	http://troels.arvin.dk/db/rdbms/ :: http://en.wikipedia.org/wiki/Comparison_of_SQL_database_management_systems
Sep 11 02:08:34 <pg_docbot>	http://www.wikivs.com/wiki/MySQL_vs_PostgreSQL :: http://www.intitec.com/varios/FabalabsResearchPaper-OSDBMS-Eval.pdf
Sep 11 02:08:34 <pg_docbot>	https://www.postgresql.org/docs/current/static/functions-comparisons.html#ROW-WISE-COMPARISON :: https://www.postgresql.org/docs/current/static/functions-comparison.html
Sep 11 02:08:34 <pg_docbot>	http://www.sql-workbench.eu/dbms_comparison.html
Sep 11 02:09:19 <theseb>	ok
Sep 11 02:09:25 <theseb>	yikes
Sep 11 02:09:27 <localhorse>	RhodiumToad: but the lateral one should be faster?
Sep 11 02:09:32 <localhorse>	in theory?
Sep 11 02:09:54 <RhodiumToad>	localhorse: assuming you have reasonable indexes, yes
Sep 11 02:10:36 <localhorse>	i have these two indexes: create index on points (updated_at, id); create index on points using gist (loc);
Sep 11 02:10:43 <localhorse>	are they reasonable enough?
Sep 11 02:10:51 <localhorse>	or how to improve them for this query specifically?
Sep 11 02:10:56 <localhorse>	for the lateral one
Sep 11 02:11:59 <RhodiumToad>	you'll need an index on points(parent_id)
Sep 11 02:12:22 <localhorse>	a separate one, or added to the first?
Sep 11 02:12:30 <RhodiumToad>	separate
Sep 11 02:12:45 <localhorse>	ok
Sep 11 02:13:36 <agohoth>	ll
Sep 11 02:14:09 <ningu>	theseb: databases have to perform in very specific ways, often to high performance requirements, and different implementations have added extensions of various sorts over the years
Sep 11 02:14:31 <ningu>	but basically, choosing your database is an important choice
Sep 11 02:20:36 <theseb>	ningu: ok...thanks
Sep 11 02:24:26 <localhorse>	RhodiumToad: so you said `select p2.*, c.count from (...) p2 left join lateral (select count(*) from points q where q.parent_id=p2.id) c on true`, why `on true`?
Sep 11 02:24:52 <ningu>	localhorse: all joins need an "on" condition specified
Sep 11 02:25:04 <ningu>	even if effectively there is nothing to put there
Sep 11 02:25:13 <localhorse>	then why are we even joining?
Sep 11 02:25:39 <ningu>	lateral joins work differently from other joins (and are also different syntactically)
Sep 11 02:25:45 <localhorse>	ok
Sep 11 02:26:11 <ningu>	in this case, the count needs to be in the subquery and needs to be done once per joined row
Sep 11 02:27:09 <ningu>	I don't think the count would come out right without lateral... you can try it
Sep 11 02:27:20 <localhorse>	RhodiumToad: but is this really faster than the non-lateral version then?
Sep 11 03:26:34 <jhammerman>	Hi Postgres IRC, does anyone have experience with HikariCP rapid recovery in a HA failover scenario? We have built both HA-AP and HA-CP architectures, and Im am trying to find specifc information regarding in-flight transactions (so that I can help our users safely retry the correct class of errors, with backoff, etc.)
Sep 11 03:28:59 <peerce>	never even heard of hikari, but in general, if there's a failover, any pending transactions that ahven't been committetd have to be restarted from the beginning.
Sep 11 03:32:47 <dive-o>	jhammerman: all I know about HikariCP as far as that goes is that it really, really doesn't like it when the database goes away.
Sep 11 03:33:04 <dive-o>	I'm no java guy though so I can't really give any useful details other than that though
Sep 11 03:34:26 <jhammerman>	OK, thanks yall
Sep 11 03:42:50 <Slade>	i wonder if there is still much of an advantage of using int/enums instead of just storing the string  (gender  1/Male 2/Female etc)  (1/ok 2/canceled 3/noaccess)
Sep 11 03:54:57 <peerce>	well, just storing the strings won't ensure you don't have random values there
Sep 11 03:55:14 <peerce>	and string comparisions are much more work than integer comparisions
Sep 11 03:55:29 <peerce>	plus the tuples are larger, as are the indicies
Sep 11 03:55:42 <peerce>	when you multiply thtat by millions of rows it becomes quite significant
Sep 11 11:12:47 *	Disconnected ()
**** ENDING LOGGING AT Wed Sep 11 11:12:47 2019

**** BEGIN LOGGING AT Wed Sep 11 11:13:12 2019

Sep 11 11:13:12 *	Now talking on #postgresql
Sep 11 11:13:12 *	Topic for #postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 11 11:13:12 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Tue Sep 10 01:19:16 2019)
Sep 11 11:13:12 *	Channel #postgresql url: https://www.postgresql.org
Sep 11 11:33:52 <adsf>	can windowing functions show you % over or % under the average?
Sep 11 11:35:06 <Myon>	you can divide the result by something
Sep 11 11:35:18 <adsf>	good shout
Sep 11 11:35:20 <adsf>	thnx
Sep 11 11:46:54 <Berge>	enoq: It doesn't. Generally, use the postgres docs, they're very good and accurate. Most things on the Internet aren't.
Sep 11 11:47:07 <enoq>	Berge I see, thank you
Sep 11 11:47:14 <enoq>	I had trouble finding it in the postgres docs
Sep 11 11:57:32 <Moonsilence>	Hi! I am looking for a boolean expression to check wether a text value can be cast to type UUID. I need something like 'mytextval'::text -> false (no valid uuid) and '50dc6ce5-76db-7be8-59ba-b25553b8631e'::text -> true (valid uuid)
Sep 11 11:57:56 <Myon>	~ 'regexp'
Sep 11 11:58:20 <Moonsilence>	is there a regexp to match uuid v4 format?
Sep 11 11:59:06 <Moonsilence>	ah yes, it's ugly but i found one ;-) thanks Myon
Sep 11 12:00:21 <Myon>	alternatively, write a plpgsql function that tries the cast and catches the exception
Sep 11 12:24:40 <snatcher>	\copy (select cn from tn where cnn=v) to '/tmp/fn'; what's the way to copy with evaluated escapes (newlines etc) in this case?
Sep 11 12:26:26 <ilmari>	snatcher: what do you mean "evaluated escapes"? you can use CSV format, which quotes values containing newlines (and commas)
Sep 11 12:28:21 <snatcher>	ilmari: output file contains  \\u0\n etc instead unicode symbols or newlines for example
Sep 11 12:28:42 <snatcher>	i mean "\n" as  string
Sep 11 12:29:22 <Myon>	there's also COPY BINARY
Sep 11 12:29:50 <Myon>	or maybe you could just \o /tmp/fn, and run a plain SELECT
Sep 11 12:29:55 <Myon>	(\a \t)
Sep 11 13:15:56 <snatcher>	still contains PGCOPY header
Sep 11 13:17:36 <snatcher>	maybe i misunderstand something, what's the correct way to write string "as is" to file?
Sep 11 13:18:07 <colo-work>	snatcher, using what? a UNIX shell?     printf 'as is' > file
Sep 11 13:18:17 <colo-work>	(that does NOT include a trailing newline)
Sep 11 13:18:27 <snatcher>	colo-work: using pgsql
Sep 11 13:19:10 <colo-work>	ah. what's your actual objective?
Sep 11 13:19:50 <snatcher>	colo-work: parse that string on other host as file
Sep 11 13:20:26 <snatcher>	\copy (select cn from tn where cnn=v) to '/tmp/fn' binary; includes PGCOPY header
Sep 11 13:22:25 <snatcher>	>Binary COPY OUT files are only intended for consumption by COPY IN commands. There is no way to prevent Postgres from writing the file/row/field headers.
Sep 11 13:22:41 <snatcher>	so there is no way
Sep 11 13:39:50 <doev>	I have child tables, that inherits from a master table. In the master table, I need a field "label", that depends on the child table. What is best practise to do that? If I use a view, the code will be complex. what else is possible?
Sep 11 13:43:12 <doev>	If there is no better solution, I will use a label-field in the master table, and control it by the insert/update trigger.
Sep 11 13:49:25 <moldy>	doev: what does "depends on the child table" mean?
Sep 11 13:49:41 <moldy>	doev: and why does the field not live on the child table?
Sep 11 13:51:53 <nickb>	plpgsql function definitions are tied to the snapshot for volatile functions in read committed mode, right?
Sep 11 13:52:05 <doev>	moldy, i.e. the label should be Child1.a, Child2.name or Child3.whatever ....
Sep 11 13:53:01 <moldy>	doev: hmm, i don't see why the label field is not on the child tables then
Sep 11 13:53:37 <doev>	moldy, cause I want to query the master table.
Sep 11 13:59:20 <[patrik]>	doev: how many child tables do you have? is it a fixed number?
Sep 11 14:00:32 <[patrik]>	doev: im thinking you can slap the label field in the child tables and create a view and query that when you need "label" rather than query your master table. So create a view with all the label fields joined together from the child tables.
Sep 11 14:02:07 <Bish>	if i want to order a dataset by data i only have in memory how would i go with it?
Sep 11 14:02:15 <Bish>	put it into an array, and check the index of the thingie?
Sep 11 14:02:32 <Bish>	say i have [1,2,3] in my dateset and the order i have is [2,3,1]
Sep 11 14:05:30 <[patrik]>	bish maybe you can select unnest(your_array) order by 1;
Sep 11 14:05:54 <[patrik]>	and then put it back into an array data type again.
Sep 11 14:06:09 <Bish>	nono, maybe i expressed myself to stupid
Sep 11 14:06:26 <Bish>	i want to order an existing table, by an order i have in ram
Sep 11 14:06:46 <Bish>	say i have "user" [1,2,4,6,9] and the order i have in memory is [4,6,9]
Sep 11 14:06:56 <Bish>	then i want order "user" by their index of that memory array
Sep 11 14:07:05 <Bish>	if that index cannot be found then place it at the end
Sep 11 14:07:50 <jtech>	Hi Had executed the command "DROP ROLE IF EXISTS my_user;" but the below message shows up:ERROR: role "my_user" cannot be dropped because some objects depend on it DETAIL: owner of default privileges on new relations belonging to role my_user in schema public SQL state: 2BP01
Sep 11 14:08:02 <jtech>	Then I have executed the below command:REASSIGN OWNED BY my_user TO postgres;
Sep 11 14:08:15 <jtech>	So, the below commands it works but I am concerned about if there is any chance to drop any object that wasn't reassigned to postgres user before. Can it happen or I am getting crazy here?DROP OWNED BY my_user;DROP ROLE IF EXISTS my_user;
Sep 11 14:14:46 <ilmari>	jtech: see the second-last note in https://www.postgresql.org/docs/current/sql-reassign-owned.html
Sep 11 14:18:49 <jtech>	ilmari the issue is cannot be dropped using only REASSIGN OWNED, it is allowed to drop only if applied DROP OWNED BY - third-last note
Sep 11 14:19:33 <ilmari>	The REASSIGN OWNED command does not affect any privileges granted to the old_roles on objects that are not owned by them. Likewise, it does not affect default privileges created with ALTER DEFAULT PRIVILEGES. Use DROP OWNED to revoke such privileges.
Sep 11 14:20:10 <ilmari>	so yes, you ned to do REASSIGN OWNED, then DROP OWNED (in all datbases the user owns objects or has privileges) before dropping the role
Sep 11 14:22:19 <jtech>	oh ok.. so, is there any change to miss an object for the new reassigned role after executed DROP OWNED BY previous role?
Sep 11 14:23:28 <jtech>	because I just want drop the roles not any objects that should be reassigned to a new role at his point
Sep 11 14:24:33 <ariejan>	Hi. I have a simple table that periodically receives a reported status. Basically it's a list of what the status was a different points in time. I want to write a query to find when the last status change happened. Here's a small example: https://gist.github.com/ariejan/70e7a5ef426ad5844ff6da3d4ae39f94
Sep 11 14:25:30 <ilmari>	"reassign owned" will reassign ownership that can be reassigned. the subsequent drop owned will drop things that are associated with the role but aren't really ownership per se (granted and default privileges)
Sep 11 14:25:44 <doev>	[patrik] the number of child tables is not fixed.
Sep 11 14:25:57 <jtech>	ilmari just read again your comment and realized that "DROP OWNED" revoke privileges and don't drop objects. thanks
Sep 11 14:27:02 <jtech>	now I am confused haha
Sep 11 14:28:01 <[patrik]>	doev: that sucks more than a little :)
Sep 11 14:29:03 <doev>	well, I think there is no perfect solution. I think a label filed in master, managed by insert/update triggers in the child table, can be what I need.
Sep 11 14:29:17 <ilmari>	jtech: if someone creates an object owned by the role between REASSIGN OWNED and DROP OWNED, that will get dropped
Sep 11 14:30:18 <doev>	Can I forbid direct insert/delete in a master table? But not with usage of privileges.
Sep 11 14:30:38 <durango>	Im currently experiencing this issue: database is not accepting commands to avoid wraparound data loss in database <-- is there anyway to resolve this WITHOUT restarting the server and halting the entire database?
Sep 11 14:31:23 <[patrik]>	bish: that doesnt make sense to me. the table (i think a db table) is not in any guaranteed order, and you build some wanted sort-order in memory.
Sep 11 14:31:49 <Bish>	with "in memory" i mean the memory of the client
Sep 11 14:33:04 <[patrik]>	ok so the client software builds a list of "users" and their wanted sort order, and then what is supposed to happen?
Sep 11 14:33:28 <jtech>	ilmari um.. so if I run again REASSIGN OWNED and DROP OWNED the recent objects should not be dropped as they were just reassigned, is that correct?
Sep 11 14:33:29 <[patrik]>	the database is going to reorder stuff based on this client wishlist of order?
Sep 11 14:33:57 <Bish>	yis!
Sep 11 14:34:12 <Bish>	so i want something like
Sep 11 14:38:55 <[patrik]>	ok i would do this probably with a temporary table or CTE of some sort. Make a plpgsql function to take that array of "users" (user_ids).
Sep 11 14:39:53 <[patrik]>	so we have 5 users in the array (5,10,15,20,25) and we make the index in the array the sort order, so if you want different sort order, you have the client reorganize the array-order.
Sep 11 14:41:20 <[patrik]>	so we say the index of the array is the sort order. Create a temporary table and select user_id from users where user_id in (.....); and insert that into the temproary table along with the array-index of each id.
Sep 11 14:42:00 <[patrik]>	so you will have a table like this temp_table(user_id,sort_order) and have the function populate it based on the client wishlist.
Sep 11 14:42:20 <lluad>	... order by foo <> 5, foo <> 10, foo <> 15, foo <> 20, foo <> 25
Sep 11 14:42:48 <[patrik]>	so it would be 5,1 10,2 15,3 20,4 and 25,5 in the example. and then return it as select * from temp_table order by sort_order.
Sep 11 14:43:23 <Myon>	order by foo not in (5, 10, 15, 20, 25), foo
Sep 11 14:44:12 <lluad>	That's not the same thing (and won't work, I don't think).
Sep 11 14:45:32 <lluad>	(Though I'm vague on quite what not in () will turn in to when you pry the cover off)
Sep 11 14:49:22 <RhodiumToad>	[patrik]: no point using a temp table
Sep 11 14:49:28 <Bish>	that's what i thought
Sep 11 14:49:55 <RhodiumToad>	select * from unnest(array[5,10,15,20]) with ordinality as a(id,ord) join ... order by a.ord;
Sep 11 14:49:59 <[patrik]>	RhodiumToad: fair enough, there is always a more efficient way to do it in postgres :)
Sep 11 14:50:37 <RhodiumToad>	pg will even avoid the sort when sorting by ordinality if the rest of the plan doesn't mess up the sort order
Sep 11 14:51:28 <Bish>	RhodiumToad: but i want order an existing table according to that array i give it in
Sep 11 14:52:36 <RhodiumToad>	yes, that's what the query does
Sep 11 14:53:21 <Bish>	order by a.id you mean?
Sep 11 14:53:39 <Bish>	after the join
Sep 11 14:53:45 <Bish>	im confused
Sep 11 14:54:28 <Bish>	i was expecting something like order by index of user_id in {1,2,3,4,5}::[Int]
Sep 11 14:54:41 <Bish>	integer[] rather i guess
Sep 11 14:55:25 <RhodiumToad>	that's generally less efficient
Sep 11 14:55:33 <Bish>	 guess so
Sep 11 14:55:58 <RhodiumToad>	select * from unnest(array[5,10,15,20]) with ordinality as a(id,ord) join users u on u.user_id=a.id  order by a.ord;
Sep 11 14:56:41 <ariejan>	Is there a way to group sequential values, but across the entire table? E.g. a, a, b, b, a would result in a, b, a (not a, b)?
Sep 11 14:57:26 <RhodiumToad>	you can do stuff with lag() to detect consecutive values according to some ordering of the rows
Sep 11 15:03:49 <aborigen1020>	d
Sep 11 15:05:30 <aborigen1020>	hi all. How to assemble odbc-driver from source code? command ./configure is says "error: C preprocessor "/lib/cpp" fails sanity check"
Sep 11 15:06:42 <lseactuary>	i have say 10 tables in postgresql. i want to do something so user A has permissions to read 2 of them and not the others, B has other permissions etc. permissions can change. I am wondering what the setup should be to enable this? creating a role per user?
Sep 11 15:07:08 <Myon>	aborigen1020: is /lib/cpp a working C preprocessor?
Sep 11 15:07:14 <ariejan>	RhodiumToad I can use lag() to add the previous state to the current one. Maybe I can use that as a sub-query to select rows where the current state is not equal to the previous state.
Sep 11 15:08:33 <[patrik]>	lseactuary: create a role per "application use case" instead. Like APP_USER, APP_SUPERUSER etc.
Sep 11 15:09:00 <[patrik]>	and grant your select, update, insert privs to the different tables to these application usecase roles.
Sep 11 15:09:30 <lseactuary>	that is what i was thinking, but with 10 tables, we are looking at many combinations. that is why i was thinking a role per user would be better?
Sep 11 15:09:47 <aborigen1020>	Myon, not certain
Sep 11 15:10:20 <lseactuary>	[patrik] e.g. 1 user could have table1, table2; user2 could have table1, table3, user3 could have nothing, user4 could have table1,table2,table3 etc
Sep 11 15:10:24 <Myon>	more details in config.log
Sep 11 15:10:27 <lseactuary>	that is already 4 combinations in just 3 tables
Sep 11 15:10:50 <lseactuary>	also permissions are read only
Sep 11 15:12:52 <[patrik]>	lseactuary: 10 tables is a rather small data model. Also it is my experience that when you allow "individual" grants rather than application-use-case grants, you'll always have the self proclaimed Excel-master dude of the place login with his ODBC-driver and Excel and mess shit up.
Sep 11 15:13:08 <lluad>	aborigen1020, configure generally leaves a configure.log file lying around that shows exactly what failed and where
Sep 11 15:13:38 <aborigen1020>	Myon, various version psql-odbc give various error. e.g.  "psqlodbc-09.06.0500" says "error: unixODBC library "odbcinst" not found". But  in config.log: "ODBC_CONFIG='/usr/local/bin/odbc_config'"
Sep 11 15:13:51 <[patrik]>	I've seen this more times than i'd like to remember. Im still subscribing to the application-use-case role based stuff, so you allow access only through your app. Not directly to tables for individual users.
Sep 11 15:13:55 <lseactuary>	[patrik] in reality there are hundreds, i am just testing on a smaller subset
Sep 11 15:14:06 <[patrik]>	oh ok.
Sep 11 15:14:17 <lseactuary>	[patrik] do you mean some API gateway which manages permission?
Sep 11 15:14:31 <lseactuary>	the plan was for only admin to grant permissions, i will write this as code once we get the request
Sep 11 15:14:38 <lseactuary>	im just trying to figure out the right mechanism
Sep 11 15:14:42 <lseactuary>	so i can code it
Sep 11 15:14:56 <Myon>	aborigen1020: why compile such an old version?
Sep 11 15:15:50 <aborigen1020>	Myon, i use postgresql "old' version - 9.6.5
Sep 11 15:15:58 <Myon>	doesn't matter
Sep 11 15:16:03 <Myon>	the driver is compatible
Sep 11 15:16:16 <Myon>	also, why use 9.6.5 and not 9.6.15?
Sep 11 15:16:40 <Myon>	your OS should have packages for psqlodbc, use these
Sep 11 15:17:51 <aborigen1020>	<Myon>, yes, have, but with version from repository i got segfault...
Sep 11 15:18:18 <[patrik]>	lseactuary: no. just identify which functions (forms or webpage or whatever) use which tables and in your application grant the correct permissions to the role(s). When the next application user comes along just grant him "APP_USER" or whatever.
Sep 11 15:18:22 <dotjosh_>	I'm running pg_dump for the first time on windows and I can see it writing the file to about 94MB and stopping, but the pg_dump process is never ending.  Using c:\Program Files\PostgreSQL\10\bin>pg_dump -U myuser -W -F t mydbname > d:\backup.tar
Sep 11 15:18:34 <aborigen1020>	so i tryed install driver from source
Sep 11 15:18:51 <Myon>	even more reason to use a newer version
Sep 11 15:19:04 <Myon>	which OS is that, btw?
Sep 11 15:19:49 <xocolatl>	in a C trigger, what's the best way to find out if a column has been updated or not?
Sep 11 15:20:20 <aborigen1020>	Myon, Debian 9.2
Sep 11 15:20:52 <Myon>	you should deploy upgrades, which have a current PostgreSQL version
Sep 11 15:20:55 <lavalike>	how many triggers is a foreign key worth?
Sep 11 15:21:00 <Myon>	for a newer psqlodbc, use apt.postgresql.org
Sep 11 15:21:27 <xocolatl>	lavalike: two
Sep 11 15:22:35 <lavalike>	xocolatl: really! I was starting to think it was a lot more
Sep 11 15:22:42 <xocolatl>	why?
Sep 11 15:23:16 <lavalike>	I started out with implementing INSERT so that it does check the referenced id in the other table, but then there's UPDATE, and DELETE
Sep 11 15:23:42 <xocolatl>	that's the second trigger, on the other side
Sep 11 15:23:42 <Myon>	aborigen1020: also, there's proper odbc libs in Debian, your /usr/local/bin suggests that you messed around a lot
Sep 11 15:23:52 <Myon>	and /lib/cpp... good luck with that system
Sep 11 15:24:17 <xocolatl>	lavalike: looks like it's 4 triggers
Sep 11 15:24:52 <aborigen1020>	and with psqlodbc-11.00.0000 i got : "error: unixODBC library "odbcinst" not found".
Sep 11 15:25:05 <lavalike>	xocolatl: some of those VERBs can be squashed together?
Sep 11 15:25:07 *	xocolatl should know this, since he implemented his own fks with 4 triggers
Sep 11 15:25:28 <xocolatl>	insert/update on fk side, update/delete on pk side
Sep 11 15:26:05 <ilmari>	Myon: /lib/cpp is an alterntive managed by the 'cpp' package
Sep 11 15:26:42 <Myon>	ilmari: *this* /lib/cpp is something that makes ./configure choke
Sep 11 15:26:48 <ilmari>	ah
Sep 11 15:27:09 <Myon>	aborigen1020: are odbcinst and odbcinst1debian2 properly installed?
Sep 11 15:27:20 <ilmari>	aborigen1020: what does 'update-alternatives --display cpp' output?
Sep 11 15:27:22 <lavalike>	xocolatl: good point, DELETE is not needed on the first side
Sep 11 15:27:55 <aborigen1020>	<Myon>, i will be happy to use version from  Debian repository, but with her i do not use odbc, because got segfault..
Sep 11 15:28:37 <aborigen1020>	Myon, yes
Sep 11 15:30:32 <aborigen1020>	<ilmari> - cpp - a  link best version is /usr/bin/cpp
Sep 11 15:30:32 <aborigen1020>	  links currentrly points to /usr/bin/cpp
Sep 11 15:30:32 <aborigen1020>	  link cpp is /lib/cpp
Sep 11 15:30:32 <aborigen1020>	/usr/bin/cpp  priority 10
Sep 11 15:30:39 <aborigen1020>	automatic mode
Sep 11 15:32:19 <xocolatl>	lavalike: what are you working on?
Sep 11 15:33:06 <lavalike>	xocolatl: I was helping a person yesterday and they wanted to make foreign key constraints transitive, the only solution I could come up with was using triggers, and then I realized I needed more than 1
Sep 11 15:33:17 <aborigen1020>	<Myon> and i assemble unixODBC from sources, and use option --with-unixodbc=/path/to/unixODBC/ and gets the same error - "odbcinst not found"
Sep 11 15:33:23 <xocolatl>	lavalike: transitive?
Sep 11 15:33:38 <lavalike>	xocolatl: I agree it warrants some explanation hang on
Sep 11 15:39:08 <lavalike>	xocolatl: https://pastebin.com/raw/AeYnWCQ0 they were trying to make the last insert fail, because the referenced rows do not reference the same row in foo
Sep 11 15:39:47 *	xocolatl studies
Sep 11 15:40:27 <xocolatl>	I see
Sep 11 15:42:10 <xocolatl>	lavalike: create table quux (id, barid, bazid, fooid, foreign key (barid, fooid) references bar, foreign key (bazid, fooid) references baz);
Sep 11 15:43:42 <xocolatl>	not sure how this should be modeled "correctly"
Sep 11 15:44:02 <xocolatl>	what's the real-world use case for such a model?
Sep 11 15:44:15 <lavalike>	that I dont' know
Sep 11 15:44:52 <xocolatl>	you don't know the real-world use case?
Sep 11 15:44:56 <lavalike>	nope
Sep 11 15:45:01 <Myon>	aborigen1020: I'd think whatever makes your current install segfault will also make the build segfault, so better try to fix the real problem
Sep 11 15:45:15 <Myon>	the Debian packages should really Just Work
Sep 11 15:45:18 <xocolatl>	oh, was it something in this channel?  I was thinking you were at a client
Sep 11 15:45:33 <Myon>	if something segfaults, install the dbgsym package, and run gdb
**** BEGIN LOGGING AT Wed Sep 11 15:47:02 2019

Sep 11 15:47:02 *	Now talking on #postgresql
Sep 11 15:47:02 *	Topic for #postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 11 15:47:02 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Tue Sep 10 01:19:16 2019)
Sep 11 15:47:02 *	Channel #postgresql url: https://www.postgresql.org
Sep 11 15:47:04 <lavalike>	(:
Sep 11 15:50:04 <lavalike>	how does  FOREIGN KEY (barId, fooId) REFERENCES bar  work?
Sep 11 15:50:38 <aborigen1020>	Myon, i use gdb from this command "isql -v dsn-name", and got some small lines: "Program received signal SIGSEGV, Segmentation fault.
Sep 11 15:50:39 <aborigen1020>	strlen () at ../sysdeps/x86_64/strlen.S:106
Sep 11 15:50:39 <aborigen1020>	106	../sysdeps/x86_64/strlen.S: No such file.
Sep 11 15:50:56 <lavalike>	oh, yeah, fooId is in the table now, nevermind
Sep 11 15:50:57 <xocolatl>	lavalike: what do you mean?  (it was shorthand, I can write out the whole thing if you want)
Sep 11 15:51:17 <lavalike>	no I get it, I was thinking in the given example it seemed to pick barId from the quux table but fooId from the bar table and I was puzzled
Sep 11 15:51:32 <xocolatl>	no
Sep 11 15:52:04 <lavalike>	my brain refused to pick up the added column to quux even tho it had processed that it was there
Sep 11 15:52:06 <xocolatl>	also unfortunate is postgres doesn't realize that bar(id, fooid) is unique even though id is the pk, so a second index is required
Sep 11 15:52:19 <aborigen1020>	Myon, send this to bugs.debian.org? :)
Sep 11 15:58:15 <Myon>	aborigen1020: yes, but please install debug symbols first
**** ENDING LOGGING AT Wed Sep 11 16:33:19 2019

**** BEGIN LOGGING AT Wed Sep 11 16:33:28 2019

Sep 11 16:33:28 *	Now talking on #postgresql
Sep 11 16:33:28 *	Topic for #postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 11 16:33:28 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Tue Sep 10 01:19:16 2019)
Sep 11 16:33:28 *	Channel #postgresql url: https://www.postgresql.org
Sep 11 16:38:03 <dognosewhiskers>	I've spent the last "numerous hours" swearing and fiddling with my PostgreSQL installation (Windows 10). The EnterpriseDB installer for Windows has broken somehow. Likely Microsoft's rather than EnterpriseDB, Inc.'s fault, but still, it gives that nonsensical error about not being able to finish the installation and you end up with a semi-broken installation (no matter what "tricks" you try to solve it -- they don't work) and have to figure
Sep 11 16:38:03 <dognosewhiskers>	out how to initdb and start PG on your own. No "service" is installed, so I now need to have an ugly extra cmd.exe window running at all times (as if I didn't already have enough of those...) just to keep PG up.
Sep 11 16:38:11 <dognosewhiskers>	So to clarify: I have my PG running now, but it's in a very... "flimsy" state. I don't feel comfortable doing it like this, and it took me a very long time and lots of effort to figure out. If somebody from EnterpriseDB is reading this, you should fire up a disgusting Windows 10 VM and try to figure this out ASAP. (I don't blame you for not keeping up with the constant random breaking changes by MS.)
Sep 11 16:39:01 <rivyn>	Why not contact EnterpriseDB support about it?
Sep 11 16:40:27 <dognosewhiskers>	rivyn: Because they probably are in here and also, e-mail has been made frustrating these days.
Sep 11 16:47:19 <rivyn>	Probably not, actually.
Sep 11 16:49:33 <lol-md5>	xocolatl: <xocolatl> also unfortunate is postgres doesn't realize that bar(id, fooid) is unique even though id is the pk, so a second index is required
Sep 11 16:49:37 <lol-md5>	could you elaborate on this?
Sep 11 16:50:52 <xocolatl>	lol-md5: a foreign key requires a unique index on the pk side.  if you have a compound foreign key (such as (id, fooid)) that contains something that is already known unique/notnull, you still need to have a separate index over all the keys.  that's just dumb
Sep 11 16:51:17 <xocolatl>	postgres should be able to use the pk index on (id) for a foreign key containing it
Sep 11 16:51:28 <lol-md5>	xocolatl: you mean you have to index (id, fooid)?
Sep 11 16:51:31 <xocolatl>	yes
Sep 11 16:51:41 <lol-md5>	hum
Sep 11 16:55:03 <Zr40>	did nobody bother to write a patch, or is there an actual reason why that's the case?
Sep 11 16:55:15 <xocolatl>	I don't think anyone has bothered
Sep 11 17:04:55 <davidfetter>	Myon, I was wondering whether pg_dirtyread, or at least some of its infrastructure, should go into core
Sep 11 17:05:25 <davidfetter>	Myon, this is because it appears to depend pretty tightly on tupconvert from core
Sep 11 17:11:31 <localhorse>	does the order of conditions chained with AND in a WHERE clause matter? are they shortcircuit-evaluated?
Sep 11 17:12:16 <peerce>	the expression is optimized, and yes if any expression is false, it doesn't have to eval the rest
Sep 11 17:12:33 <peerce>	AFAIK, there's no guaranteed execution order of a AND b AND c
Sep 11 17:15:37 <xocolatl>	localhorse: there is absolutely no guarantee of execution order.  you can't "protect" one condition with another like you can in imperative languages
Sep 11 17:15:54 <localhorse>	ok good
Sep 11 17:18:31 <esran>	xocolatl, if id is the pk then surely your foreign key is just id and it doesn't matter what fooid is so drop it from the fk?
Sep 11 17:18:47 <xocolatl>	surely not!
Sep 11 17:21:50 <xocolatl>	there are many reasons to want more columns in the fk
Sep 11 17:36:29 <davidfetter>	right now, surrogate keys are often a way to save space. I wonder if there isn't a way to implement FKs so that they're pointers to the PKs instead of separate storage
Sep 11 17:38:44 <xocolatl>	wouldn't solve this problem
Sep 11 17:42:40 <zakharyas>	Noob question, but how I can both return and use the result of a function in a SELECT?  I want to do something like SELECT length(foo) AS bar, bar - 3, but the second use of "bar" is treated as a column, and it doesn't exist.
Sep 11 17:43:46 <peerce>	length(foo) as bar, lenth(foo)-3 as bar3
Sep 11 17:43:52 <peerce>	length, even.
Sep 11 17:44:06 <peerce>	you cna't reference aliases in other expressions at the same level
Sep 11 17:44:23 *	davidfetter DCCs peerce some coffee
Sep 11 17:44:28 <zakharyas>	peerce: that would work but I'd prefer not to repeat the function as it's rather expensive.
Sep 11 17:44:48 <peerce>	then use a CTE
Sep 11 17:44:54 <xocolatl>	zakharyas: you'll have to use a superquery then
Sep 11 17:45:04 <davidfetter>	zakharyas, pg is smart enough to execute functions appropriately...if they're not marked as VOLATILE
Sep 11 17:45:27 <davidfetter>	(in which case it's still smart enough, and it executes them each time they're called)
Sep 12 10:08:34 *	Disconnected ()
**** ENDING LOGGING AT Thu Sep 12 10:08:34 2019

**** BEGIN LOGGING AT Thu Sep 12 10:08:58 2019

Sep 12 10:08:58 *	Now talking on #postgresql
Sep 12 10:08:58 *	Topic for #postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 12 10:08:58 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Tue Sep 10 01:19:16 2019)
Sep 12 10:08:59 *	Channel #postgresql url: https://www.postgresql.org
Sep 12 10:09:37 <enoq>	what I've done in the past is to create some text field that contains contents of all fields
Sep 12 10:09:46 <enoq>	the ones that I want to query
Sep 12 10:11:14 <aborigen1020>	peerce, no. All configs for access via odbc are identical
Sep 12 10:12:03 <peerce>	you could create a full text index that was based on the concatenation of the fields.
Sep 12 10:12:17 <peerce>	and use the full text search primitives
Sep 12 10:12:24 <peerce>	??FTS
Sep 12 10:12:24 <pg_docbot>	http://www.sai.msu.su/~megera/postgres/gist/tsearch/V2/ :: http://rachbelaid.com/postgres-full-text-search-is-good-enough/
Sep 12 10:12:24 <pg_docbot>	http://www.sai.msu.su/~megera/postgres/fts/doc/index.html :: https://www.postgresql.org/docs/current/static/textsearch.html
Sep 12 10:12:30 <enoq>	thank you
Sep 12 10:13:48 <Myon>	aborigen1020: there's something fishy with the filesystem I think, config.log is full of errors that it can't find conftest.c
Sep 12 10:16:03 <aborigen1020>	and this file is missing on the 2 systems
Sep 12 10:31:46 <enoq>	in mysql there's a size limit for indices, is there such a thing in postgres?
Sep 12 10:31:58 <enoq>	mainly interested in indices over text fields
Sep 12 10:38:13 <Myon>	enoq: yes, somewhere around 1-2kB
Sep 12 10:38:41 <enoq>	is that a reason to go for varchar(255)?
Sep 12 10:38:49 <enoq>	instead of text
Sep 12 10:39:05 <Myon>	varchar(1000) is a thing as well
Sep 12 10:39:23 <Myon>	(won't help with long utf8 chars, though)
Sep 12 10:39:25 <enoq>	I suppose if you have an index over a text field it will throw an exception if it surpasses the index limit
Sep 12 10:39:38 <Myon>	yes
Sep 12 10:39:39 <peerce>	a varchar(255) would error if you tried to store a longer string in it, is that what you want?
Sep 12 10:40:08 <pstef>	possibly
Sep 12 10:40:27 <pstef>	if you want to avoid getting another error (that the value can't fit into an index)
Sep 12 10:40:29 <peerce>	any sort of error exception requires the whole transaction be rolled back
Sep 12 10:40:55 <pstef>	my workaround has been index on left(field, limit)
Sep 12 10:41:28 <Myon>	pstef: but I guess the planner then needs that in the query as well?
Sep 12 10:41:38 <Myon>	hash indexes would work I guess
Sep 12 10:42:19 <pstef>	yes, sorry for being vague. you'd need left(field, limit) in the query too, and then the desired "x = field" predicate
Sep 12 10:42:29 <enoq>	is varchar implemented on bytes? so a full unicode string would take 4 times it's size?
Sep 12 10:42:48 <Myon>	enoq: varchar(n) is n characters
Sep 12 10:43:47 <pstef>	hash indexes seems a good idea now; I didn't have those the last time I had this problem :)
Sep 12 10:44:08 <Myon>	same here
Sep 12 10:44:53 <enoq>	ah I see
Sep 12 10:45:00 <enoq>	but that of course affects indices
Sep 12 10:45:40 <enoq>	so varchar 512 would fit into the 2kb maximum for an index (worst case)
Sep 12 10:49:37 <enoq>	correct?
Sep 12 11:10:15 <kevinsjoberg>	I'm trying to write a query for this problem http://sqlfiddle.com/#!17/a6646/2. I've played around a bit but does not manage to get anything giving me the desired result. Any tips?
Sep 12 11:46:41 <Renter>	kevinsjoberg: so that's a puzzle where you're given the schema and the problem and need to produce the result?
Sep 12 11:46:50 <Renter>	Or do you need to alter the schema as well?
Sep 12 11:48:00 <kevinsjoberg>	Renter: The schema I've written myself to demonstrate the problem I'm having. The schema is a simplified version of my application's schema.
Sep 12 11:48:40 <kevinsjoberg>	The problem could be explained as "Given n amount of people belonging to the same fact, the same people need to be part of the same family to be considered valid".
Sep 12 11:49:22 <kevinsjoberg>	I get false positives when I dabble with the queries myself (I'm no SQL-expert by any means).
Sep 12 11:49:31 <Renter>	Is there a reason why you only have the maps?
Sep 12 11:49:47 <Renter>	To me relational databases start off by having single lines for single entities in tables
Sep 12 11:49:59 <Renter>	So for instances, to me this has three types of entities: facts, persons and families
Sep 12 11:50:53 <Renter>	The need becomes more obvious if you add some more metadata to facts
Sep 12 11:50:56 <kevinsjoberg>	Renter: There is a reason to it. This data does not map directly to tables in my database but are constructed from multiple data sources.
Sep 12 11:51:15 <kevinsjoberg>	Renter: Some of the data, I don't even control.
Sep 12 11:51:32 <Renter>	yeah, so your _maps tables are nxm relations that bind different tables together right?
Sep 12 11:52:54 <Renter>	the problem becomes obvious when you simply want a list of unique facts, you have to write something like 'select distinct fact_id from fact_maps order by fact_id'
Sep 12 11:53:33 <Myon>	kevinsjoberg: select fact_id, family_id, count(*) from fact_maps fc join family_maps fm on fc.person_id = fm.person_id group by 1, 2 having count(*) > 1;
Sep 12 11:59:34 *	Disconnected ()
**** ENDING LOGGING AT Thu Sep 12 11:59:34 2019

**** BEGIN LOGGING AT Thu Sep 12 11:59:57 2019

Sep 12 11:59:57 *	Now talking on #postgresql
Sep 12 11:59:57 *	Topic for #postgresql is: PostgreSQL 12beta4 is coming on Thursday. Get ready to test! || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 12 11:59:57 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Tue Sep 10 01:19:16 2019)
Sep 12 11:59:58 *	Channel #postgresql url: https://www.postgresql.org
Sep 12 12:00:07 <kevinsjoberg>	Myon: Thanks, it seems to be working alright. I think the having part was what I was missing.
Sep 12 12:06:40 <gajus>	given that LATERAL JOINs and DISTINCT ON can often solve the same problem, is there a clear reason to always prefer one over the other?
Sep 12 12:07:44 <gajus>	My rough way of picking one or the other depends on how large the dataset set it
Sep 12 12:08:00 <gajus>	distinct on generally will have faster query execution plan but will be more memory greedy
Sep 12 12:08:11 <gajus>	lateral will be more memory efficient but slower
Sep 12 12:08:18 <gajus>	is there anything else to consider?
Sep 12 12:16:37 <Janni>	Hello.
Sep 12 12:17:20 <Janni>	Does anybody know how to log the parameters of failed queries?
Sep 12 12:18:03 <Janni>	I DO get parameters in my logs for queries that are slower than the log_min_duration threshold. That allows me to reproduce and debug.
Sep 12 12:18:32 <Janni>	However when a query times out or there is some other ERROR, I do not get any parameters in the logs, which makes things much harder.
Sep 12 12:41:28 <enoq>	does unique check null?
Sep 12 12:41:43 <enoq>	I have an optional field that should be either null or unique
Sep 12 12:42:12 <Myon>	what happened when you tried?
Sep 12 12:42:30 <enoq>	I'm currently defining the schema upfront
Sep 12 12:43:14 <enoq>	ah nvm "However, two null values are never considered equal in this comparison. That means even in the presence of a unique constraint it is possible to store duplicate rows that contain a"
Sep 12 12:43:33 <enoq>	https://www.postgresql.org/docs/11/ddl-constraints.html#DDL-CONSTRAINTS-UNIQUE-CONSTRAINTS
Sep 12 12:46:09 <enoq>	I suppose that's why you use IS NULL rather than = NULL
Sep 12 13:38:33 <ne2k>	I'm writing a trigger to modify a value on insert based on a modifier stored in another table. it feels as though it would be efficient to do this as a per statement trigger, but this appears to only be allowed AFTER insert; should I suck it up and do it per row, or is there a way to do what I want per statement?
Sep 12 13:41:49 <Myon>	you'll need a transition table for that, and PG11+ (or was it 10?)
Sep 12 13:42:11 <Myon>	otherwise, statement level triggers don't see individual rows
Sep 12 13:42:36 <ne2k>	Myon, I have 11, but the docs say that they can only be used on AFTER. and I presume it would be bad form to allow it to insert the (unmodified) data, and the do an update to modify it?
Sep 12 13:42:56 <ne2k>	if that would even work anyway -- it would end up being an upsert, which has to be written with a per-row thing anyway
Sep 12 13:44:49 <Myon>	yeah that sounds horrible
Sep 12 13:45:43 <ne2k>	I'll just do it per row
Sep 12 13:46:35 <aborigen1020>	Myon, i quit trying to build psql-odbc drivers from src. I created new cluster, is created dump from working cluster and restored him in new cluster. After this, i changed settings for DSN (on /etc/odbc.ini - write on port to connection), and ALL is working! Congratulations! This is large success.
Sep 12 13:47:23 <Myon>	odbc is quite happy to segfault if something is wrong in the config, afaict
Sep 12 13:48:09 <aborigen1020>	I must say one thing: i dont install package odbc-postgresql, but copy two lib for him from working servers
Sep 12 13:50:15 <aborigen1020>	config odbc is standarted on all my servers, and not writted by "nahds" - only copy.
Sep 12 14:19:41 <Mikjaer>	I am going to create a table which needs to hold historical data for each row, what is the best way to do this? An exampe, date, name and phonenumber and we want to be able to know what phonenumber a given person had at a given time. The best / most efficient way to do this?
Sep 12 14:22:54 <ne2k>	Mikjaer, you basically just need to add a timestamp column and add it to the primary key; then you can select distinct on that order by ts desc to get the most recent one, or select where ts between a and b, or whatever.
Sep 12 14:23:22 <Mikjaer>	ne2k: that was our idea as well, but we're afraid that the performance is going to be horible?
Sep 12 14:23:53 <ne2k>	Mikjaer, you can either do that and handle it in the queries, or have two tables, one with the current data and one with historic data, which you append to with a trigger when you update the main one
Sep 12 14:25:53 <ne2k>	Mikjaer, I would do it with a single table, and make a view for the "current one"; then you can access the current data as if it's a table, and you don't have to write any different queries, and then later, if you decide to optimize it by moving it to two tables and a trigger, you don't need to change your queries
Sep 12 14:26:37 <ne2k>	Mikjaer, how much data and how many reads/writes per time are we talking about, here anyway?
Sep 12 14:27:41 <Mikjaer>	around 100 millions rows, and it's only being used for report generating
Sep 12 14:29:00 <Mikjaer>	We store the entire dataset on a different server, and record changes, because we need to be able to show the entire history of the data, for legal purposes.
Sep 12 14:30:54 <ne2k>	Mikjaer, so you're likely to access the "current" one far, far more often than any of the old ones?
Sep 12 14:31:22 <Mikjaer>	yes
Sep 12 14:31:49 <ne2k>	and is the historic data never required to be accessible on the "live" server, only through some other access method?
Sep 12 14:32:10 <Mikjaer>	but the system is to be used by accountants that might want to know things like "How many values have been altered by more then x during period z" and not have to wait several minutes to get data back
Sep 12 14:32:48 <Mikjaer>	The historic data is only used by accountants, lawyers and stuff like that if we need to proove something in a court or towards a goverment agency
Sep 12 14:33:55 <ne2k>	select * from phone_archive where ts between za and zb group by userid having count(*) > x;
Sep 12 14:34:08 <ne2k>	I can't imagine that is going to take minutes on 100 million rows
Sep 12 14:34:45 <Mikjaer>	That was my opinon as well
Sep 12 14:35:09 <ne2k>	Mikjaer, but given the size, and the fact that current is used a lot, would make me optimize it from the start
Sep 12 14:35:58 <ne2k>	i.e. store current in one table, and then update archive (possibly on a foreign table) with a trigger
Sep 12 14:37:40 <Mikjaer>	Can
Sep 12 14:37:49 <Mikjaer>	Can't i do that with something like cached-view?
Sep 12 14:46:40 <theseb>	Why do subqueries need parens like this... "select <column list> from (<select statement>) <name>;
Sep 12 14:48:09 <Moonsilence>	because the from list are individual table expressions... syntactically needed in the case of subqueries
Sep 12 14:48:49 <Moonsilence>	think of each subquery as a tabular result, from which you select just like directly from real tables.
Sep 12 14:49:10 <Moonsilence>	...FROM table_a, (subquery), table_b, view_c, ...
Sep 12 14:52:45 <theseb>	"select 1, 2, 3" returns a single record of raw data...how would you return MULTIPLE records?  I tried "select ( (1, 2, 3), (4, 5, 6) );" but that barfed
Sep 12 14:54:16 <Moonsilence>	have a look at the examples in the docs for the sql-command VALUES
Sep 12 14:54:20 <Moonsilence>	??values
Sep 12 14:54:20 <pg_docbot>	https://www.postgresql.org/docs/current/static/sql-values.html
Sep 12 15:00:19 <ne2k>	theseb, values ((select(1,2,3))), ((select(4,5,6))); does what you want. not sure why you'd want to, mind you
Sep 12 15:01:46 <ilmari>	that gives you two rows of one column each containing a row value
Sep 12 15:02:02 <ilmari>	values (1,2,3),(3,4,5); gives you two rows of three columns
Sep 12 15:04:46 <theseb>	ilmari, ne2k:  values (1, 2, 3), (4, 5, 6); indeed gives 2 rows of 3 cols.....in mysql that syntax does not work....why isn't this more universal?
Sep 12 15:05:40 <Myon>	the likely answer is "mysql isn't following the SQL standard there"
Sep 12 15:11:13 <ne2k>	theseb, perhaps a more important question is: what are you actually trying to do?
Sep 12 15:11:54 <theseb>	ne2k: thanks...i sometimes want/need to create table like objects from raw data because i don't have permission to add to the database
Sep 12 15:12:04 <theseb>	ne2k: it seems select 1, 2, 3;
Sep 12 15:12:12 <Myon>	there's also UNION
Sep 12 15:12:14 <theseb>	ne2k; gives me a single record of raw data
Sep 12 15:12:24 <theseb>	ne2k: i just got stuck at doing multiple records
Sep 12 15:12:45 <Myon>	select 1,2,3 from generate_series(1,3);
Sep 12 15:13:23 <ilmari>	theseb: the VALUES page shows the equivalent SELECT ... UNION ALL sequence
Sep 12 15:13:42 <theseb>	Myon: wait...suppose you wanted 1st record to have 3 columns w/ values "a", "b", "c"....and second record to contain "d", "e", "f"
Sep 12 15:13:52 <Myon>	values
Sep 12 15:13:54 <ilmari>	if you need it to work on datbases that don't support the standard VALUES syntax
Sep 12 15:14:06 <theseb>	Myon: values ("a", "b", "c"), ("d", "e", "f")
Sep 12 15:14:28 <theseb>	Myon: sadly PG seems to be the only sane SQL today
Sep 12 15:14:54 <Myon>	not our problem :)
Sep 12 15:15:07 <theseb>	:)
Sep 12 15:15:17 <Myon>	also, it's 'a', not "a"
Sep 12 15:15:21 <theseb>	right
Sep 12 15:15:31 <ne2k>	theseb, depending on what you're actually trying to do, you might want to consider jsonb columns too.
Sep 12 15:15:52 <theseb>	Myon: does SELECT have a way to do the equivalent of values ("a", "b", "c"), ("d", "e", "f") ?
Sep 12 15:15:56 <theseb>	(w/ single quotes) ?
Sep 12 15:16:26 <theseb>	Myon: maybe it will be portable
Sep 12 15:16:37 <ilmari>	14:13 < ilmari> theseb: the VALUES page shows the equivalent SELECT ... UNION ALL sequence
Sep 12 15:16:40 <ilmari>	theseb: ^^
Sep 12 15:16:56 <ne2k>	theseb, that bit about "what are you /actually/ trying to do" still seems to be rather vague
Sep 12 15:17:04 <ilmari>	theseb: https://www.postgresql.org/docs/current/sql-values.html#id-1.9.3.184.8
Sep 12 15:17:21 <theseb>	ne2k: ok...i'll give more details....sec
Sep 12 15:19:20 <theseb>	ne2k: https://pastebin.com/wC3vf6ii
Sep 12 15:19:33 <theseb>	ne2k: see those long hex strings?
Sep 12 15:19:56 <doev>	Can a identity/always column used in combination with inheritance ?
Sep 12 15:20:17 <theseb>	ne2k: the script pulls some data from a database on a specific company identified by its unique hex strings......i want to rerun that script for several companies all with different hex strings
Sep 12 15:20:23 <doev>	It seems, if all child tables use an own counter.
Sep 12 15:20:49 <theseb>	ne2k: so basically i want to do something like a for loop where i alter those hex strings for each loop (company)
Sep 12 15:21:24 <theseb>	ne2k: because i can't add to the database....i need to make a table like structure of the hex strings on the fly to somehow use in a "for loop" type construct
Sep 12 15:22:17 <ne2k>	theseb, and you have some bizarre restriction that you can't create an actual table of the values?
Sep 12 15:23:21 <wds>	Hey, anyone that can help with a design question relating to narrow, large tables with timeseries-like data?
Sep 12 15:23:31 <theseb>	ne2k: yes
Sep 12 15:23:32 <ne2k>	wds, ??ask
Sep 12 15:23:39 <ne2k>	??ask
Sep 12 15:23:39 <pg_docbot>	http://www.catb.org/~esr/faqs/smart-questions.html :: https://workaround.org/getting-help-on-irc
Sep 12 15:23:47 <ne2k>	don't ask to ask, just ask
Sep 12 15:23:48 <theseb>	ne2k: it will say.."you don't have permission to create tables or some such"
Sep 12 15:24:37 <wds>	Got a table with just 4 columns; id, value, source and timestamp. Most queries involve "select value, source, time where source = x and time between a and b"
Sep 12 15:25:12 <wds>	table has like 40mil rows, and once a query is going to return a few tens of thousand, the planner uses a sequential scan
Sep 12 15:25:31 <ne2k>	wds, do you have an index on (source, time)?
Sep 12 15:25:45 <wds>	Yup, it still prefers sequential scan
Sep 12 15:25:57 <wds>	we got it to use index scanning when using "include value" on that same index
Sep 12 15:26:08 <wds>	I suppose because it then no longer has to access the table at all
Sep 12 15:26:11 <ne2k>	wds, I was having much the same sort of problem the other day.
Sep 12 15:26:19 <wds>	but that means our entire table is basically stored in the index, which seems a little... silly?
Sep 12 15:26:44 <ne2k>	wds, what is id for?
Sep 12 15:27:03 <wds>	it's really just the primary key and hasn't served a purpose in our application (so far).
Sep 12 15:27:23 <ne2k>	wds, isn't (source, time) more likely to be the primary key?
Sep 12 15:28:23 <wds>	It's a possibility, we haven't made the decision to limit the data that way
Sep 12 15:29:07 <ne2k>	theseb, so you basically want table company(name, hex1, hex2, hex3) and to lateral join that to you query you pasted, substituting the hex values for the ones from the table?
Sep 12 15:29:23 <ne2k>	theseb, except the table company has to be a literal
Sep 12 15:30:33 <ne2k>	wds, that include thing is new to me, I'd need to read up on it
Sep 12 15:31:39 <wds>	Clustering has helped the performance as well, but I'm a bit skeptical that for 40mil rows, our queries take over 2 minutes on an unclustered table?
Sep 12 15:32:00 <ne2k>	"However, an index-only scan can return the contents of non-key columns without having to visit the index's table, since they are available directly from the index entry." presumably, the cost of doing an index scan and visiting the table for each row is deemed to be greater than a seq scan in your case.
Sep 12 15:32:43 <ne2k>	wds, wait, what? selecting the rows between two times takes TWO MINUTES with a seq scan on 40m rows? that seems crazy
Sep 12 15:34:56 <wds>	ne2k Yup! That's what is baffling us
Sep 12 15:35:25 <wds>	Dataset doesn't seem too big to have reasonably performant queries even on a badly optimized database
Sep 12 15:36:51 <wds>	although we just finished a manual vacuum analyze and the query we've been testing with went from 152s to 54s
Sep 12 15:37:18 <theseb>	ne2k: i think so
Sep 12 15:37:30 <theseb>	ne2k: wait...lateral join?
Sep 12 15:37:30 <wds>	but that's also strange to us because we exclusively do insert and select statements, no update/delete. It was just after (re-) adding the composite index though
Sep 12 15:37:45 <theseb>	ne2k: i learned about inner, left, right, full, cross joins
Sep 12 15:38:00 <theseb>	ne2k: are you saying i missed one? there is also a LATERAL join?
Sep 12 15:38:19 <ne2k>	wds, lateral join is like a foreach; it allows you to use values on the left of the join in where conditions on the right
Sep 12 15:38:35 <theseb>	ne2k: was that for me?
Sep 12 15:38:50 <ne2k>	theseb, yes, apologies
Sep 12 15:39:00 <theseb>	ne2k: i was wondering how to do a for loop in SQL...if lateral joins is the way then they are definitely important
Sep 12 15:40:00 <ne2k>	theseb, you can always write a procedural function in plpgsql (or other), but lateral join allows you to do a lot of "foreach"-y things without needing to resort to it
Sep 12 15:40:49 <ne2k>	wds insert into test2 (select * from generate_series(1, 7000) a, generate_series('2019-01-01T00:00:00', now(), '1 hour') b, random()); # just making me 40m rows of test data.
Sep 12 15:41:26 <ne2k>	it's taking... quite a while
Sep 12 15:41:49 <theseb>	ne2k: lateral seems new... https://heap.io/blog/engineering/postgresqls-powerful-new-join-type-lateral
Sep 12 15:41:52 <wds>	okay so after VACUUM ANALYZE, our sequentially scanning for 1.9mil result rows in our 40mil table takes about 42-44 seconds. Still long. Forcing it to use (source, time) index takes 18 seconds
Sep 12 15:41:57 <theseb>	ne2k: it is a a "power new join type"
Sep 12 15:42:02 <ne2k>	theseb, it's relatively new
Sep 12 15:42:02 <theseb>	powerful*
Sep 12 15:42:24 <theseb>	ne2k: i can't believe SQL is 50 years old and there is no universal way to do a for loop
Sep 12 15:42:33 <theseb>	or that it was only added few years ago
Sep 12 15:42:38 <theseb>	ne2k: why is that?
Sep 12 15:44:57 <wds>	ne2k And if we only select source,time columns, it's like a 0.5s index scan and done. That's more like what we're expecting :D
Sep 12 15:45:02 <dognosewhiskers>	It's scary to me that BigSQL was removed from the Windows download page for PG. Now there's just EnterpriseDB's installer left, which, as I reported the other day, is broken now (likely due to random Microsoft changes to Windows 10 since it worked recently). I sure hope they don't remove Windows support entirely.
Sep 12 15:45:11 <ne2k>	wds, yes, that's because they're actually in the index
Sep 12 15:45:29 <wds>	Mh. So is it recommended/normal to index all columns in a narrow, large table?
Sep 12 15:45:44 <wds>	That's it's going to take twice the storage space
Sep 12 15:49:12 <ne2k>	wds, not something I can answer, I'm afraid. i've just tried select source, ts from test2 where source=99 and ts between '1 Mar 2019' and '31 May 2019'; and that took 1.7s
Sep 12 15:49:56 <ne2k>	oh, and now the same thing takes 6ms. as does select source, ts, value
Sep 12 15:50:08 <ne2k>	guess the index wasn't cached
Sep 12 15:50:30 <wds>	Yea so clearly we have some performance problems in general
Sep 12 15:50:49 <ne2k>	wds, what is the storage of your value column?
Sep 12 15:51:36 <wds>	\d+ gives storage: main for that column
Sep 12 15:53:04 <wds>	also, source is a foreign key to another table, in case that would matter?
Sep 12 15:54:00 <ne2k>	not for selects, it shouldn't
Sep 12 15:54:22 <ne2k>	wds, as I said, this is kinda beyond me, but I'd be very interested in the answers you get back
Sep 12 15:54:56 <ne2k>	wds, but when I do select source, ts, value from test2 where source=x and ts between a and b; it does it with an index scan and it's hella fast
Sep 12 15:55:11 <wds>	and value is not in your index?
Sep 12 15:55:16 <ne2k>	wds nope
Sep 12 15:55:25 <ne2k>	storage is plain on all columns
Sep 12 15:56:29 <wds>	Okay, we've tested some IO speeds, and while we run the super slow selects, the postgres processes add up to maybe 5MB/s (tops) while the statement is running (using oistat on ubuntu)
Sep 12 15:56:35 <ne2k>	just realized I foolishly inserted the same single random value for all the data, rather than a different one for each
Sep 12 15:57:06 <watmm>	Hi all. Can someone tell me why a standby with standby_mode on would be stuck recovering the last remaining megabyte or so of a db indefinitely? i.e. replication never completes :/
Sep 12 16:07:43 <ilmari>	watmm: standby_mode=on means that it will continuosly keep recovering WAL as it gets it, insted of stopping at the end
Sep 12 16:08:06 <ilmari>	watmm: do you have restore_command or primary_conninfo configured?
Sep 12 16:10:09 <ilmari>	watmm: if you want to be able to run read-only queries against the standby, set hot_standby=on
Sep 12 16:12:32 <watmm>	ilmari: I have hot_standy on, and primary_conninfo is defined, but no restore_command.
Sep 12 16:13:20 <watmm>	Oh i do have a pg_restore tool, it's just not in a conf anywhere
Sep 12 16:14:01 <Bish>	does a cascade delete not activate triggers?
Sep 12 16:14:27 <watmm>	The problem sounds like the one described in the pg_standby section of https://pgdash.io/blog/postgres-physical-replication.html
Sep 12 16:17:59 <Bish>	a after trigger will not fire for a cascade delete?
Sep 12 16:23:44 <wds>	Is there a way to monitor/test performance of IO operations on a query?
Sep 12 16:24:06 <wds>	*shifty eyes at our hard drives*
Sep 12 16:46:09 *	ChanServ gives channel operator status to xocolatl
Sep 12 16:46:28 *	xocolatl has changed the topic to: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 12 16:46:31 *	ChanServ removes channel operator status from xocolatl
Sep 12 16:49:51 <watmm>	Hrm. An update on the above. The standbys have the same wal files as the master so perhaps they are up to date, but the size of their DBs is smaller. Is this normal?
Sep 12 16:51:35 <doev>	How can I insert into a table with not use any values ... just to get an new row with id.
Sep 12 16:52:21 <Moonsilence>	Hi! How can orphan temp tables be left in the database? Will these eventually get dropped?
Sep 12 16:52:45 <xocolatl>	Moonsilence: yes
Sep 12 16:53:05 <xocolatl>	doev: insert into tablename default values;
Sep 12 16:55:02 <Moonsilence>	we had a power outage this morning and since then autovacuum is logging thousands of messages "found orphan temp table". I dont understand how they could have been left behing, since afaik, the application that created them, also died due to power loss, hence sessions gone, hence temp tables should dissappear.
Sep 12 16:55:30 <doev>	xocolatl, yes works
Sep 12 16:55:58 <Moonsilence>	even after rebooting my instance, they still remain and autovacuum is noticing them
Sep 12 16:56:14 <Moonsilence>	how can I fix this?
Sep 12 16:56:34 <xocolatl>	depending on your version autovacuum will just get rid of them, or when a new connection uses the same temp schema, or when wraparound comes
Sep 12 16:56:44 <xocolatl>	you can fix it yourself by dropping the temp schemas
Sep 12 16:57:46 <xocolatl>	drop schema pg_temp_4 cascade;  and similar
Sep 12 17:04:48 <Moonsilence>	thanks
Sep 12 17:04:52 <harks>	I have a query with 1.3 s explain analyze time, yet the query is already running for 1 minute without returning stuff.
Sep 12 17:05:29 <xocolatl>	harks: it's not something silly like forgetting the ; at the end, right?
Sep 12 17:05:37 <harks>	The query now finished after 4 min. Any ideas?
Sep 12 17:05:59 <xocolatl>	it might have been stuck behind a lock
Sep 12 17:06:08 <xocolatl>	if you run it again right now, does it take a long time?
Sep 12 17:08:00 <harks>	It's 1.4 sec now. But I'm the only one working on this local db.
Sep 12 17:08:31 <harks>	That's annoying.
Sep 12 17:10:19 <harks>	I hate bugs that vanish once I start to analyze them.
Sep 12 17:18:41 <Moonsilence>	xocolatl: Seems that 9.4 autovacuum doesnt drop orphaned temp tables, however in my 11 instances i found these log messages: autovacuum: dropping orphan temp table
Sep 12 17:19:03 <Moonsilence>	so somewhere between 9.4 and 11 this automatic cleanup was implemented
Sep 12 17:19:20 <G3nka1>	Hi can anyone help me with this error "Table has type tid at ordinal position 1, but query expects character varying."
Sep 12 17:22:35 <Moonsilence>	there are hidden columns xmin that might have been explicitly selected... these have type tid
Sep 12 17:27:03 <depesz>	G3nka1: what's the query?
Sep 12 17:27:14 <depesz>	and also, can you show us \d of the table?
Sep 12 17:27:19 <xocolatl>	Moonsilence: yes, I said "depends on the version"
Sep 12 17:27:20 <G3nka1>	depesz, Its an update query on a table with 11 collumns and first column is char var and last is timestamp and the rest being text
Sep 12 17:27:57 <G3nka1>	The update query is by a benchmark and I am not sure which collumns it is trying to update
Sep 12 17:27:59 <depesz>	G3nka1:please note I asked about query (and \d) and not "a description of a query"
Sep 12 17:29:54 <G3nka1>	depesz, https://paste.ubuntu.com/p/tT7ZPsCnBx/
Sep 12 17:31:05 <G3nka1>	>> Error in processing update to table: usertableorg.postgresql.util.PSQLException: ERROR: table row type and query-specified row type do not match Detail: Table has type tid at ordinal position 1, but query expects character varying.
Sep 12 17:31:06 <depesz>	G3nka1: i still don't know what the query is. also - did you try dropping the policy and retrying?
Sep 12 17:31:16 <G3nka1>	I am still digging the exact update query
Sep 12 17:36:54 <G3nka1>	Oh maybe I know whats wrong, will get back to you depesz thank you for your interest
Sep 12 17:52:00 <spread12>	does postgres support distributed transactions on linux?
Sep 12 17:53:48 <pstef>	what are distributed transactions?
Sep 12 17:54:27 <spread12>	nvm, the limitation is in my library
Sep 12 17:55:55 <G3nka1>	Okay I figured out that only after adding the policy on my table, update queries are failing. https://paste.ubuntu.com/p/mHg5XSPWTg/ Here is the function. I am sorry I still don't know the exact update query yet
Sep 12 17:56:07 <incognito>	spread12: if you mean distributed over network, i once saw an extension that gives to postgresql the capability of sharding data
Sep 12 17:57:32 <spread12>	this is the limitation: https://github.com/dotnet/corefx/issues/13532
Sep 12 17:58:51 <incognito>	spread12: yes, postgresql libs has this feature : they call that => connection pooling
Sep 12 18:01:50 <incognito>	spread12: http://www.npgsql.org/doc/connection-string-parameters.html => see the Pooling paragraph
Sep 12 18:05:30 <G3nka1>	i wonder twhat the type mismatch really is
Sep 12 18:09:08 <incognito>	G3nka1: what is your app ? it's weird. it's trying to update the ctid ?
Sep 12 18:09:48 <G3nka1>	incognito, I wrote that policy as an requireement to log every row that has an operation made on
Sep 12 18:10:27 <incognito>	G3nka1: before the policies, it was working ?
Sep 12 18:10:31 <RhodiumToad>	if you want to log modifications to the table you should use a trigger, not RLS
Sep 12 18:10:39 <G3nka1>	Yes incognito it was working
Sep 12 18:11:00 <RhodiumToad>	if you want to log reads, consider whether your requirements are reasonable
Sep 12 18:11:37 <G3nka1>	you are right RhodiumToad , this was mostly an hack and an experiment to benchmark if read logs enabled
Sep 12 18:12:03 <RhodiumToad>	G3nka1: nevertheless, I think you have found a bug - what pg version are you using?
Sep 12 18:12:26 <G3nka1>	9.5
Sep 12 18:12:40 <RhodiumToad>	9.5.what?
Sep 12 18:13:25 <G3nka1>	9.5.19
Sep 12 18:13:31 <spread12>	incognito: i was talking about this feature: https://www.npgsql.org/doc/transactions.html?q=distributed
Sep 12 18:13:55 <G3nka1>	Infact I didn
Sep 12 18:13:58 <G3nka1>	t
Sep 12 18:14:01 <RhodiumToad>	G3nka1: what was the exact error message, including any CONTEXT lines, and the explain of the failing query?
Sep 12 18:14:04 <G3nka1>	see this in earlier version of pg
Sep 12 18:14:57 <G3nka1>	It worked on 9.5.17 when I tried a few months ago, the same function
Sep 12 18:18:18 <RhodiumToad>	hm, tried to reproduce on 11.5 and failed
Sep 12 18:20:49 <RhodiumToad>	whut, did someone break readline detection in autoconf in the back branches?
Sep 12 18:21:37 <RhodiumToad>	oh, my mistake
Sep 12 18:22:14 <RhodiumToad>	typoed a directory name
Sep 12 18:23:05 <incognito>	G3nka1: in the example i just saw, they never use the table_name in the USING clause
Sep 12 18:23:27 <G3nka1>	SO, I am running YCSB workloada on table usertable with above policy. Typically this is how the update query will look 'usertable SET field4 = 'allow' WHERE field0 = 'ads'",,,,,,"UPDATE usertable SET field4 = 'allow' WHERE field0 = 'ads'",,,""' Pretty simple
Sep 12 18:23:42 <G3nka1>	incognito, I don
Sep 12 18:24:07 <G3nka1>	*I don't think its YCSB issue, rather something to do with postgres recent patchs
Sep 12 18:25:40 <RhodiumToad>	hm.
Sep 12 18:26:26 <RhodiumToad>	ok, I can reproduce on latest 9.5 stable
Sep 12 18:26:48 <G3nka1>	Should I file a bug RhodiumToad ?
Sep 12 18:27:03 <RhodiumToad>	yes
Sep 12 18:27:52 <RhodiumToad>	it doesn't seem to fail on 11.
Sep 12 18:29:09 <G3nka1>	RhodiumToad,  plpgsql immutable as $$ begin raise info 'log: %', $1; return true , likely this is the root cause combined with RLS?
Sep 12 18:29:39 <G3nka1>	or is it any policy?
Sep 12 18:53:36 *	Disconnected ()
**** ENDING LOGGING AT Thu Sep 12 18:53:36 2019

**** BEGIN LOGGING AT Thu Sep 12 18:54:02 2019

Sep 12 18:54:02 *	Now talking on #postgresql
Sep 12 18:54:02 *	Topic for #postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 12 18:54:02 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Thu Sep 12 16:46:28 2019)
Sep 12 18:54:02 *	Channel #postgresql url: https://www.postgresql.org
Sep 12 18:56:40 <G3nka1>	filed thanks RhodiumToad
Sep 12 19:07:49 <admin123>	anyone know how to speed this up? https://explain.depesz.com/s/fDSM
Sep 12 19:08:10 <admin123>	it's an event table with a start column of type date. I already have an index on date
Sep 12 19:08:17 <xocolatl>	it's 2ms ...
Sep 12 19:09:28 <depesz>	admin123: why do you want to speed it up?
Sep 12 19:10:08 <admin123>	xocolatl: you are pointing out something I did not consider haha
Sep 12 19:10:55 <admin123>	nvm sql is taking 41ms but the request as a whole takes 330ms. The speed up should be higher up the stack, probably in the serialization layer.
Sep 12 19:11:13 <admin123>	I want my whole response to be under 100ms
Sep 12 19:12:16 <ilmari>	make sure you're not doing multiple SQL queries that could be a single one with some joines
Sep 12 19:12:19 <ilmari>	*joins
Sep 12 19:13:26 <admin123>	this django debug toolbar says 29 queries in 22ms, request time 800ms. The slowness must be after the sql.
Sep 12 19:15:32 <peerce>	how much data are you reading ?
Sep 12 19:17:25 <admin123>	peerce: in the db or the response size?
Sep 12 19:26:51 <peerce>	the response size
Sep 12 19:27:03 <davidfetter_work>	hi
Sep 12 19:28:26 <davidfetter_work>	I have a completely static DB consisting of one table of sha1 hashes of leaked passwords.  Would it make sense to make its PK index "hot" with pg_prewarm or similar tricks?
Sep 12 19:28:38 <davidfetter_work>	(index is ~21GB, if that matters)
Sep 12 19:30:49 <RhodiumToad>	depends what kinds of lookup rates you want to handle
Sep 12 19:31:07 <RhodiumToad>	I would generally say not
Sep 12 19:31:24 <davidfetter_work>	pretty high
Sep 12 19:31:51 <RhodiumToad>	tens/sec? hundreds/sec? thousands/sec?
Sep 12 19:32:09 <davidfetter_work>	probably hundreds per second at peak
Sep 12 19:32:22 <RhodiumToad>	probably not worth bothering with then.
Sep 12 19:32:29 <davidfetter_work>	OK
Sep 12 19:33:10 <davidfetter_work>	I'd noticed that lookups can take from sub-ms (repeat of a very recent lookup) up through mid-hundreds of ms. I'd like to get my variance down, even if the minimum time goes up
Sep 12 19:33:31 <RhodiumToad>	mid-hundreds of ms is surprising unless you have slow i/o.
Sep 12 19:33:34 <admin123>	peerce: 46KB I think
Sep 12 19:33:56 <RhodiumToad>	try explain (analyze,buffers) to see how many buffers are being read
Sep 12 19:34:04 <RhodiumToad>	also track_io_timing
Sep 12 19:35:28 <davidfetter_work>	It's on EC2 with gp2 storage. I suppose I could buy more IOPS if that was actually going to give a good chance of helping. This is more about latency than throughput, obvs
Sep 12 19:35:44 <RhodiumToad>	ah.
Sep 12 19:35:50 <RhodiumToad>	how much ram on the instance?
Sep 12 19:36:03 *	davidfetter_work checks
Sep 12 19:36:48 <davidfetter_work>	dfetter@shadow01-db-new-admin-01:~$ cat /proc/meminfo
Sep 12 19:36:48 <davidfetter_work>	MemTotal:       32939544 kB
Sep 12 19:36:48 <davidfetter_work>	MemFree:          261456 kB
Sep 12 19:36:48 <davidfetter_work>	MemAvailable:   28506968 kB
Sep 12 19:37:14 <davidfetter_work>	I could put it on its own instance in cases the other DBs are being noisy neighbors
Sep 12 19:37:20 <davidfetter_work>	case*
Sep 12 19:37:54 <RhodiumToad>	what you really want I guess is to prewarm only the interior of the index and not the leaves
Sep 12 19:38:10 <RhodiumToad>	not sure how you could do that
Sep 12 19:38:16 <davidfetter_work>	I was about to ask
Sep 12 19:38:51 <davidfetter_work>	is there some way to find out where the interior is and span out, say, to the first set of branches from there?
Sep 12 19:39:00 <davidfetter_work>	er, where the root is
Sep 12 19:40:50 <RhodiumToad>	yes
Sep 12 19:41:04 <RhodiumToad>	a bit of work with pageinspect
Sep 12 19:42:46 <davidfetter_work>	oh, and I noticed when I made a hash index on the same column, it was getting chosen preferentially, even though I was getting numbers more like 5ms than the usual 3ms I was getting with the b-tree
Sep 12 19:43:02 <RhodiumToad>	the hash index was likely smaller?
Sep 12 19:43:10 <davidfetter_work>	yeah, about 15GB
Sep 12 19:43:33 <RhodiumToad>	that might give you more consistent lookup times
Sep 12 19:46:29 *	davidfetter_work rebuilds the hash index, having dropped it earlier
Sep 12 19:47:24 <brainicism>	i'm trying to perform a PITR recovery, with a base backup and some archived WAL files. I'm getting a 'invalid checkpoint record' when trying to start the server when relying on the `recovery.conf`, but don't receive that error when copying my WAL files directly to pg_wal/. anyone have any clues on how to debug this?
Sep 12 19:50:33 <RhodiumToad>	brainicism: does the base backup contain a backup_label file?
Sep 12 19:55:43 <brainicism>	RhodiumToadyes
Sep 12 19:56:56 <peerce>	is there a valid restore_command in the recovery.conf ?
Sep 12 19:57:20 <brainicism>	yes
Sep 12 19:57:27 <brainicism>	and its under the directory $PGDATA
Sep 12 20:11:44 <RhodiumToad>	brainicism: what's the content of the backup_label, and what were the actual log messages from starting the server with an empty pg_wal dir?
Sep 12 20:14:14 <brainicism>	```
Sep 12 20:14:19 <brainicism>	Uploaded file: https://uploads.kiwiirc.com/files/f53dc51bc4be37f94bd166ad9cbf6624/pasted.txt
Sep 12 20:14:42 <brainicism>	Uploaded file: https://uploads.kiwiirc.com/files/ff71275b6ef59929a17c13f22de5bd1f/pasted.txt
Sep 12 20:16:27 <RhodiumToad>	you're sure you put the recovery.conf in the right place?
Sep 12 20:16:44 <RhodiumToad>	it has to go in the data dir, NOT in the dir with the other conf files if those are elsewhere
Sep 12 20:17:04 <brainicism>	in $PGDATA right?
Sep 12 20:17:33 <brainicism>	its located at $PGDATA/recovery.conf
Sep 12 20:18:00 <RhodiumToad>	yeah
Sep 12 20:18:07 <brainicism>	postgres@MININT-KKCUCBK:~/10/main$ cat $PGDATA/recovery.conf
Sep 12 20:18:23 <RhodiumToad>	wait, what OS?
Sep 12 20:18:28 <brainicism>	ubuntu
Sep 12 20:18:35 <RhodiumToad>	and what is the actual value of $PGDATA?
Sep 12 20:19:02 <brainicism>	 /home/pg_data
Sep 12 20:19:11 <RhodiumToad>	the recovery.conf should be in /var/lib/postgresql/10/main  according to those logs
Sep 12 20:19:46 <brainicism>	which log are you looking at?
Sep 12 20:19:54 <RhodiumToad>	the one you just pasted above
Sep 12 20:19:57 <peerce>	are you running a hand compiled version of postgres or something?  /home/pg_data isn't any sort of normal debian/ubuntu data directory
Sep 12 20:20:06 <RhodiumToad>	Error: /usr/lib/postgresql/10/bin/pg_ctl /usr/lib/postgresql/10/bin/pg_ctl start -D /var/lib/postgresql/10/main -l /var/log/postgresql/postgresql-10-main.log -s -o  -c config_file="/etc/postgresql/10/main/postgresql.conf"  exited with status 1:
Sep 12 20:20:08 <brainicism>	i made the directory
Sep 12 20:20:11 <RhodiumToad>	see ^^ that?
Sep 12 20:20:29 <xocolatl>	postgres's $HOME is /usr/lib/postgresql
Sep 12 20:20:37 <brainicism>	oh oops
Sep 12 20:20:43 <RhodiumToad>	irrelevant
Sep 12 20:20:53 <xocolatl>	I mean /var/lib/postgresql
Sep 12 20:20:54 <peerce>	what counts is the -D /var/lib/postgresql/10/main
Sep 12 20:21:01 <RhodiumToad>	brainicism: you created the directory, but where did you actually restore the base backup to?
Sep 12 20:21:13 <brainicism>	 /var/lib/postgresql/10/main
Sep 12 20:21:33 <RhodiumToad>	ok. so does /var/lib/postgresql/10/main/recovery.conf  exist
Sep 12 20:21:41 <brainicism>	no, should it be there?
Sep 12 20:22:01 <brainicism>	i was under the impression it should be wherever $PGDATA pointed to
Sep 12 20:22:14 <peerce>	its wherever the postgres SERVER's $PGDATA is pointed to.
Sep 12 20:22:17 <peerce>	not your user account
Sep 12 20:22:31 <brainicism>	and thats at  /var/lib/postgresql/10/main/ ?
Sep 12 20:22:35 <RhodiumToad>	$PGDATA is only the default value for the -D option is. Explicitly specifying -D when starting the server overrides that.
Sep 12 20:22:39 <brainicism>	ah
Sep 12 20:22:43 <brainicism>	got it
Sep 12 20:23:20 <RhodiumToad>	also, if data_directory is specified in postgresql.conf, then that overrides even -D
Sep 12 20:23:53 <RhodiumToad>	when we talk about PGDATA, we mean whichever the final value of data_directory is
Sep 12 20:24:11 <brainicism>	it worked!
Sep 12 20:24:17 <brainicism>	thanks guys :D
Sep 12 20:24:40 <brainicism>	i didnt know about the -D
Sep 12 20:37:16 <regedit>	hello 
Sep 12 20:38:26 <regedit>	how do people usually do human name searching in with postgresql? closest match, a bit of fuzzy matching, ranked results based on how close the results match etc... or is this rarely done with postgresql?
Sep 12 20:40:16 <RhodiumToad>	probably not done any more or any less often than with other databases
Sep 12 20:40:48 <regedit>	okk
Sep 12 20:40:50 <RhodiumToad>	there are modules for soundex, metaphone, and trigram similarity
Sep 12 20:41:19 <xocolatl>	I wish soundex actually worked
Sep 12 20:41:24 <regedit>	ok lemme look those up
Sep 12 20:41:27 <RhodiumToad>	also levenshtein distance
Sep 13 07:15:49 *	Disconnected ()
**** ENDING LOGGING AT Fri Sep 13 07:15:49 2019

**** BEGIN LOGGING AT Fri Sep 13 07:16:13 2019

Sep 13 07:16:13 *	Now talking on #postgresql
Sep 13 07:16:13 *	Topic for #postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 13 07:16:13 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Thu Sep 12 16:46:28 2019)
Sep 13 07:16:13 *	Channel #postgresql url: https://www.postgresql.org
Sep 13 07:19:47 <RhodiumToad>	p321: what's the output of  select txid_current_snapshot();
Sep 13 07:20:54 <p321>	RhodiumToad, my Windows got rebooted to install Windows update. In the worst time possible... I am back now. Probably miss some of your posts. Can you please repost?
Sep 13 07:21:19 <RhodiumToad>	I just did
Sep 13 07:21:26 <RhodiumToad>	that's the only thing you missed
Sep 13 07:22:15 <p321>	RhodiumToad: Thanks. It returns: 1042361:1042361:
Sep 13 07:23:01 <RhodiumToad>	ok. and what about  show vacuum_defer_cleanup_age;
Sep 13 07:23:20 <p321>	RhodiumToad: 0
Sep 13 07:23:50 <RhodiumToad>	and if you do  vacuum verbose me.test8;  what is the output?
Sep 13 07:24:45 <p321>	RhodiumToad: I executed I am waiting. Probably gonna take some time. It looks to me 188 million rows are still there. I will post when it is finished.
Sep 13 07:24:59 <RhodiumToad>	using a paste site please
Sep 13 07:29:52 <p321>	RhodiumToad: https://pastebin.com/MBzk7R8L
Sep 13 07:30:37 <RhodiumToad>	ok, so it worked that time
Sep 13 07:30:52 <RhodiumToad>	most likely there was still some other transaction open when you tried it before
Sep 13 07:30:53 <p321>	RhodiumToad: Now repeating. SELECT n_dead_tup from pg_stat_user_tables where relname = 'test8'; and I get 0 which if fine.
Sep 13 07:31:54 <p321>	RhodiumToad: Thanks for help. Really appreciate it.
Sep 13 08:16:03 <maxter>	can postgres use more than 1 CPU for a query?
Sep 13 08:16:57 <incognito>	maxter: yes, indeed it depends on the query
Sep 13 08:17:22 <incognito>	see max_parallel_workers parameter
Sep 15 22:38:27 *	Disconnected ()
**** ENDING LOGGING AT Sun Sep 15 22:38:27 2019

**** BEGIN LOGGING AT Sun Sep 15 22:38:53 2019

Sep 15 22:38:53 *	Now talking on #postgresql
Sep 15 22:38:53 *	Topic for #postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 15 22:38:53 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Thu Sep 12 16:46:28 2019)
Sep 15 22:38:53 *	Channel #postgresql url: https://www.postgresql.org
Sep 15 22:39:29 <bencc>	xocolatl: I'm missing from clause but it seems to work
Sep 15 22:39:46 <bencc>	I usually try to understand the query before trying because if it works it might be misleading
Sep 16 00:19:18 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep 16 00:19:18 2019

**** BEGIN LOGGING AT Mon Sep 16 00:19:45 2019

Sep 16 00:19:45 *	Now talking on #postgresql
Sep 16 00:19:45 *	Topic for #postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 16 00:19:45 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Thu Sep 12 16:46:28 2019)
Sep 16 00:19:45 *	Channel #postgresql url: https://www.postgresql.org
Sep 16 00:57:17 <s3a>	Hello, everyone. :) I'm using Debian GNU/Linux, and when I try to run this PHP file ( http://dpaste.com/2TD8SQM#wrap ) (which tries to connect to a Postgresql database), I get the error "Fatal error: Uncaught Error: Call to undefined function pg_connect() in /home/user/NetBeansProjects/MyPhpProject/src/index.php:13 Stack trace: #0 {main} thrown in /home/user/NetBeansProjects/MyPhpProject/src/index.php on line 13". I have installed the php-pgsql,
Sep 16 00:57:17 <s3a>	php7.3-pgsql and php-mdb2-driver-pgsql packages. I have restarted the apache server (and I even rebooted, just in case). I also uncommented the extension=php_pdo_pgsql.dll and extension=php_pgsql.dll lines in the /etc/php/7.0/apache2/php.ini file (but it feels odd to me that they're dlls in a GNU/Linux environment). I also added the line "host all myusername 0.0.0.0/0 md5 #I added this line." in the /etc/postgresql/11/main/pg_hba.conf file (and the
Sep 16 00:57:18 <s3a>	total file is http://dpaste.com/0WJFJ1K#wrap ). So, could someone please help me get my PHP file to successfully connect to my Postgresql database?
Sep 16 09:05:08 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep 16 09:05:08 2019

**** BEGIN LOGGING AT Mon Sep 16 09:05:35 2019

Sep 16 09:05:35 *	Now talking on #postgresql
Sep 16 09:05:35 *	Topic for #postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 16 09:05:35 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Thu Sep 12 16:46:28 2019)
Sep 16 09:05:35 *	Channel #postgresql url: https://www.postgresql.org
Sep 16 09:05:37 <depesz>	based on the transaction status?
Sep 16 09:05:42 <RhodiumToad>	or anything
Sep 16 09:05:46 <depesz>	yeah. but I'm not sure you can.
Sep 16 09:05:48 <p321>	depesz: Excellent. Thanks a lot.
Sep 16 09:05:59 <RhodiumToad>	you can't as it stands
Sep 16 09:07:01 <RhodiumToad>	maybe we could add a %?x{*=somestring:!=otherstring:...}   construct
Sep 16 09:08:48 <depesz>	it would be enough if you could %`whatever-command %x`
Sep 16 09:08:53 <depesz>	but apparently you can't.
Sep 16 09:11:35 <dars>	`/usr/bin/pg_dumpall --file "/home/darshan/xyz" --host "<hostname here>" --port "5432" --username "paxcom" --password "passwordhere" "postgres" --verbose --role "paxcom" `
Sep 16 09:11:45 <dars>	Is this command right?
Sep 16 09:12:02 <dars>	I need to backup my database
Sep 16 09:13:54 <p321>	depesz: and RhodiumToad: I have not incorporated my existing prompt with red "*" it is: \set PROMPT1 '%n@%M:%>_%/=#%[%033[1;31m%]%x%[%033[0m%]% '      Thanks for help. Really appreciate it.
Sep 16 09:14:39 <dars>	Please help me wth above cmd
Sep 16 09:14:42 <RhodiumToad>	dars: no, you can't put the password on the command line
Sep 16 09:15:01 <dars>	RhodiumToad Then how to send password
Sep 16 09:15:12 <RhodiumToad>	you'll be prompted for it
Sep 16 09:15:18 <RhodiumToad>	possibly several times
Sep 16 09:15:37 <RhodiumToad>	you can put it in a PGPASSWORD environment variable or in a .pgpass file
Sep 16 09:16:03 <RhodiumToad>	you probably shouldn't use the --role parameter
Sep 16 09:16:35 <RhodiumToad>	the rest is ok, but it's probably better to use pg_dumpall -g and a separate pg_dump -Fc or -Fd for each database
Sep 16 09:27:20 <tangara>	 ID int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
Sep 16 09:27:31 <tangara>	i got error.  Please help
Sep 16 09:27:36 <RhodiumToad>	what error
Sep 16 09:27:42 <tangara>	even I use Integer also wrong
Sep 16 09:28:36 <tangara>	yntax error at or near "int"LINE 2:     ID int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,               ^SQL state: 42601
Sep 16 09:30:33 <RhodiumToad>	that sounds like the error is on the previous line?
Sep 16 09:31:08 <RhodiumToad>	what pg version?
Sep 16 09:38:39 <tangara>	11
Sep 16 09:39:20 <tangara>	it cannot be previous line cos this is the first definition of the field ...the first line is basically create table...
Sep 16 09:39:35 <tangara>	the rest of the fields are all ok so I do not know why it is giving me problem
Sep 16 09:39:57 <RhodiumToad>	what is the first line _exactly_
Sep 16 09:40:12 <tangara>	or should I try using psql ..so it is the same way to do the create table right ..just plonk the whole sql table onto psql
Sep 16 09:40:41 <tangara>	INSERT INTO public.oldmembers (    ID int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
Sep 16 09:40:55 <RhodiumToad>	that's not a CREATE TABLE
Sep 16 09:41:09 <tangara>	I am using INSERT because I have to create the table manually via PGAdmin 4
Sep 16 09:41:18 <RhodiumToad>	you can't do that
Sep 16 09:41:26 <tangara>	cos I can't use create table for strange reason
Sep 16 09:41:32 <RhodiumToad>	why not?
Sep 16 09:41:46 <RhodiumToad>	you certainly can't create tables with INSERT
Sep 16 09:41:50 <tangara>	cos the other day when I have an important demo I desparately rebuild my database table on the 11th hour
Sep 16 09:41:55 <tangara>	and it won't work
Sep 16 09:42:34 <tangara>	so I did it by using pgAdmin4 to manually create a table name first before inserting all the fields using a SQL sCRIPT
Sep 16 09:42:58 <RhodiumToad>	you're very confused
Sep 16 09:43:10 <RhodiumToad>	you cannot create tables or columns with INSERT
Sep 16 09:43:11 <tangara>	I still have that email with me
Sep 16 09:43:48 <tangara>	i am not sure what is wrong with the system cos I am a copy of the table in postgres and then in postgres I have a membership database
Sep 16 09:44:01 <RhodiumToad>	you cannot create tables or columns with INSERT
Sep 16 09:44:02 <tangara>	so somehow it won't allow me to enter the same table
Sep 16 09:44:13 <tangara>	i forgot how the same table was created
Sep 16 09:44:39 <tangara>	@RhodiumToad I merely enter the fields name
Sep 16 09:44:44 <tangara>	not create the Table
Sep 16 09:44:55 <tangara>	cos it won't allow me to create table direct
Sep 16 09:45:00 <RhodiumToad>	of course it will
Sep 16 09:45:50 <tangara>	that's the strange reason....and nobody is able to resolve the problem for me and I can't keep on spending all the hours whole day a few days try to crack this
Sep 16 09:46:42 <tangara>	anyway, now I just want to quickly insert all the table columns and be dond with it.
Sep 16 09:46:46 <RhodiumToad>	you cannot create tables or columns with INSERT
Sep 16 09:46:51 <tangara>	can I use that synaxt
Sep 16 09:46:54 <RhodiumToad>	no!
Sep 16 09:46:58 <RhodiumToad>	ffs
Sep 16 09:47:00 <tangara>	the table is created already RhodiumToad
Sep 16 09:47:07 <RhodiumToad>	with what columns?
Sep 16 09:47:20 <tangara>	now only left with inserting the columns name
Sep 16 09:47:32 <RhodiumToad>	you do not "insert" columns
Sep 16 09:47:40 <RhodiumToad>	what columns does the table have?
Sep 16 09:47:57 <tangara>	ok. so i have to use edit table and follows by add column name is that right ?
Sep 16 09:48:04 <RhodiumToad>	no
Sep 16 09:48:12 <tangara>	alter table
Sep 16 09:48:17 <RhodiumToad>	yes
Sep 16 09:48:52 <RhodiumToad>	alter table public.oldmembers add column id integer generated by default as identity primary key;   will work as long as the table doesn't already have a column of that name
Sep 16 09:51:14 <tangara>	tks. done.
Sep 16 09:51:47 <RhodiumToad>	now, why do you think that you can't use create table?
Sep 16 09:52:30 <tangara>	now  I have another problem - I have altered an old table from generated Id always to generated by Default but then it won't allow me to insert the id via web but another table havning the same setting has no such problem
Sep 16 09:53:31 <tangara>	to your question, I am not sure...cos I have the same table name in postgres which i have created via psql ...yeah...now I can really access psql
Sep 16 09:53:36 <RhodiumToad>	"won't allow" tells us nothing, what actually happened when you tried?
Sep 16 09:54:09 <tangara>	but maybe I didn't add in membership which is the database name...so it create another one in postgres which is the username
Sep 16 09:54:36 <tangara>	and somehow I cam not able to create another same name table in memberhsip
Sep 16 09:55:15 <tangara>	anyway, can you please advise me about the insertion thing cos no matter what I did, it just won't allow me to insert...
Sep 16 09:55:19 <RhodiumToad>	you're still telling us nothing
Sep 16 09:55:31 <nbjoerg>	tangara: precise error messages please
Sep 16 09:55:39 <RhodiumToad>	never say "won't allow" or "not able" - tell us WHAT HAPPENED WHEN YOU TRIED
Sep 16 09:55:57 <RhodiumToad>	give us the actual commands you used and the error message or other response
Sep 16 09:56:00 <tangara>	give me 15min cos I havn't start my IDE
Sep 16 09:59:05 <tangara>	sorry i need to ask this question first
Sep 16 09:59:08 <tangara>	>psql -h localhost -p 5433 -U postgres copy oldmembers FROM 'd:\memberparticulars.csv' (FORMASV, HEADER)
Sep 16 09:59:24 <tangara>	so now it gives me this error COPY doesnt exist
Sep 16 09:59:46 <tangara>	psql: FATAL:  database "copy" does not exist
Sep 16 09:59:56 <tangara>	i think i need to remove copy right ?
Sep 16 10:00:04 <incognito>	tangara: typo error, see psql --help
Sep 16 10:00:16 <incognito>	particularly -c option
Sep 16 10:00:36 <tangara>	incognito should I remove copy in order to copy my d:\.. to oldmembers ?
Sep 16 10:00:44 <RhodiumToad>	don't do it like that
Sep 16 10:00:55 <RhodiumToad>	you'll have endless pain from the shell quoting
Sep 16 10:01:53 <RhodiumToad>	if you're doing this manually and not from a script, then just do  psql -h localhost -p 5433 -U postgres -d yourdbname
Sep 16 10:02:17 <RhodiumToad>	then AT THE PSQL PROMPT, enter the \copy oldmembers FROM 'd:\memberparticulars.csv' (FORMAT CSV, HEADER)    command
Sep 16 10:05:36 <tangara>	ERROR:  null value in column "strremark" violates not-null constraint
Sep 16 10:05:50 <tangara>	how do I allow copying of null value ?
Sep 16 10:06:55 <RhodiumToad>	why did you create the column with NOT NULL if you want nulls in it?
Sep 16 10:07:19 <RhodiumToad>	alter table oldmembers alter column strremark drop not null;
Sep 16 10:07:43 <tangara>	erm... it is NOT Null
Sep 16 10:07:59 <tangara>	it has always been Not null...even the old database
Sep 16 10:08:05 <tangara>	using MYSQL
Sep 16 10:08:08 <tangara>	I think
Sep 16 10:08:30 <tangara>	erm...so i have to drop not null..I see
Sep 16 10:08:37 <tangara>	stupid me..ha ha
Sep 16 10:10:38 <tangara>	ERROR:  character with byte sequence 0x81 in encoding "WIN1252" has no equivalent in encoding "UTF8"
Sep 16 10:10:51 <tangara>	i have non-English characters inside...
Sep 16 10:11:04 <tangara>	how do I make it copy the characters as well ?
Sep 16 10:11:34 <tangara>	i think let me alter everything to drop not null first
Sep 16 10:12:16 <RhodiumToad>	if it was not null in the old database, why are there nulls in the file?
Sep 16 10:12:34 <RhodiumToad>	is the file in UTF8?
Sep 16 10:12:55 <RhodiumToad>	if so, make sure you do  set client_encoding = 'UTF8';  before the copy
Sep 16 10:16:25 <tangara>	yap, i just did.
Sep 16 10:16:40 <tangara>	but now i have another problem - missing data in a column
Sep 16 10:17:01 <tangara>	so is it possible to copy with missing data ?
Sep 16 10:17:12 <RhodiumToad>	where did this csv file come from? how was it created?
Sep 16 10:17:20 <tangara>	from my webhost
Sep 16 10:17:24 <RhodiumToad>	HOW
Sep 16 10:17:34 <tangara>	i just downloaded it into csv file
Sep 16 10:17:40 <tangara>	i also download one with sql format
Sep 16 10:17:59 <tangara>	it is in maria db
Sep 16 10:18:50 <RhodiumToad>	I have no idea if mariadb is capable of generating valid CSV files
Sep 16 10:18:59 <RhodiumToad>	what is the exact error you got?
Sep 16 10:19:15 <tangara>	it just mentioned a column name with no data
Sep 16 10:19:22 <RhodiumToad>	what is the exact error you got?
Sep 16 10:19:37 <tangara>	ERROR:  missing data for column "strnricno"CONTEXT:  COPY oldmembers, line 3073: ""
Sep 16 10:20:07 <RhodiumToad>	how many lines are in the file?
Sep 16 10:20:16 <tangara>	3000 plus
Sep 16 10:20:27 <RhodiumToad>	exactly how many lines are in the file
Sep 16 10:20:34 <RhodiumToad>	specifically, is line 3073 the last line
Sep 16 10:21:14 <tangara>	yap
Sep 16 10:21:26 <RhodiumToad>	and what column number is column "strnricno" ?
Sep 16 10:22:31 <tangara>	it is after id so 2nd column
Sep 16 10:23:30 <RhodiumToad>	is line 3073 empty or does it contain anything?
Sep 16 10:23:52 <tangara>	i think it is empty
Sep 16 10:24:04 <tangara>	let me double check
Sep 16 10:25:31 <tangara>	3072 is the last line
Sep 16 10:26:20 <RhodiumToad>	how does the last line end?
Sep 16 10:27:56 <tangara>	prettey much the same as other lines
Sep 16 10:28:13 <RhodiumToad>	what is the last character on the last line
Sep 16 10:28:25 <RhodiumToad>	learn to be PRECISE in answering questions
Sep 16 10:28:32 <tangara>	"
Sep 16 10:28:50 <tangara>	for every field with data it will be enclosed with " "
Sep 16 10:28:59 <tangara>	so the last column it is the same "xxxx"
Sep 16 10:29:10 <RhodiumToad>	does the sequence "" ever appear in the middle of a field in your data?
Sep 16 10:29:16 <RhodiumToad>	or just " on its own?
Sep 16 10:29:20 <tangara>	so do i write a script to remove all the empty "" ?
Sep 16 10:29:23 <RhodiumToad>	NO
Sep 16 10:29:26 <RhodiumToad>	ffs
Sep 16 10:29:29 <tangara>	it is always " xxx "
Sep 16 10:29:51 <tangara>	for every column including the last column at last row
Sep 16 10:30:12 <RhodiumToad>	are there any " characters that are NOT the start or end of a field
Sep 16 10:31:04 <tangara>	i am not sure...i need to use find ..hang on
Sep 16 10:33:07 <tangara>	there are a toal of 228 ""
Sep 16 10:33:16 <tangara>	as in no data inside
Sep 16 10:33:27 <RhodiumToad>	but I didn't ask that question, did I
Sep 16 10:33:39 <RhodiumToad>	are there any " characters that are NOT the start or end of a field
Sep 16 10:33:43 <tangara>	if you ask just how many alone
Sep 16 10:33:49 <tangara>	i am not sure how to find it
Sep 16 10:34:11 <tangara>	not at all
Sep 16 10:34:22 <tangara>	all have " at the start and end of a field
Sep 16 10:34:36 <RhodiumToad>	are there any " characters that are NOT the start or end of a field
Sep 16 10:36:25 <tangara>	not at all
Sep 16 10:36:34 <tangara>	i think
Sep 16 10:36:44 <tangara>	how do I find out ?
Sep 16 10:36:54 <tangara>	i have to import into Excel ?
Sep 16 10:36:58 <tangara>	to see ?
Sep 16 10:36:59 <RhodiumToad>	ghod, no
Sep 16 10:37:00 <tangara>	or ?
Sep 16 10:37:09 <RhodiumToad>	excel's csv handling is hopeless
Sep 16 10:37:34 <tangara>	anyway my ms office is gone after my computer crash
Sep 16 10:37:40 <tangara>	so i only have open source one
Sep 16 10:38:27 <RhodiumToad>	make sure that there isn't a stray blank line at the end of the file.
Sep 16 10:39:08 <tangara>	nope
Sep 16 10:39:09 <RhodiumToad>	i.e. the newline (CR or CRLF) following the " on line 3072 should be the last one or two characters in the file
Sep 16 10:39:48 <RhodiumToad>	if there's an extra newline, then this is exactly the error that you would get.
Sep 16 10:41:18 <tangara>	ok. now what can i do to copy the empty data field ?
Sep 16 10:44:14 <tangara>	or do I need to use PYTHON to do it ?
Sep 16 10:44:32 <dars>	How to get count of all  the tables of my database?
Sep 16 10:46:49 <depesz>	dars: select count(*) from pg_class where relkind = 'r'; ?
Sep 16 10:47:01 <tangara>	i think i know the reason already
Sep 16 10:47:30 <tangara>	cos there is indeed missing data before that missing data in strnricno
Sep 16 10:47:47 <tangara>	i think i need to put in all the null value again before copying
Sep 16 10:48:05 <dars>	Thank you depesz
Sep 16 10:49:14 <dars>	depesz It is returning count = 0
Sep 16 10:53:18 <tangara>	ERROR:  extra data after last expected column
Sep 16 10:53:38 <tangara>	it seems it is impossible for postgresql to copy data from csv file ...
Sep 16 10:53:52 <tangara>	i thought I will manually add in the missing column
Sep 16 10:54:04 <tangara>	but when i added it then give me the above error
Sep 16 10:54:15 <tangara>	so it is no ending of errors :(
Sep 16 10:56:56 <Myon>	dars: that can't be true, it should return at least 70-ish catalog tables
Sep 16 10:58:16 <localhorse>	when writing it like this, is NULL used as default value? `ALTER TABLE foobar ADD COLUMN baz TEXT NULL;` why doesn't it require the DEFAULT keyword?
Sep 16 11:00:24 <depesz>	dars: i find it unlikely
Sep 16 11:00:35 <depesz>	dars: show me screenshot of what you did.
Sep 16 11:02:26 <Myon>	localhorse: NULL is just the opposite of a NOT NULL constraint
Sep 16 11:02:26 <depesz>	localhorse: null/not null specifies whether null value is allowed.
Sep 16 11:02:30 <depesz>	localhorse: also, it'
Sep 16 11:02:42 <depesz>	s the default. so you don'
Sep 16 11:02:47 <depesz>	t have to specify null
Sep 16 11:02:53 <depesz>	how come i press enter too soon?
Sep 16 11:03:14 <dars>	depesz: https://pasteboard.co/IxEopNn.png
Sep 16 11:03:15 <Myon>	Gremlins
Sep 16 11:03:19 <localhorse>	ah right
Sep 16 11:03:28 <Myon>	dars: 'r'
Sep 16 11:03:32 <localhorse>	i never write NULL, that's why it looked weird to me
Sep 16 11:03:37 <depesz>	dars: why the hell did you put kinator whatever in the relkind ?
Sep 16 11:03:46 <depesz>	dars: i typed: relkind = 'r'
Sep 16 11:04:29 <depesz>	relkind is a kind (Type of) relation. 'r' means 'regular table'
Sep 16 11:04:57 <dars>	I need to find the number of tables in my db
Sep 16 11:05:04 <depesz>	then run the query i gave you
Sep 16 11:05:59 <tangara>	anybody here can tell me what should i do to import my database :(
Sep 16 11:06:28 <RhodiumToad>	we tried but you're not cooperating
Sep 16 11:07:30 <dars>	Yeah it is working
Sep 16 11:07:35 <dars>	Thank you
Sep 16 11:08:10 <dars>	Now I want to know is their any way t pause pg_dump and continue later?
Sep 16 11:08:27 <depesz>	darsi dont think so.
Sep 16 11:08:34 <depesz>	dars: what is the real problem you're trying to solve?
Sep 16 11:09:01 <dars>	I am creating bkup of a 840GB size database right now
Sep 16 11:09:17 <depesz>	ok, and ?
Sep 16 11:10:04 <dars>	How much time pg_dump_all is going to take for that?
Sep 16 11:10:09 <depesz>	long
Sep 16 11:10:28 <dars>	how long? approx?
Sep 16 11:10:32 <depesz>	couple of notes: 1. do not use pg_dumpall. 2. when using pg_dump, use -Fd and -j
Sep 16 11:10:40 <depesz>	can't approximate, i have no idea on speed of your system.
Sep 16 11:11:07 <dars>	i3, with 8GB RAM
Sep 16 11:11:47 <depesz>	the most imporatant factor is speed of hard disk. also. 850GB database on 8gb server? damn. even my laptop has more memory
Sep 16 11:12:17 <dars>	that is config of my system not server
Sep 16 11:12:23 <dars>	server is amazon rds
Sep 16 11:13:37 <depesz>	still. there is no real way to estimate it, for me. you can dump a table that is ~ 2G, and multiply the time by 420.
Sep 16 11:16:21 <eofs>	dars: Use RDS's Snapshots to backup things. If you need to access the data as well then you might consider using https://aws.amazon.com/dms/
Sep 16 11:18:01 <tangara>	meaning RhodiumToad
Sep 16 11:18:57 <tangara>	i already answered your last question that there isn't a line or black line at the end
Sep 16 11:19:25 <tangara>	and after that there is no reply from you
Sep 16 11:19:37 <tangara>	and so I thought you don't have the answer or solution
Sep 16 11:19:42 <Myon>	tangara: the problem is that you aren't answering questions in a useful way
Sep 16 11:20:26 <tangara>	but there isn't a black line and how should I answer if you can't accept this answer ?
Sep 16 11:20:27 <Myon>	and your problem seems to be shifting around a lot
Sep 16 11:20:53 <tangara>	yes. at first. but after that it is on that question only
Sep 16 11:21:04 <tangara>	and all the way on that question only
Sep 16 11:21:08 <Myon>	so what is the precise problem now?
Sep 16 11:21:20 <tangara>	so, based on my last reply, what should i do next ?
Sep 16 11:21:31 <Myon>	ERROR:  extra data after last expected column
Sep 16 11:21:42 <RhodiumToad>	bah. power issue here. I'll be back in a bit.
Sep 16 11:21:48 <Myon>	how many columns does the table have?
Sep 16 11:21:58 <tangara>	i waited for so long and there is no continuation so naturally i thought RhodiumToad is also trying to think what could be the way to solve it
Sep 16 11:22:17 <depesz>	tangara: please answer Myons question
Sep 16 11:22:28 <Myon>	RhodiumToad is really the most patient and helpful person in here
Sep 16 11:22:38 <tangara>	oh no. we should continue at the point which Rhodium Toad asked me tht question and then I answered it from there
Sep 16 11:22:51 <Myon>	if you manage to upset him, that means you are doing *much* worse than anyone else in here seeking advise
Sep 16 11:22:53 <tangara>	yes. I agreed. So are many of othere people hre and out there
Sep 16 11:23:18 <depesz>	tangara: do you want help, or do you want to complain about not getting help and/or RhodiumToad ?
Sep 16 11:23:22 <Myon>	tangara: so. I asked you something, and all I get in return is unhelpful babbling
Sep 16 11:23:24 <tangara>	that's why i will / must keep him and others who have helped me in my buddhism prayers
Sep 16 11:23:41 <depesz>	if you want help, please answer question from myon. if you want to complain - well, continue. but please be advised that it's not going to help you in the long run.
Sep 16 11:24:18 <tangara>	@depesz you are not being helpful.  We must continue where Rhodium last question and my reply from there.
Sep 16 11:24:30 <depesz>	tangara: ok. have a nice life.
Sep 16 11:24:34 <tangara>	not after what I have said because there is no link
Sep 16 11:24:42 <Myon>	I'll stop here, sorry
Sep 16 11:25:32 <tangara>	cos the whole thing or the discussion started with me answering all the questions posted by Rhodium Toad till there is no further reply and then of course i tried things out myself right
Sep 16 11:25:44 <tangara>	so everything after that doesn't count
Sep 16 11:25:55 <tangara>	it has to continue from the last line I replied him...
Sep 16 11:26:17 <tangara>	because I do not know what is it he wanted me to do next to resolve this copying problem
Sep 16 11:26:54 <adsf>	would a window function sound about right if im after an average count of rows by a group by?
Sep 16 11:27:34 <incognito>	depesz: me, i want to complain : we don't get the right question in order to give the right answer ^^
Sep 16 11:27:49 <Myon>	adsf: you can combine group by and window functions, but likely you just want select avg(foo) from bar group by moo;
Sep 16 11:27:59 <depesz>	adsf: do you just want the average count?
Sep 16 11:28:16 <depesz>	adsf: if yes, then: select avg(count) from (Select count(*) from table group by whatever) x;
Sep 16 11:28:22 <tangara>	not sure why am i getting attacks like that
Sep 16 11:28:28 <adsf>	i have an aggregate mat view where i need the avg count of rows by a group
Sep 16 11:28:43 <depesz>	adsf: select avg(count_column) from your_matview;
Sep 16 11:28:53 <tangara>	maybe people don't want me to be here to ask question that is difficult to get an answer or No answer for my problem
Sep 16 11:28:57 <adsf>	oh i mean this is the query making the mat view :)
Sep 16 11:29:08 <Myon>	tangara: you are talking so much it's hard to see what your problem is, so keep it to the facts
Sep 16 11:29:09 <tangara>	i think there is no solutions hence the ATTACKS !!!
Sep 16 11:29:19 <depesz>	adsf: sorry, not sure what you mean.
Sep 16 11:29:20 <adsf>	or are you saying just do count, then just use avg on it
Sep 16 11:29:38 <adsf>	depesz: your first answer makes sense to me. with the sub query
Sep 16 11:30:00 <tangara>	@Myon I have stated there was no reply since my last reply to Rhodium Toad hence I went on to try things out.
Sep 16 11:30:21 <tangara>	and then I was told that I was not being Cooperative....
Sep 16 11:30:28 <tangara>	why has it ended like that ?
Sep 16 11:30:34 <adsf>	Myon: i think the issue with that is its not a numeric value, its an id.
Sep 16 11:30:38 <Myon>	now cooperate in waiting for him please
Sep 16 11:31:14 <tangara>	i think I have to be blunt : Even though Rhodiim Toad is VERY FRIENDLY and HELPFUL.  This problem of mine is nothing BUT DIFFICULT
Sep 16 11:31:35 <tangara>	@Myon I can accept  a reply like yours now
Sep 16 11:31:53 <tangara>	BUT I THINK I am not being Uncorporative !
Sep 16 11:32:00 <tangara>	I am putting the FACTS right.
Sep 16 11:32:06 <incognito>	<tangara> why has it ended like that ? <= because someone gave you the answer and you didn't try it, instead you went to another workaround not working
Sep 16 11:32:22 <tangara>	that Rhodium Toad also don't have an answer which is fine cos nobody in this world knows everything
Sep 16 11:32:28 <tangara>	and he is good in what he knows
Sep 16 11:32:38 <tangara>	maybe my problem is too BEGINNER
Sep 16 11:32:55 <tangara>	And he only deals with EXPERIENCED developer problem
Sep 16 11:33:11 <tangara>	so Beginner one he just don't have an answer which is fine
Sep 16 11:33:25 <tangara>	but don't say that I am being Uncoporative
Sep 16 11:34:08 <tangara>	ok. Today is just too much. I am tired.  I have so many problems to deal with and struggling on my own because that's what most people want that happening to me.. So many tragic in my life already.
Sep 16 11:34:20 <incognito>	tangara: to be cooperative : you only need to check if the answer resolve your problem; and give us log if not
Sep 16 11:34:47 <adsf>	hrmm is it risky to make a mat view from a mat view?
Sep 16 11:34:50 <tangara>	people just wanted me not to be successful in this developer things and jobs and so everywhere people are being so HOSTILE
Sep 16 11:34:56 <tangara>	Too many tragic things already.
Sep 16 11:34:59 <tangara>	bye.
Sep 16 11:35:19 <RhodiumToad>	yeesh
Sep 16 11:35:27 <adsf>	well that was melodramatic
Sep 16 11:35:27 <cartan>	is it plugged in now?
Sep 16 11:35:57 <Myon>	RhodiumToad: http://paste.debian.net/hidden/4d8c688e/ (but maybe better ignore it)
Sep 16 12:21:27 <Moonsilence>	Hi! Is there a way to identify the associated session pid of a given temporary table?
Sep 16 12:31:03 <depesz>	Moonsilence: afaik no.
Sep 16 12:31:50 <Moonsilence>	Thanks depesz! Havent seen you around here lately :)
Sep 16 12:32:07 <depesz>	i'm virtually always here, not always talking, though :)
Sep 16 12:32:49 <Moonsilence>	then I meant 'talking'
Sep 16 12:33:09 <depesz>	:). and i'm off again. bbl.
Sep 16 13:07:23 <bikeshedder>	Does anyone know a good way to implement a sort function that sorts strings like that: ['1', '1.2.5', '1.3', '2.20', '20.1']. Maybe there is some builtin collation which supports that? I expect this to be a quite common requirement so I wonder if I just overlooked it or if it really requires a custom key function.
Sep 16 13:09:18 <bikeshedder>	Aw. That example was terrible. I just realized that the alphanumeric sort already does that. I should have added a '3' and '20' in the list. e.g.: ['1', '1.2.5', '1.3', '2.20', '3', '20.1']. If I sort that alphanumerical I'll get '20.1' before '3'. I know how to solve that in code but I wonder how this would be applied to a postgresql index
Sep 16 13:10:41 <mbecroft>	I have a PL/pgSQL function that returns type text. Internally, it has a dynamic "EXECUTE ... INTO" a record variable. I then return a field of the record, casted to the correct return type as follows: "RETURN rec.val::text;"
Sep 16 13:12:40 <mbecroft>	Unfortunately, the type of the "rec" variable is determined and cached the first time the function is called, so on subsequent invocations when the EXECUTE results in a different type, it fails with "ERROR:  type of parameter ... does not match that when preparing the plan ... CONTEXT:  ... at RETURN"
Sep 16 13:13:35 <mbecroft>	Per following reference, this can be worked around using EXECUTE (see para starting with "The mutable nature of record variables...":
Sep 16 13:13:38 <mbecroft>	https://www.postgresql.org/docs/9.6/plpgsql-implementation.html
Sep 16 13:14:31 <mbecroft>	But since I can't EXECUTE the actual RETURN statement itself, the advice given there (use EXECUTE) doesn't seem to help. Any ideas?
Sep 16 13:15:16 <mbecroft>	The error message indicates it is in the RETURN statement itself that the type mismatch occurs
Sep 16 13:15:44 <mbecroft>	As far as I know there is no way to dynamically execute the return statement itself!
Sep 16 13:16:03 <mbecroft>	Or more to the point, its expression argument
Sep 16 13:16:11 <mbecroft>	Help!?
Sep 16 13:18:19 <saper>	how different are different types of "rec"?
Sep 16 13:20:11 <mbecroft>	saper: They might be anything at all, but currently I am testing with text and numeric
Sep 16 13:21:22 <mbecroft>	It appears that "EXECUTE ... INTO" a record variable works OK as no error is thrown at that point - the record variable's type is changing dynamically at each function invocation
Sep 16 13:21:38 <mbecroft>	It complains when I try to RETURN a column of the record...
Sep 16 13:21:51 <harks>	bikeshedder: I'm not entirely sure, if I understand your requirements correctly, but I think you just want to sort by regexp_split_to_array(version_number, '\.')::bigint[]
Sep 16 13:27:28 <mbecroft>	saper: I am just writing a minimal test case to demonstrate the issue
Sep 16 13:30:48 <saper>	mbecroft: would casting it to text in the dynamic query work?
Sep 16 13:31:08 <DuckyDev>	Hi guys, I've this simple query ( https://pastebin.com/K23e18cW  ) where a.aid and ar.aid are returned, but is there any easy way to return everything from the two tables EXCEPT ar.aid?
Sep 16 13:31:38 <mbecroft>	saper: good point. Looks like I am not - this may fix it!
Sep 16 13:32:07 <bikeshedder>	harks, I think the sorting I'm looking for is called "numerical sorting". The most prominent example that comes to mind is windows filename sorting. "10.png" comes after "2.png" And "10-10.png" comes after "10-2.png".
Sep 16 13:32:53 <mbecroft>	saper: Yay! Obvious in retrospect...
Sep 16 13:33:00 <mbecroft>	All working now :D
Sep 16 13:33:18 <mbecroft>	Thank you very much for pointing out the obvious
Sep 16 13:33:51 <saper>	mbecroft: I think your function is too generic to work well, but here it is
Sep 16 13:34:48 <mbecroft>	saper: It is a little ungainly, but necessary and the least bad way to achieve what's required...
Sep 16 13:35:26 <mbecroft>	I had what seemed like a separate issue as well, but it may have just been a different manifestation of this. Will see in a moment...
Sep 16 13:36:17 <mbecroft>	No...all fixed :D
Sep 16 13:36:22 <mbecroft>	Whew!
Sep 16 13:36:55 <mbecroft>	Thank you again saper! Sometimes the obvious needs to be pointed out.
Sep 16 13:38:13 <saper>	Another happy customer^G
Sep 16 13:38:50 <harks>	bikeshedder Never heard of it. Doesn't seem very useful to me. Maybe it's just regexp_split_to_array(version_number, '\D+')::bigint[] ? Or should strings be sorted as well? If you understand your semantics, just build an immutable function.
Sep 16 13:38:58 <bikeshedder>	harks, https://imgur.com/501SymI ... that's how lots of users expect sorting to work.
Sep 16 15:45:11 *	Disconnected ()
**** ENDING LOGGING AT Mon Sep 16 15:45:11 2019

**** BEGIN LOGGING AT Mon Sep 16 15:45:39 2019

Sep 16 15:45:39 *	Now talking on #postgresql
Sep 16 15:45:39 *	Topic for #postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 16 15:45:39 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Thu Sep 12 16:46:28 2019)
Sep 16 15:45:40 *	Channel #postgresql url: https://www.postgresql.org
Sep 16 15:46:33 <Myon>	it'll Just Work, yes
Sep 16 15:46:41 <mia>	thank you!
Sep 16 16:11:14 <slax0r>	hello, is it possible to copy database data from one database to the other? I'm trying with FDW, but it doesn't allow me to add a localhost host, so I need to add the external IP and it's slow as hell.. I thought of `with template` but I'm copying a production database to a test database, so I can't really disconnect clients, dump and restore from the server is not possible, since the DB is running as
Sep 16 16:11:20 <slax0r>	an Azure service
Sep 16 16:11:35 <slax0r>	is there any other way to copy and not having it go REALLY slowly?
Sep 16 16:12:28 <Myon>	pg_dump -h db1 | psql -h db2
Sep 16 16:13:08 <Myon>	you can parallelize both parts, but then you need to store the result on disk (pg_dump -h db1 -Fd -j; pg_restore -h db2 -j)
Sep 16 16:13:21 <krychu>	How can I insert a new row only if the last recent one is older than X? The requirement is that this works concurrently. I think I cannot achieve it with SELECT FOR UPDATE, and INSERT ON CONFLICT. The only way I can think of is to use transaction with table lock, but perhaps there is a better way.
Sep 16 16:14:16 <slax0r>	host is the same, so I'm guessing `pg_dump -h db -d dbname1 | psql -h db -d dbname2` along with auth stuff?
Sep 16 16:14:30 <Myon>	yes
Sep 16 16:14:54 <mobidrop>	you can only do it with locking because there's time between getting the "last recent one" and inserting a new one
Sep 16 16:15:00 <slax0r>	I could try this, but doubt it will be any quicker since I'm connected over vpn
Sep 16 16:15:01 <Myon>	krychu: SERIALIZABLE isolation should be able to do that
Sep 16 16:15:05 <chris64>	hello
Sep 16 16:16:03 <krychu>	Myon checking this out now
Sep 16 16:16:23 <slax0r>	oh, pg_dump | psql will flip out on conflicts, right? :/
Sep 16 16:16:53 <Myon>	it will ignore errors by default
Sep 16 16:17:52 <slax0r>	ok, thanks for your help!
Sep 16 16:19:58 <krychu>	Myon serializable 8-) is the thing, awesome stuff
Sep 16 16:19:59 <krychu>	thnx
Sep 16 16:20:05 <Myon>	aye
Sep 16 16:20:16 <Bish>	if you were about to design a datastructure-model where every access, even read access should be monitored(and by whom it has been accessed), how would you do it?
Sep 16 16:20:38 <Bish>	normal tables + audit logs (would you create a dbuser for every real user? to be able to use that in audit logs)
Sep 16 16:20:47 <Bish>	event sourcing? it would be really happy about opinions
Sep 16 16:22:07 <doev>	Can I reset a whole database server? I installed a fresh one, but reading in some dumbs going wrong. Can I start with a new Server, without reinstall?
Sep 16 16:22:27 <Myon>	doev: you can just call initdb again (rm -rf first)
Sep 16 16:23:14 <doev>	Myon thank you
Sep 16 16:27:45 <doev>	with rm -rf ... you mean, the old datadir? /var/lib/postgresql/10/main in my case
Sep 16 16:28:24 <adsf>	should i be concerned about locks on tables that can have fairly high volume of upserts concurrently? there are no for updates or any selects going on for that table.
Sep 16 16:28:24 <Myon>	on Debian, do pg_dropcluster 10 main; pg_createcluster 10 main
Sep 16 16:28:25 <ilmari>	doev: if you're using debian/ubuntu, use pg_dropcluster and pg_createcluster
Sep 16 16:29:03 <Myon>	adsf: locks are normal, especially ACCESS SHARE locks
Sep 16 16:29:25 <Myon>	that just means "please no one drop this table while I'm using it"
Sep 16 16:29:28 <adsf>	Myon: okie dokie. It doesn't seem like it will be a huge issue.
Sep 16 16:30:05 <adsf>	i have a few processes that write to the same table and usually they don't have to wait long if they overlap on a row
Sep 16 16:30:44 <Myon>	that's why you should keep transactions short
Sep 16 16:31:07 <ilmari>	https://www.postgresql.org/docs/current/explicit-locking.html shows the different lock levels, what takes them, and how they block eachother
Sep 16 16:31:52 <adsf>	is there any locking for updaing a child table? (locking parent)
Sep 16 16:32:14 <adsf>	ilmari: thanks, I had that up also :)
Sep 16 16:33:30 <krychu>	Myon if a transaction is serializable is it guaranteed to behave as serial with other read committed transactions or only with other serializable transactions?
Sep 16 16:35:40 <Myon>	the guarantee is for the transaction, if other transactions care less, they can see different results
Sep 16 16:36:50 <Myon>	in your case, it won't prevent non-serializable transactions from violating this requirement
Sep 16 16:37:25 <Myon>	you could probably install a trigger that enforces it by bailing out if not in serialzable mode
Sep 16 16:37:44 <Myon>	but tbh I don't have much experience with that in practise
Sep 16 16:37:56 <krychu>	ok, gonna give it a shot
Sep 16 16:38:01 <doev>	ok, reset works fine. Next time I remember, that the config-files are reseted too :)
Sep 16 16:38:05 <krychu>	need to run will chat later, thanks again
Sep 16 17:04:51 <enoq>	when do you think about scaling postgres
Sep 16 17:05:30 <enoq>	we've got a web shop that fetches some store information from a server that is hooked up to a postgres db
Sep 17 10:23:00 *	Disconnected ()
**** ENDING LOGGING AT Tue Sep 17 10:23:00 2019

**** BEGIN LOGGING AT Tue Sep 17 10:23:27 2019

Sep 17 10:23:27 *	Now talking on #postgresql
Sep 17 10:23:27 *	Topic for #postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 17 10:23:27 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Thu Sep 12 16:46:28 2019)
Sep 17 10:23:27 *	Channel #postgresql url: https://www.postgresql.org
Sep 17 10:28:43 <StucKman>	Berge: well, I didn't really did any measures :(
Sep 17 10:31:35 <Berge>	StucKman: That's the only way to make sure what you're doing is helping
Sep 17 10:34:16 <Myon>	afaict SSL doesn't compress at all?
Sep 17 10:35:02 <StucKman>	noy by default, but the only way to make pg compress by itself is enabling ssl
Sep 17 10:36:07 <StucKman>	I just noticed that I can't control the compression level either
Sep 17 10:36:27 <StucKman>	so I'll just stick to ssh tunnel, it's easier to setip
Sep 17 10:36:51 <Berge>	Myon: Not since CRIME
Sep 17 10:36:56 <Berge>	Or shouldn't, at least.
Sep 17 10:37:16 <Myon>	"OpenSSL 1.1.0 disables compression by default"
Sep 17 10:37:30 <Myon>	https://www.postgresql.org/docs/current/libpq-connect.html
Sep 17 10:37:59 <Berge>	StucKman: Remember to measure it!
Sep 17 10:38:07 <Myon>	does openvpn still do compression?
Sep 17 10:38:45 <Berge>	It's off by default, afaik
Sep 17 10:38:54 <Berge>	But I think maybe you can still turn it on
Sep 17 10:39:11 <Myon>	ssh is likely the best/easiest option anyway
Sep 17 10:39:22 <Berge>	But OpenVPN had a CRIME-related vulnerability a few years back.
Sep 17 10:39:25 <Berge>	yeah
Sep 17 10:39:42 <Berge>	It'd be nice with a tool that's as easy to use as ssh that'd do zstd or zstd-adapt over the network, though.
Sep 17 10:39:59 <Berge>	Perhaps you can mangle socat into doing it.
Sep 17 10:39:59 <ioguix>	but keeping a ssh tunnel up is not very conveniant
Sep 17 10:40:14 <Berge>	ioguix: autossh
Sep 17 10:40:59 <Myon>	fwiw what I recently started using is LocalForward 7632 /var/run/postgresql/.s.PGSQL.5432
Sep 17 10:41:10 <Myon>	forwards a local TCP port to a remote PostgreSQL UNIX socket
Sep 17 10:41:33 <ioguix>	Berge: thanks, I wasn't aware of this program
Sep 17 10:41:46 <Myon>	(TCP because ssh is bad at removing the UNIX socket after closing, and fails on the next connect)
Sep 17 10:41:48 <Berge>	Myon: As in ssh -L?
Sep 17 10:41:52 <Berge>	I didn't know it could to unix sockets, that's neat
Sep 17 10:42:09 <Myon>	-L would be the cli version, yes
Sep 17 10:42:10 <StucKman>	awesome
Sep 17 10:42:13 <ioguix>	I will compare it with our home grew perl daemon we use for supervision to multiplex the ssh connexions
Sep 17 10:42:21 <Berge>	I use -L and -R all the time
Sep 17 10:43:13 <Berge>	I've actually considered doing something akin to what StucKman wants to do for a postgres client program that grabs huge result sets, but when I measured (using ssh with compression), the difference wasn't all that great
Sep 17 10:43:53 <Berge>	Parsing and processing time in the client is much greater than the network transfer speed anyway, in our case.
Sep 17 10:43:58 <Berge>	The gains are very, very modest in practice.
Sep 17 10:44:11 <ioguix>	compression without adding the crypto thing would be ideal
Sep 17 10:44:25 <Berge>	Why?
Sep 17 10:44:28 <Berge>	For CPU reasons?
Sep 17 10:44:57 <ioguix>	crypto is already handled by the pgsql proto
Sep 17 10:45:07 <ioguix>	if needed
Sep 17 10:45:48 <Myon>	yeah that seems orthogonal and useful
Sep 17 10:46:12 <StucKman>	Berge: good point, I simpy see the networkd capping while fetching w/o compression
Sep 17 10:46:33 <StucKman>	capping=saturing
Sep 17 10:46:34 <Myon>	.oO(some users might actually benefit from compressed queries... WHERE id IN (..., ...) )
Sep 17 10:47:15 <ioguix>	Myon: really? on a LAN?
Sep 17 10:47:29 <ioguix>	how huge would be your IN() list??
Sep 17 10:47:46 <Myon>	I've seen megabytes (but that suggestion wasn't entirely serious)
Sep 17 10:48:22 <capitol>	badly designed hibernate queries use huge in lists
Sep 17 10:48:52 <Berge>	StucKman: What's the netowrk speed?
Sep 17 10:49:13 <DarkUranium>	Berge, I think LZ4 might also be a good choice (apart from zstd).
Sep 17 10:50:09 <Berge>	DarkUranium: yeah, likely. zstd has done better than lz4 in my tests, but it'll of course depend on datasets and CPU (features)
Sep 17 10:50:26 <Berge>	And of course what you want to optimise for.
Sep 17 10:50:54 <DarkUranium>	Yeah. I'm thinking LZ4 for compression speed/CPU use, zstd for compression ratio.
Sep 17 10:51:06 <DarkUranium>	(as a rule of thumb, ofc)
Sep 17 10:51:18 <Berge>	Well, to achieve the same compression level as zstd, lz4 needed quite a bit more CPU time in my tests
Sep 17 10:51:32 <Berge>	As always, a speed-compression tradeoff
Sep 17 10:51:53 <DarkUranium>	To be fair, LZ4 is very much decompression-speed optimized, not compression-speed.
Sep 17 10:52:08 <DarkUranium>	But I didn't mean at the same compression level anyways.
Sep 17 10:52:24 <Berge>	The zstd training mode is also cool
Sep 17 10:53:03 <DarkUranium>	I have been toying with the idea of compressing some of my larger columns using either zstd or LZ4, with a predetermined table.
Sep 17 10:53:19 <DarkUranium>	(they're 30kB on average ... not huge, but when you got thousands, it adds up)
Sep 17 10:53:27 <Berge>	You can pre-train a dictionary for zstd on data sets similar to what you're going to compress.
Sep 17 10:53:33 <DarkUranium>	Exactly.
Sep 17 10:53:49 <Berge>	Which helps very much for small datasets.
Sep 17 10:54:24 <chris64>	Berge: does it only speed up the compression or does it also improve the compression ratio?
Sep 17 10:54:39 <Berge>	chris64: Both, to the same degree
Sep 17 10:54:43 <Berge>	ish
Sep 17 10:54:45 <chris64>	cool
Sep 17 10:54:54 <DarkUranium>	Berge, I'd argue it helps even more for what I'm considering, which is small data *items* (per-column compression)
Sep 17 10:55:01 <Berge>	DarkUranium: yep
Sep 17 10:55:52 <DarkUranium>	Anyhow, the reason I was erring towards LZ4 here was to reduce CPU use, and because it's simpler (more portable).
Sep 17 10:56:49 <nickb>	Hi. We had a "weird" crash yesterday: https://dpaste.de/4oRH/raw that resulted in replica losing a WAL segment (or part of it). This is the second time this has happened in the last few months, when primary removes a WAL-segment that wasn't actually shipped. Unfortunately I don't have WAL from primary, but I do have WAL from replica. Pg 10.9
Sep 17 10:57:48 <DarkUranium>	Berge, I don't have zstd or LZ4 handy here, but interestingly, zip compression only gives me a 1.07 ratio.
Sep 17 10:57:57 <DarkUranium>	rar gives me ~2.0
Sep 17 10:58:06 <Berge>	apt install zstd
Sep 17 10:58:06 <DarkUranium>	(DEFLATE vs whatever RAR uses ... LZMA?)
Sep 17 10:58:17 <DarkUranium>	In Windows ATM.
Sep 17 11:00:20 <Myon>	nickb: filesystem/kernel bug?
Sep 17 11:00:45 <Myon>	nickb: or... what's "0_wal"?
Sep 17 11:01:40 <Myon>	nickb: do you still have logs from last time?
Sep 17 11:03:46 <nickb>	Myon: ugh. EXT4 journal is clean, kernel bug is possible; No idea what 0_wal is; Logs from previous time are gone as we migrated to new hardware since then, and logs are now gone unfortunately
Sep 17 11:04:08 <Myon>	oh completely new hardware?
Sep 17 11:04:15 <Myon>	could be RAM corruption
Sep 17 11:04:35 <Myon>	is that segfault a bug?
Sep 17 11:04:38 <adsf>	if i need to update 1 table from another table, and i don't care about update on conflict, what is the default handling of insert if it finds a dupe primary key?
Sep 17 11:05:03 <Myon>	by default it raises an exception
Sep 17 11:05:11 <adsf>	ahh, will it just stop the operation?
Sep 17 11:05:15 <Myon>	yes
Sep 17 11:05:18 <f3f3lix>	we have ecc
Sep 17 11:05:22 <adsf>	guess im going down the on conflict update :p
Sep 17 11:05:31 <f3f3lix>	if it was ram, it was a multibit ecc where the parity checks out
Sep 17 11:05:45 <nickb>	Myon: we still have to investigate that segfault; Might be a bug, but it points to libc.so. How would RAM corruption lead to WAL file going missing? Also, what f3f3lix said (ECC)
Sep 17 11:08:39 <Myon>	nickb: note that it tried to move the file to 0_wal/something, of course that fails
Sep 17 11:09:25 <f3f3lix>	yes. we noted that. isn't that very odd
Sep 17 11:09:29 <Myon>	it's suspicious that "pull" and postgres were corrupted at the same time
Sep 17 11:09:49 <Myon>	or 2 min apart
Sep 17 11:10:17 <Myon>	so it'd be interesting to check if pull has a real bug, or if that's some corruption somewhere
Sep 17 11:10:28 <f3f3lix>	highly suspicious, but we have no evidence of actual hardware failure
Sep 17 12:36:22 <maxter>	I have a table on which deletes are being fired. I want a trigger that ignores the delete .. anyway to achieve this?
Sep 17 12:39:09 <mobidrop>	maxter, you could use a trigger or a rule
Sep 17 12:39:37 <maxter>	I am trying trigger but instead of is not allowed on tables
Sep 17 12:39:58 <chris64>	maxter: Did you find https://www.postgresql.org/docs/9.2/plpgsql-trigger.html (or newer)? There it says that "In the case of a before-trigger on DELETE, the returned value has no direct effect, but it has to be nonnull to allow the trigger action to proceed. Note that NEW is null in DELETE triggers, so returning that is usually not sensible. The usual idiom in DELETE triggers is to return OLD."
Sep 17 12:40:48 <chris64>	I would interpret that as returning null ignores the delete.
Sep 17 12:41:24 <maxter>	so your saying put a before delete with a return NULL will keep my row intact?
Sep 17 12:41:47 <ilmari>	I would expect that to make the DELETE return an error, not silently not delete it
Sep 17 12:42:28 <maxter>	I tired a 1/0 but got an error ..
Sep 17 12:42:30 <mobidrop>	probably it will give an exception if you return null
Sep 17 12:42:45 <chris64>	ilmari: mh, true when thinking one more about it there must be two possible reactions, pass, ignore, reject.
Sep 17 12:42:48 <chris64>	*three
Sep 17 12:42:53 <mobidrop>	postgres in general doesn't fail silently
Sep 17 12:43:59 <chris64>	but this is not failing
Sep 17 12:44:18 <chris64>	however I see your point mobidrop
Sep 17 12:44:53 <ilmari>	actually, I just tested it, and it does seem to silently not delete the rows when I add a BEFORE DELETE  FOR EACH ROW EXECUTE PROCEDURE trigger_returning_null()
Sep 17 12:45:49 <chris64>	interesting
Sep 17 12:45:58 <mobidrop>	ok documentation seems to be right
Sep 17 12:46:14 <chris64>	so would one then be supposed to throw some kind of error?
Sep 17 12:47:01 <chris64>	to reject the delete and abort the transaction?
Sep 17 12:56:46 <maxter>	worked. even though we assume some error should come.
Sep 17 12:56:56 <maxter>	cleanly ignores
Sep 17 13:04:38 <Intelo>	insert
Sep 17 13:04:41 <Intelo>	sorry
Sep 17 13:06:12 <Intelo>	I want to insert a table row but only if the id does not exists. So I need "select for insert"?
Sep 17 13:07:00 <Myon>	that doesn't work because there's row to lock (what SELECT FOR UPDATE does)
Sep 17 13:07:18 <Myon>	you want INSERT ON CONFLICT... DO NOTHING
Sep 17 13:08:15 <Intelo>	hm
Sep 17 13:08:50 <Intelo>	Myon,  is that upsert?
Sep 17 13:08:54 <Myon>	yes
Sep 17 13:09:06 <Myon>	with the "up" part muted
Sep 17 13:09:44 <Myon>	you could also just INSERT and ignore/catch the error
Sep 17 13:10:16 <Intelo>	hm
Sep 17 13:58:42 <svip>	Does a large `col IN (...)` list make psql noticeably slower?
Sep 17 13:59:49 <Myon>	the parse needs huge amounts of RAM if the list is really long
Sep 17 14:00:19 <svip>	Suppose I have a large table, T, (say 1 million rows), and I have a list of 3000 IDs, that I want to return of.  But I wish to restrict it to 100 result (`LIMIT 100`), but I am including all 3000 IDs in the WHERE clause, to ensure they are sorted correctly, i.e. so it is the first 100 rows of those 3000 elements according to a specific sort.
Sep 17 14:00:40 <svip>	Myon: So the limit is on the parser?
Sep 17 14:00:56 <RhodiumToad>	use an array instead of IN
Sep 17 14:01:18 <RhodiumToad>	WHERE id = ANY (?)   where the ? is an array parameter, as long as you're not using psycopg2
Sep 17 14:02:25 <svip>	I am using Go's PostgreSQL driver.
Sep 17 14:03:35 <svip>	And arrays are faster than IN?
Sep 17 14:06:50 <chris64>	svip: I think he's proposing to use a prepared statement then, so that the array is only inserted as a placeholder and transferred separately not in text but in binary form
Sep 17 14:07:20 <svip>	I was going to do it as a prepared statement in either case.
Sep 17 14:07:23 <chris64>	this then more efficient than parsing 3000 values from the string. however the query execution probably won't benefit
Sep 17 14:07:52 <svip>	Or does prepared statements not work properly with IN?
Sep 17 14:08:09 <chris64>	RhodiumToad: does that make a difference then?
Sep 17 14:08:37 <chris64>	because it's not parsed anyway then, no?
Sep 17 14:09:28 <RhodiumToad>	the parsing speed is better with arrays even if you do  =ANY('{1,2,3,4,...}'::integer[])  with an interpolated literal, but of course you should use a real parameter
Sep 17 14:09:54 <svip>	The IDs in this case are UUIDs.
Sep 17 14:10:10 <RhodiumToad>	most drivers pass parameters in text mode, not binary, but the specialized array-value parser is still far more efficient than the SQL parser
Sep 17 14:10:31 <chris64>	RhodiumToad: interesting, thanks!
Sep 17 14:11:00 <RhodiumToad>	the IN (blah,blah,blah) stuff goes through the general bison grammar, which is no speed demon and does a lot of memory allocations for each token
Sep 17 14:12:18 <RhodiumToad>	(the problem with psycopg2 is that it interpolates array parameters as ARRAY[blah,blah,blah] which means you're right back to the bison parser again)
Sep 17 14:21:33 <AWizzArd>	Is there some way to test if pg is listening when connecting via telnet? Some simple text protocol that would allow me to simulate being a client?
Sep 17 14:22:02 <RhodiumToad>	no, the protocol isn't really amenable to text
Sep 17 14:28:35 <AWizzArd>	RhodiumToad: oki
Sep 17 14:28:40 <AWizzArd>	thx
Sep 17 14:44:10 <pstef>	CodeIgniter uses pg_advisory_lock() to serialize the reads and writes of PHP session data and it feels wrong, but at the same time I don't know how to do that better
Sep 17 14:44:59 <pstef>	in all cases, the choice is between waiting (which risks a DOS) and erroring (but how to handle that?)
Sep 17 14:47:58 <Myon>	select for update?
Sep 17 14:48:23 <Myon>	or just plain UPDATE
Sep 17 14:49:38 <pstef>	ok, but then another session wants to do the same. and it either waits (and subsequently saves its data when the first session is done) or it doesn't wait (and errors out)
Sep 17 14:49:59 <pstef>	well, sorry, they all are the same session really, just different threads
Sep 17 14:51:04 <Myon>	the usual trick is to use INSERT instead of UPDATE, and periodically consolidate entries back to the original row
Sep 17 14:52:09 <pstef>	I thought of that as well. but the PHP session data is a blob that PHP encodes and decodes for itself
Sep 17 14:57:30 <Myon>	the real fix for session data is to write stuff less often
Sep 17 14:57:47 <Myon>	"user is still alive" doesn't need to be recorded for each single mouse click
Sep 17 14:59:03 <velix>	I'm trying to do 140 left joins with USING :D
Sep 17 15:03:47 <velix>	silence :D
Sep 17 15:04:09 <Myon>	"don't"
Sep 17 15:05:05 <pstef>	Myon: sure I'd like less contention, but the fundamental problem remains
Sep 17 15:08:08 <Myon>	pstef: use SELECT FOR UPDATE instead?
Sep 17 15:10:24 <velix>	nooooooooo ERROR:  54000: target lists can have at most 1664 entries
Sep 17 15:10:57 <pstef>	Myon: doesn't avoid the problem of two DB sessions wanting to do the SELECT FOR UPDATE
Sep 17 15:11:38 <Myon>	this is a php problem, not a PostgreSQL one, isn't it?
Sep 17 15:11:42 <pstef>	I think the obvious conclusion is that since we don't want to wait on the other DB session, one of them will have to handle the conflict
Sep 17 15:12:01 <velix>	Is there no way to raise the amount of maximum columns?
Sep 17 15:12:21 <Myon>	velix: only by recompiling
Sep 17 15:12:28 <velix>	;...-(
Sep 17 15:12:30 <Berge>	But do you _really_ need it?
Sep 17 15:12:30 <Myon>	the limesurvey people recommend doing that
Sep 17 15:12:34 <velix>	Berge: Yes!
Sep 17 15:12:39 <Berge>	Myon: wow
Sep 17 15:12:49 <velix>	Myon: Guess what I'm doing ;)
Sep 17 15:12:58 <velix>	Survey stuff.
Sep 17 15:13:16 <Myon>	https://manual.limesurvey.org/Instructions_for_increasing_the_maximum_number_of_columns_in_PostgreSQL_on_Linux
Sep 17 15:13:25 <pstef>	debatable. I think "it's about how PG is used in a particular case" is not incorrect
Sep 17 15:15:29 *	velix switching to R
Sep 17 15:15:32 <velix>	;)
Sep 17 15:15:49 <Myon>	velix: jsonb should work well there
Sep 17 15:16:22 <velix>	Myon: I'm merging 140 tables on a single key to get > 2000 columns. How can jsonb help here?
Sep 17 15:17:31 <Myon>	for storing the columns in a single table
Sep 17 15:18:05 <Myon>	you could also jsonb_build_object in the query, but of course that's a kludge
Sep 17 15:21:13 <velix>	Myon: but quering this would be the next problem. I need to export as normal columns.
Sep 17 15:21:18 <velix>	I'm working on a work.around in R
Sep 17 15:24:37 <theseb>	Can someone explain issue with second small snippet here?... https://pastebin.com/SD6mB2Ph
Sep 17 15:24:54 <theseb>	I just added a FROM clause to end of a working select
Sep 17 15:25:23 <RhodiumToad>	what on earth made you think that was allowed?
Sep 17 15:27:23 <theseb>	RhodiumToad: i thought you can repeat any select by just appending a from
Sep 17 15:27:39 <theseb>	RhodiumToad: or in this case a "WITH ..... SELECT ..."
Sep 17 15:28:00 <theseb>	with a as (select k as u),
Sep 17 15:28:07 <theseb>	There is a smaller version w/ same issue
Sep 17 15:28:09 <theseb>	^^
Sep 17 15:28:25 <RhodiumToad>	you can't randomly append clauses
Sep 17 15:29:17 <theseb>	RhodiumToad: What i just pasted has 5 lines.....The first 4 depend on k......The 5th link sets k....what is the problem? how fix? is it a trivial parentheses or something?
Sep 17 15:29:22 <RhodiumToad>	look at the syntax in psql using  \h select
Sep 17 15:30:36 <theseb>	RhodiumToad: this is like last nite's discussion...if you think of 1st 4 lines as defining f(k)....the 5th line sets k = 2.....I thought the whole mess would return f(2) !
Sep 17 15:31:19 <theseb>	RhodiumToad: i'm in psql...\h just lists the SQL keywords
Sep 17 15:31:42 <RhodiumToad>	\h select
Sep 17 15:31:54 <incognito>	FROM <= only once in a query
Sep 17 15:32:13 <RhodiumToad>	all of the clauses appear only once, in fact
Sep 17 15:33:43 <theseb>	select
Sep 17 15:33:57 <theseb>	RhodiumToad: that bombed also...
Sep 17 15:34:02 <RhodiumToad>	also, you can't think of the WITH clauses as defining a function, because they don't do that; the WITH subqueries cannot reference anything defined outside the WITH clause
Sep 17 15:34:12 <RhodiumToad>	what bombed?
Sep 17 15:34:20 <theseb>	select
Sep 17 15:34:29 <RhodiumToad>	what did you do exactly
Sep 17 15:34:30 <theseb>	RhodiumToad: that did
Sep 17 15:34:43 <theseb>	RhodiumToad: i wrapped the original 4 lines in parens
Sep 17 15:34:52 <RhodiumToad>	why did you do that?
Sep 17 15:35:04 <theseb>	RhodiumToad: because i thought you can have multiple FROMs if you use parens?
Sep 17 15:35:12 <RhodiumToad>	*headdesk*
Sep 17 15:35:15 <theseb>	RhodiumToad: subqueries n' all that
Sep 17 15:35:20 <RhodiumToad>	that doesn't mean you can randomly add parens!
Sep 17 15:35:32 <RhodiumToad>	a subquery can appear only in very specific places
Sep 17 15:36:26 <theseb>	RhodiumToad: you said something that broke my heart... "the WITH subqueries cannot reference anything defined outside the WITH clause"
Sep 17 15:36:42 <RhodiumToad>	at the current query level that is
Sep 17 15:36:57 <RhodiumToad>	they can contain outer references if the WITH clause is itself in a subquery
Sep 17 15:37:21 <RhodiumToad>	also, in a non-recursive WITH, they can only reference CTEs defined earlier in the clause
Sep 17 15:37:45 <RhodiumToad>	CTEs are _not functions_, they behave like tables
Sep 17 15:38:02 <theseb>	RhodiumToad: I rewrote it with one from but still bombed...see this gem...
Sep 17 15:38:06 <theseb>	with k as (select 2),
Sep 17 15:38:27 <RhodiumToad>	I don't know what you think you just pasted in channel but it was just one line
Sep 17 15:38:35 <theseb>	RhodiumToad: now i have the 2nd CTE referencing k from the 1st CTE
Sep 17 15:38:43 <theseb>	RhodiumToad: lemmie try pastebin
Sep 17 15:38:49 <RhodiumToad>	use the damn paste site like you're supposed to (and ideally dpaste.de rather than that pastebin shit)
Sep 17 15:39:51 <theseb>	RhodiumToad: https://dpaste.de/b79R
Sep 17 15:40:19 <theseb>	RhodiumToad: why is that wrong?
Sep 17 15:40:32 <theseb>	RhodiumToad: it is so short it is running out of things to go wrong i hope
Sep 17 15:40:33 <RhodiumToad>	because you're confusing tables and columns
Sep 17 15:40:44 <RhodiumToad>	the name of a CTE is a _table_, not a column
Sep 17 15:41:25 <RhodiumToad>	with k as (select 2 as v), a as (select v as u from k), ...
Sep 17 15:42:29 <theseb>	RhodiumToad: i moved k to column..why this still bombed then?.. https://dpaste.de/CJB6
Sep 17 15:43:53 <RhodiumToad>	because you ignored what I said
Sep 17 15:44:36 <theseb>	RhodiumToad: ok...thinking...sec
Sep 17 15:44:38 <RhodiumToad>	the columns in scope in a given subquery are (a) the ones from the tables in the FROM clause of that subquery, plus any outer references
Sep 17 15:45:06 <RhodiumToad>	outer references come only from _enclosing_ queries, not other queries at the same level
Sep 17 15:45:58 <RhodiumToad>	you can't ever reference anything from a CTE without using the CTE name _in a FROM clause_
Sep 17 15:46:20 <theseb>	RhodiumToad: hmmm
Sep 17 15:46:52 <theseb>	that last comment helped
Sep 17 15:51:14 <theseb>	RhodiumToad: wow...thanks..i got something that worked but my brain hurts ;)
Sep 17 16:30:33 <eacameron>	Can a SERIALIZABLE READ ONLY DEFERRABLE ever *fail* due to another write in the middle?
Sep 17 16:43:25 <Zr40>	yes
Sep 17 16:51:37 <ilmari>	I thought point of READ ONLY DEFERRABLE was that it waits until it can get a snapshot that can't possibly have serialisation conflicts
Sep 17 16:54:35 <ilmari>	When all three of [SERIALIZABLE READ ONLY DEFERRABLE] are selected for a transaction [] it is able to run [] without any risk of contributing to or being canceled by a serialization failure - https://www.postgresql.org/docs/current/sql-set-transaction.html
Sep 17 17:19:39 <Sihar>	hello all, I want to ask, anyone ever install stolon using docker swarm?
Sep 17 17:19:59 <Sihar>	how we initial cluster when first time install?
Sep 17 17:21:01 <peerce>	what is stolon ?
Sep 17 17:24:02 <Sihar>	tools for make postgresql as cloud native HA
Sep 17 17:24:15 <Sihar>	https://github.com/sorintlab/stolon
Sep 17 17:24:49 <peerce>	best to ask them, thats not a postgres native project, and i've never heard of it before even after years of using postgres
Sep 17 17:26:08 <Sihar>	alright peerce
Sep 17 17:27:05 <asakahs>	zxcv
Sep 17 17:46:28 <velix>	 min(numeric, numeric) doesn't exist? I'm confused.
Sep 17 17:46:55 <velix>	ah no.
Sep 17 17:46:58 <velix>	DONT ANSWER
Sep 17 18:20:53 <davidfetter_work>	hi
Sep 17 18:21:22 <davidfetter_work>	I noticed that we got a commit recently for date formats in fractional seconds
Sep 17 18:22:31 <davidfetter_work>	it leaves out FF7-FF9 because we don't store time at ns precision. Is that feasible, or is it made in anticipation of feasibility?
Sep 17 18:23:37 <ysch>	eacameron: No. At least, if it does, it would be a bug, I guess.
Sep 17 18:25:38 <eacameron>	ysch: Ah good. So the once the deferrable read starts it effectively blocks other writes until it's done?
Sep 17 18:26:07 <eacameron>	Or, it has a consistent snapshot of state that can't be changed in the middle
Sep 17 18:26:22 <ysch>	eacameron: Nothing like it. ;) Reads _never_ block writes. It reads the consistent snapshot, like RR would.
Sep 17 18:26:50 <eacameron>	Ah perfect.
Sep 17 18:27:09 <eacameron>	Thank you. So there's no need to add retrying logic around a DEFERRABLE read-only serializable transaction
Sep 17 18:29:31 <ysch>	eacameron: Well, not sure what happens if it bumps into a DDL lock (due ALTER TABLE in a concurrent session, for instance).
Sep 17 18:30:51 <jonez>	depesz, it works fine in 10.x with a GIN
Sep 17 18:31:03 <jonez>	note I'm using ltree[]
Sep 17 18:31:48 <davidfetter_work>	I'm so sorry
Sep 17 18:34:22 <jonez>	it is a proper usage afaik
Sep 17 18:38:52 <depesz>	jonez: interesting. as far as I can tell, in my pg 13 - ltree doesn't have support for gin indexes.
Sep 17 18:39:03 <depesz>	so, not sure what you have working, and would love to see it.
Sep 17 18:40:41 <depesz>	oh, interesting. i was able to create the gin index too.
Sep 17 18:41:16 <jonez>	could you try it on 9.6?
Sep 17 18:41:22 <depesz>	i don't have 9.6
Sep 17 18:41:26 <jonez>	hmm
Sep 17 18:41:45 <depesz>	but if you have, just do: create table test (id serial primary key, ls ltree[]); CREATE INDEX path_gin_idx ON test USING GIN (ls);
Sep 17 18:41:50 <depesz>	and you will immediately know.
Sep 17 18:42:56 <jonez>	I get that same error about how ltree[] has no default operator class.
Sep 17 18:43:25 <depesz>	jonez: based on my quick look, it seems that gin index can be created, but it's using array_ops, so i guess it will have limited functionality.
Sep 17 18:43:39 <jonez>	but gist will work?
Sep 17 18:43:47 <depesz>	sure. docs mention usage of gist.
Sep 17 18:43:57 <jonez>	ok I will try that
Sep 17 18:59:04 <jonez>	ok, gist is working. cut a couple of ms off the execution time for a common query. ty for your  help :
Sep 17 18:59:05 <jonez>	:)
Sep 17 19:10:01 <davidfetter_work>	so about ns timestamps...
Sep 17 19:11:03 <davidfetter_work>	would that change fundamentally how we deal with them?
Sep 17 19:11:26 <esran>	is there any way to identify currently running queries for a session that are nested deep within a function call?
Sep 17 19:11:49 <davidfetter_work>	I'd love to know how if there is
Sep 17 19:12:12 <davidfetter_work>	I don't suppose the functions are SQL all the way down...
Sep 17 19:17:21 <steve-chavez>	Question regarding counting. I have a view like this: CREATE VIEW test AS SELECT id, expensive_op(id) FROM items;
Sep 17 19:17:40 <steve-chavez>	And I do: SELECT count(id) FROM test;
Sep 17 19:18:15 <steve-chavez>	I mistakenly thought that count would imply that expensive_op(id) is not executed but it does.
Sep 17 19:18:36 <steve-chavez>	Is there a way to prevent that?
Sep 17 19:19:52 <steve-chavez>	I've confirmed that it is executed by a RAISE NOTICE inside expensive_op.
Sep 17 19:29:37 <steve-chavez>	Doing a SELECT 1 FROM test; also results in expensive_op(id) being executed.
Sep 17 19:36:20 <Zr40>	steve-chavez: is expensive_op declared stable or volatile?
Sep 17 19:36:52 <steve-chavez>	volatile
Sep 17 19:37:07 <davidfetter_work>	that might account for it
Sep 17 19:37:32 <Zr40>	s/might/will/
Sep 17 19:37:33 <steve-chavez>	Oh.. stable would prevent it being called? Let me try..
Sep 17 19:38:05 <davidfetter_work>	steve-chavez, if the outputs depend exactly and only on the inputs, you can mark it IMMUTABLE
Sep 17 19:38:14 <Zr40>	yes, it will. But remember that STABLE is a promise that the function has no side effects, or in other words, you are okay with it being called zero times or ten times when you expect it to be called once
Sep 17 19:38:53 <Zr40>	IMMUTABLE has a bunch of other restrictions too
Sep 17 19:39:15 <davidfetter_work>	??immutable
Sep 17 19:39:15 <pg_docbot>	https://www.postgresql.org/docs/current/static/xfunc-volatility.html
Sep 17 19:40:36 <davidfetter_work>	Quoth the manual: For best optimization results, you should label your functions with the strictest volatility category that is valid for them.
Sep 17 19:41:16 <davidfetter_work>	If you lie to the DB, it will get its revenge ;)
Sep 17 19:42:17 <steve-chavez>	I've just confirmed it doesn't execute the expensive_op with STABLE. Amazing.. I see why it works that way now.
Sep 17 19:42:37 <steve-chavez>	Zr40, davidfetter_work: Thank you so much! :)
Sep 17 19:42:54 <davidfetter_work>	always happy to help
Sep 17 19:45:38 <theseb>	hi
Sep 17 19:45:42 <xocolatl>	does anyone know off hand the history of why dropping a domain drops the columns associated with them?
Sep 17 19:46:15 <davidfetter_work>	xocolatl, that sounds like a really unpleasant discovery. sorry to hear about it.
Sep 17 19:46:51 <xocolatl>	I've known about it for a while, but was reminded of it today
Sep 17 19:47:00 <davidfetter_work>	xocolatl, I presume you'd want the columns to demote to the next non-dropped level
Sep 17 19:47:14 <xocolatl>	I'd want standard compliance
Sep 17 19:47:34 <davidfetter_work>	what wisdom does the standard have to offer on this matter?
Sep 17 19:47:49 <xocolatl>	assign the base type to the column and copy over the constraints
Sep 17 19:48:24 <davidfetter_work>	in principle, that sounds doable.
Sep 17 19:48:37 <davidfetter_work>	so the base type, not just the next-lower remaining domain
Sep 17 19:48:50 <Zr40>	this happens without CASCADE?
Sep 17 19:48:51 <xocolatl>	the type the domain is based on
Sep 17 19:49:09 <davidfetter_work>	ah, so not necessarily the base type. cool :)
Sep 17 19:49:44 <theseb>	davidfetter_work: say....i thought of something maybe smart about SQL I was wondered your thoughts about...in procedural languages you can debug functions 1 at a time ....call it "bottom up programming"...."divide and conquer" or "encapsulation".....If I gave you a 500 line SELECT to debug....I'm not sure if you can rewrite it as a bunch of
Sep 17 19:49:44 <theseb>	separate views and CTEs that you can debug separately to do the same in SQL
Sep 17 19:49:53 *	davidfetter_work picturing layered domains like nonnegative_int and under_ten
Sep 17 19:50:10 <eacameron>	Another question about SERIALIZABLE READ ONLY DEFERRABLE: Can this be starved? I.e. can the wait for a consistent state be infinite even without a deadlock?
Sep 17 19:50:22 <mst>	davidfetter_work: I do those lots with Type::Tiny in perl
Sep 17 19:50:29 <mst>	I mean, like, the moral equivalent thereof
Sep 17 19:50:47 <eacameron>	cc ysch
Sep 17 19:51:10 <davidfetter_work>	you can do just what you described, theseb. also, SQL is (generally) easier to reason about because there isn't much by way of implementation detail there to distract you
Sep 17 19:51:21 <davidfetter_work>	mst, neat :)
Sep 17 19:51:54 <theseb>	davidfetter_work: hmmm ok..good to know
Sep 17 19:52:51 <davidfetter_work>	that said, debugging a 500-line program is still debugging a 500-line program no matter what. It's just that they're a *lot* less common in SQL than they are elsewhere.
Sep 17 19:53:34 *	davidfetter_work has heard of generated SQL from OLAP systems that runs to megabytes, but is fortunate not to have dealt with anything like that yet
Sep 17 19:53:53 <davidfetter_work>	the code, not the result sets
Sep 17 19:54:02 <theseb>	davidfetter_work: i thought SQL has no concept of scope and everything was global...i was glad to discover today that CTEs have some idea of scope....e.g. you can only refer to columns of another CTE in a CTE with a FROM
Sep 17 19:54:18 <theseb>	thanks to RhodiumToad for pointing that out
Sep 17 19:54:41 <davidfetter_work>	right, and you can't refer to other things in a target list (the part between SELECT and FROM) inside the target list
Sep 17 19:54:47 <davidfetter_work>	so there's scope
Sep 17 20:01:20 <eacameron>	To answer my own question, according to https://drkp.net/papers/ssi-vldb12.pdf, the answer is YES, SERIALIZABLE READ ONLY DEFERRED can be starved, but it is highly unlikely even with large work loads.
Sep 17 20:07:47 *	davidfetter_work wonders whether Dan will come back and work on PostgreSQL some more
Sep 17 20:07:59 <davidfetter_work>	drkp == Dan R. K. Ports
Sep 17 20:15:00 <OliverMT>	I am left joining a measurement table, constraining on data_type = "foo" or data_type = "bar"
Sep 17 20:15:11 <OliverMT>	and grouping by a source_id
Sep 17 20:15:38 <OliverMT>	no wait, sorry, as I typed it out I realized my logic was flawed :D
Sep 17 20:16:29 <andres>	davidfetter_work: Seems pretty unlikely.
Sep 17 20:16:47 <OliverMT>	I want to be able to select so I get a result row as source.name, source.id, measurements.bar_value, measurements.foo_value
Sep 17 20:17:19 <OliverMT>	and exlude any rows where I don't have both a data_type = "bar" value and data_type = "foo" value
Sep 17 20:17:48 <OliverMT>	is that possible without joining measurements as two different named joins and putting the where in the on: clause?
Sep 17 20:18:00 <OliverMT>	as in the row not being joined if the row with the correct data type doesnt exist
Sep 17 20:18:35 <Myon>	do we get to guess the schema?
Sep 17 20:19:57 <OliverMT>	I am typing up sample query
Sep 17 20:20:24 <OliverMT>	https://gist.github.com/olivermt/3f0811ea27905c2a3323afb0dfb8b185
Sep 17 20:20:39 <OliverMT>	so requirements is to have only one row per source
Sep 17 20:20:58 <OliverMT>	and to only return a result if BOTH measurement vertical and horizontal has a result for that source
Sep 17 20:21:10 <OliverMT>	the schema makes sure there can only be one horizontal or vertical measurement per source
Sep 17 20:21:15 <Myon>	yeah that would be two JOINs
Sep 17 20:21:19 <OliverMT>	ok, thought so
Sep 17 20:21:20 <Myon>	but don't do LEFT
Sep 17 20:21:21 <OliverMT>	thanks
Sep 17 20:21:26 <OliverMT>	INNER ?
Sep 17 20:21:35 <OliverMT>	if I do two INNERS I would indeed end up with zero rows if not both are present
Sep 17 20:21:43 <OliverMT>	is that reasoning correct?
Sep 17 20:21:51 <Myon>	yes
Sep 17 20:21:54 <OliverMT>	thanks
Sep 17 20:22:09 <Myon>	actually your current version isn't an OUTER (LEFT) join either because of the WHERE clause
Sep 17 20:22:39 <Myon>	outer joins are rare, you shouldn't be using them by default
Sep 17 20:24:57 <OliverMT>	that WHERE was just to illustrate the requirements but yes
Sep 17 20:32:28 <agohoth>	are there any big data firms simply reading everything into postgresql and then building all kinda data reductions?
Sep 17 20:32:32 <agohoth>	and rules stuff?
Sep 17 20:32:43 <agohoth>	can you build rules for incoming streams in psotgresql?
Sep 17 20:32:46 <agohoth>	whats new in 12?
Sep 17 20:32:54 <agohoth>	wow 11 was fast here n gone
Sep 17 20:33:13 <Myon>	read the 12beta announcement mails?
Sep 17 20:37:45 <StuckMojo>	or the full release notes (which are in beta, but there)
Sep 17 20:37:49 <ryantrinkle>	I'm trying to understand how read-only repeatable read transactions can observe non-serializable states of the DB
Sep 17 20:37:59 <ryantrinkle>	in particular, this sentence from the manual: For example, even a read only transaction at this level may see a  control record updated to show that a batch has been completed but not see one of the detail records which is logically part of the batch because it read an earlier revision of the control record.
Sep 17 20:38:19 <ryantrinkle>	if the detail records and the control record were committed in the same transaction, how could this happen?
Sep 17 20:38:33 <ryantrinkle>	https://www.postgresql.org/docs/current/transaction-iso.html#XACT-REPEATABLE-READ
Sep 17 20:39:56 <ryantrinkle>	is this for a scenario where the detail records are commited in txn 1, then the control record is updated in txn 2, but somehow the repeatable read transaction can see 2 but not 1?
Sep 17 20:48:06 <StuckMojo>	anyone doing encryption at rest in the datacenter (not aws) and if so how are you doing it? (i.e. what are you using for it)
Sep 17 20:48:22 <xocolatl>	??standard
Sep 17 20:48:23 <pg_docbot>	http://wiscorp.com/SQLStandards.html :: http://troels.arvin.dk/db/rdbms/
Sep 17 20:48:23 <pg_docbot>	http://savage.net.au/SQL/ :: http://wiki.postgresql.org/wiki/Developer_FAQ
Sep 17 20:48:23 <pg_docbot>	http://wiki.postgresql.org/wiki/Developer_FAQ#Where_can_I_get_a_copy_of_the_SQL_standards.3F :: https://wiki.postgresql.org/wiki/PostgreSQL_vs_SQL_Standard
Sep 17 20:48:23 <pg_docbot>	https://www.postgresql.org/docs/current/static/features.html
Sep 17 20:48:47 <xocolatl>	can't find 99 anymore
Sep 17 20:49:49 *	davidfetter_work doesn't quite get why people refer to previous versions of the standard. the latest supersedes all previous
Sep 17 20:50:59 *	xocolatl doesn't need for davidfetter_work to get it
Sep 17 20:53:06 <OliverMT>	Myon: I am looking at some old code I have that does something similar, and for some reason I am doing GROUP BY on the source
Sep 17 20:53:26 <OliverMT>	but I am not doing any counts or anything, do you have a clue as to any situation I would need to GROUP BY in what I descriebd above?
Sep 17 20:57:58 <OliverMT>	agohoth: a lot of 'big data firms' do that yes, using both timescaledb and citusdb
Sep 17 21:00:43 <agohoth>	oh?
Sep 17 21:02:26 <agohoth>	wow like extended postgresql
Sep 17 21:02:27 <agohoth>	woa
Sep 17 21:02:54 <agohoth>	at work they want to do batch and streams and use barfy java frameworks like argo
Sep 17 21:03:03 <agohoth>	wadda mess
Sep 17 21:03:11 <agohoth>	no one thinks about the data
Sep 17 21:03:32 <agohoth>	just an afterthought   I blame java oo programing culture and finance incentives
Sep 17 21:07:45 <Logicgate>	hey guys
Sep 17 21:07:59 <Logicgate>	I'm using go-pg and having problems with locking queries staying open
Sep 17 21:08:06 <Logicgate>	or "active" I should say
Sep 17 21:08:27 <Logicgate>	when I start my process, everything is fine and eventually the PG processes start to pile up and the CPU usage rockets to 100%
Sep 17 21:08:48 <Logicgate>	I have workers on a tick (every second), will query the DB.
Sep 17 21:09:25 <Logicgate>	it's the same query idling
Sep 17 21:09:42 <Logicgate>	https://puu.sh/Eik8J/d90135dd66.png
Sep 17 21:15:42 <agohoth>	Is there any extended postgresql that is for batch processing?
Sep 17 21:16:31 <elmcrest>	hey everybody. can one say it's better to have more smaller tables or less bigger tables?
Sep 17 21:16:47 <xocolatl>	it's better to have the tables you need
Sep 17 21:17:13 <elmcrest>	I have the choice ... so I was wondering if there maybe is a general rule of thumb
Sep 17 21:17:28 <xocolatl>	how can you have the choice?
Sep 17 21:18:01 <Logicgate>	agohoth, what do you mean?
Sep 17 21:18:13 <elmcrest>	give this Django ORM model definition ... I was going with this first because of the comfort of having nice ORM queries ... but I wonder if that makes sense for postgres https://dpaste.de/2GpB
Sep 17 21:19:08 <elmcrest>	xocolatl because I can either use inheritance, so Django just create foreignKey relations for me for "inheritance" and joins them, or I create a huge table with optional fields
Sep 17 21:19:31 <elmcrest>	the example document types aren't the end of the game, in the end I expect about 70-100 document types
Sep 17 21:20:01 <agohoth>	well my company is 80% batch ingenstion for its needs in analystics
Sep 17 21:20:07 <xocolatl>	I'd need to see actual tables and queries.  I don't know what django does
Sep 17 21:20:17 <agohoth>	they are looking at various java batch processing frameworks on apache.org
Sep 17 21:20:29 <agohoth>	django lol
Sep 17 21:20:38 <elmcrest>	I like it :P
Sep 17 21:20:46 <agohoth>	its nice?
Sep 17 21:21:11 <agohoth>	seems cart b4 horse to have a framework in python generate db tables
Sep 17 21:21:38 <elmcrest>	agohoth its nice to me at least :P
Sep 17 21:23:12 <agohoth>	Well seems to be like wrong to have some code generate some table definitons
Sep 17 21:23:16 <elmcrest>	xocolatl tables https://dpaste.de/4Aea
Sep 17 21:23:18 <agohoth>	where does the db engineering come in?
Sep 17 21:23:36 <elmcrest>	agohoth at the level of implementing the ORM in python
Sep 17 21:23:49 <elmcrest>	I'm pretty sure it does the SQL job better than I would ...
Sep 17 21:24:57 <elmcrest>	xocolatl so that's the "base table" which is currently rather small ... https://dpaste.de/Nsh0
Sep 17 21:25:21 <elmcrest>	if I go with my current "design" the "Referenced by" section would grow a lot ...
Sep 17 21:28:19 <elmcrest>	so probably it doesn't really matter at my scale I guess ... I'm neither having millions of entries nor millions of tables
Sep 17 22:43:51 *	Disconnected ()
**** ENDING LOGGING AT Tue Sep 17 22:43:51 2019

**** BEGIN LOGGING AT Tue Sep 17 22:44:16 2019

Sep 17 22:44:16 *	Now talking on #postgresql
Sep 17 22:44:16 *	Topic for #postgresql is: PostgreSQL 12beta4 is out. Test! https://www.postgresql.org/about/news/1972/ || Security releases 11.5, 10.10, 9.6.15, 9.5.19, 9.4.24 are out. Upgrade ASAP! || Don't ask to ask; just ask! || Paste: type ??paste for list || Docs: https://www.postgresql.org/docs/current/ || Off topic? #postgresql-lounge || CoC: https://www.postgresql.org/about/policies/coc/
Sep 17 22:44:16 *	Topic for #postgresql set by xocolatl!xocolatl@gateway/vpn/protonvpn/xocolatl (Thu Sep 12 16:46:28 2019)
Sep 17 22:44:16 *	Channel #postgresql url: https://www.postgresql.org
Sep 17 22:52:51 <ryantrinkle>	if i run a bunch of transactions in serializable mode, is there any guarantee that their commit order in the WAL will correspond to a valid serialization of the transactions?
**** ENDING LOGGING AT Tue Sep 17 22:58:02 2019

**** BEGIN LOGGING AT Tue Sep 17 22:58:02 2019

Sep 17 23:03:17 <jonez>	greetings. I'm experimenting with 'create table as', and it appears it only looks at rows from __link once at the time I create the table. so if I add something to __link, link does not have it.
Sep 17 23:04:07 <jonez>	is there a way to create a view (or an object that updates automagically) that I can also add a gist index to?
Sep 17 23:06:12 <Intelo>	xocolatl,  just pseudo code to explain
